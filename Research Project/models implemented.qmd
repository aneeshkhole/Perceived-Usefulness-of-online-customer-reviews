# Models Implemented {#sec-models-implemented}

```{python importsBlock}
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import seaborn as sns
# from sklearn import tree
from sklearn.metrics import (
    accuracy_score,f1_score,
    precision_score,recall_score, 
    confusion_matrix, ConfusionMatrixDisplay,
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn import svm
from imblearn.over_sampling import ADASYN, SMOTE
import random
from sklearn.utils.class_weight import compute_class_weight

import statsmodels.api as sm

```

```{python DataImports}
product_data = pd.read_csv('../data/master_product_list.csv')
review_orig = pd.read_csv('../data/reviews_with_stats.csv')
reviews = pd.read_csv('../data/reviews_outlier_adjusted.csv')
reviews['prodSiteID'] = reviews['prodSiteID'].str.upper()
review_orig['prodSiteID'] = reviews['prodSiteID'].str.upper()
product_data['prodSiteID'] = product_data['prodSiteID'].str.upper()

reviews = reviews.merge(
    right=pd.DataFrame(
      product_data[
        [
            'product_price','prodSiteID',
            'prod_subjectivity','total_star_rating','site'
        ]
      ]
    ),
    on='prodSiteID',
    how='left'
)

review_orig = review_orig.merge(
    right=pd.DataFrame(
      product_data[
        [
            'product_price','prodSiteID',
            'prod_subjectivity','total_star_rating','site'
        ]
      ]
    ),
    on='prodSiteID',
    how='left'
)

reviews['is_helpful'] = np.select(
    [
        reviews['review_helpful_votes'] > 0,
        (reviews['review_helpful_votes'] == 0) | (reviews['review_helpful_votes'].isna()),
    ],
    [
        1,
        0
    ]
)

review_orig['is_helpful'] = np.select(
    [
        review_orig['review_helpful_votes'] > 0,
        (review_orig['review_helpful_votes'] == 0 )| (reviews['review_helpful_votes'].isna()),
    ],
    [
        1,
        0
    ]
)

test_metrics = pd.DataFrame(
    {
        'Model':[],
        'Useful Level':[],
        'Accuracy':[],
        'F1':[],
        'Precision':[],
        'Recall':[]
    }
)

reviews = reviews.dropna(subset=[
    'review_star_rating_adjusted', 'review_helpful_votes_adjusted',
    'review_subjectivity_adjusted', 'review_polarity_adjusted',
    'review_length_adjusted','prod_subjectivity','total_star_rating'
],axis=0)
review_orig = review_orig.dropna(subset=[
    'review_star_rating', 'review_helpful_votes',
    'review_subjectivity', 'review_polarity',
    'review_length','prod_subjectivity','total_star_rating'
],axis=0)

# tmp = pd.DataFrame(
#     reviews[reviews['productID'].isin(list(range(1,16)))]
# )

tmp = reviews[reviews['productID'].isin(list(range(1,16)))].copy()

train_frame = pd.DataFrame(
    tmp.sample(n=int(.8*len(tmp)))
)

test_frame = pd.DataFrame(
    tmp.loc[~tmp.index.isin(train_frame.index)]
)

additional_data = review_orig[~review_orig['prodSiteID'].isin(train_frame['prodSiteID'].unique())]

test_frame = pd.concat([test_frame, additional_data], ignore_index=True)

test_frame = test_frame.sample(n=int(.2*len(tmp)))

# test_frame = pd.DataFrame(
#     review_orig[~review_orig['prodSiteID'].isin(train_frame['prodSiteID'].unique())]
# )
```

```{python test}

print(test_frame.shape)
print(train_frame.shape)
```

```{python DataTransforms}
X_train,X_test,y_train,y_test= [
    StandardScaler().fit_transform(np.array(train_frame[[
        'review_star_rating_adjusted', 'review_helpful_votes_adjusted',
        'review_subjectivity_adjusted', 'review_polarity_adjusted',
        'review_length_adjusted','prod_subjectivity','total_star_rating'
    ]])),
    StandardScaler().fit_transform(np.array(test_frame[[
        'review_star_rating', 'review_helpful_votes',
        'review_subjectivity', 'review_polarity',
        'review_length','prod_subjectivity','total_star_rating'
    ]])),
    np.array(train_frame['is_helpful']),
    np.array(test_frame['is_helpful'])
]
```


```{python PCATransforms}
pca = PCA()
pcs = pca.fit_transform(X_train)
pr_df = pd.DataFrame(data=pcs)
pr_df.columns = ["PC{}".format(i) for i in range(1,len(pca.components_)+1)]
exp_var = pca.explained_variance_ratio_
cum_var = np.cumsum(exp_var)
cum_var_df = pd.DataFrame({
    'Principal Component':[f'PC{i+1}' for i in range(len(cum_var))],
    'Cumulative Variance':cum_var,
    'Explained Variance':exp_var
})
cum_var_df
pca = PCA(n_components=6)
w=pca.fit(X_train)
X_train_pca=pca.transform(X_train)
X_test_pca=pca.transform(X_test)
```

In our modeling of the collected data, we seek to investigate several models for the generalization of the work performed by @percUse.

We will examine, compare, and contrast the use of the following models: 

* Multiple Linear Regression Prediction

* Logistic Regression Classification

* K-Nearest Neighbors Classification

* Support Vector Machine Classification

## Included Variables

```{python tbl-var-selection}
#| label: tbl-var-selection
#| tbl-cap: Selected Variables for Model Training

pd.DataFrame({
    'Variable':['review_star_rating', 'review_helpful_votes',
        'review_subjectivity', 'review_polarity',
        'review_length','prod_subjectivity','total_star_rating'],
    'Pre-Transformation Type':['int','int','float','float','int','float','float'],
    'Post-Transformation Type':['PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float'],
    'Reason for Inclusion':['literature survey','literature survey','intuition','intuition','literature survey','intuition','literature survey']
}).style.hide(axis='index')
```

## Data Adjustments

As noted in our exploratory data analysis, each individual site has statistically significant differences in key variables we're considering in our modeling.  To mitigate the potential for under or overfitting, and misrepresentation due to variable scale we perform the following transformations to our data:

<!--NOTE - need information here on specific variables under consideration for our modeling-->

1. Variable outlier adjustment.  We noted in our EDA that each of the e-commerce platforms had high volumes of outliers with respect to the inter-quartile range.  We applied a transformation to our data to map any outlier variable value on a per-website basis from its value to $\mu+3\cdot sd(\text{variable})$ for high-end outliers, and $\mu-3\cdot sd(\text{variable})$ for low-end outliers.  In the event that either of these values exceeded the minimum or maximum value of the dataset, we mapped the value to the minimum or maximum accordingly.

2. Standard scaling of variables.  After adjusting outliers, we re-mapped all of our feature variables to be on the scale of the standard normal distribution $N\sim(0,1)$

3. Response variable transformation to binary value.  We denoted a single useful vote as meaning that the review was useful to customers, and mapped the value to True/1, and False/0 otherwise.

4. Dimensionality Reduction via Principal Component Analysis.  To accelerate training and evaluation by our models, we...

Here is a sample (first 10 observations) of our data prior to the transformation:

```{python tbl-pre-xform-data}
#| label: tbl-pre-xform-data
#| tbl-cap: Data (pre-transformation)

#Let's look at this from the perspective of
#key variables we're exploring...
reviews[['review_star_rating', 'review_helpful_votes',
        'review_subjectivity', 'review_polarity',
        'review_length','prod_subjectivity','total_star_rating']].head(10).style.hide(axis='index')
```

And here is a sample of our data after the applied transformations:

* Snippet of first 10 observations in the X_train set:

```{python tbl-post-xform-data}
#| label: tbl-post-xform-data
#| tbl-cap: Data (pre-transformation)

pd.DataFrame(X_train, columns=['review_star_rating', 'review_helpful_votes',
        'review_subjectivity', 'review_polarity',
        'review_length','prod_subjectivity','total_star_rating']).head(10).style.hide(axis='index')

```

Classes in y_train set were mapped as follows for all non-linear models:

* 0: if the review had no helful votes

* 1: if the review had one or more than 1 helpful votes

### Training Data

To train our dataset, we leveraged the data post-transformation to train each of our models, including the adjustments of outlier datapoints to being within 3 standard deviations of the mean of each variable.  We selected an 80% sample of this data and leveraged the same dataset to train each model.

### Testing Data

For testing, we evaluated each model against transformed data, omitting the transformation of outliers to being within 3 standard deviations of the mean.  We performed this action to enable a fair comparison of each model against one another when working with real-world data.

## Examination of the Original Multiple Linear Regression


### Inspection of Linear Model Assumptions

Generally, on examining the plot of fitted vs. observed values for the linear model, we feel our methods have produced similar outcomes as for @percUse.  The predicted values appear to have a slightly lesser scale and range than that of the observed values. 

#### Linearity of the Model

Our version of the linear model is designed as follows:
$\hat{y} = \beta_0+\beta_1X_{rsr}+\beta_2X_{rs}+\beta_3X_{rp}+\beta_4X_{rl}+\beta_5X_{ps}+\beta_6X{tsr}$

Where each of the following variables have been standard-scaled to a range between 0 and 1 for the input data:

* $X_{rsr}$ corresponds to the individual review's star rating

* $X_{rs}$ corresponds to the review's subjectivity score

* $X_{rp}$ corresponds to the review's polarity score

* $X_{rl}$ corresponds to the review's length (in words)

* $X_{ps}$ corresponds to the product description subjectivity score (replacement for the binary settings for search, experience, or mixed goods from @percUse's model).

* $X_{tsr}$ is the overall star rating for the product.

With this construct, we proceeded to generate a linear model trained on our standard scaled data. 

```{python fig-mlr-results}
#| label: fig-mlr-resultss
#| fig-cap: Linear Relationship Between Predictors and Response

##train - test

y_train_mlr,y_test_mlr = [np.array(train_frame['review_helpful_votes']),
    np.array(test_frame['review_helpful_votes'])]

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
#from sklearn.linear_model import LassoLarsIC

mlr = LinearRegression()

mlr.fit(X_train,y_train_mlr)

y_hat_base = mlr.predict(X_test)

beta_base = mlr.coef_
beta0_base = mlr.intercept_
R2_base = r2_score(y_hat_base,y_test_mlr)
MSPE_base = np.mean((y_hat_base - y_test_mlr)**2)


plt.title('Multiple Linear Regression')
plt.xlabel('Predicted Values')
plt.ylabel('Observed Values')
plt.scatter(
    y_hat_base,y_test_mlr    
)
plt.show()
```

Our testing produced the following coefficients:

$\beta_0$: `{python} round(mlr.intercept_,3)`, $\beta_1$: `{python} round(mlr.coef_[0],3)`,$\beta_2$: `{python} round(mlr.coef_[1],3)`,$\beta_3$: `{python} round(mlr.coef_[2],3)`,$\beta_4$: `{python} round(mlr.coef_[3],3)`,$\beta_5$: `{python} round(mlr.coef_[4],3)`,$\beta_6$: `{python} round(mlr.coef_[5],3)`

On new training data, the model appears to underpredict the output value, across the board, for the new data.  That being said, there is still a strong linear trend between the predictors and the response.  The intent of this model is not to predict the actual number of votes that a review might have, but whether or not that review would be considered useful to customers.  

Generally examining this result, we would conclude that there is a linear trend between these features, their coefficients, and their corresponding response.

#### Homoscedasticity on Normalized Data

This model has substantial challenges with heteroscedasticity.  Let's examine a plot of fitted vs. residuals in the model:

```{python fig-non-constant-var-1}
#| label: fig-non-constant-var-1
#| fig-cap: Heteroscedasticity in Model

plt.scatter(
    y_hat_base,y_hat_base - y_test_mlr
)
plt.xlabel('Predicted Value')
plt.ylabel('Model Residuals')
plt.title('Evaluating Constant Variance - Predicted vs. Residual Values')
plt.show()
```

There is a clear linear pattern in the residuals of this plot.  It seems that the test data may have other linear factors associated with it.  If we adjust the output to this additional linear factor, we can examine for heteroscedasticity in the adjusted plot. 

Performing a further linear regression on the fitted vs. observed values, we identified a slope $\beta_{1^*}$ of approximately 1.44 and intercept of $\beta_{0^*}$ of approximately 0.003.  Adjusting the predictions and plotting the re-fitted vs. observed values, we see the following:

```{python fig-non-constant-var-2}
#| label: fig-non-constant-var-2
#| fig-cap: Heteroscedasticity in Adjusted Model

mlr2 = LinearRegression()
x_points = y_hat_base*1.43811313 - 0.0030281865303148914
plt.scatter(
    x_points,x_points-y_test_mlr
)
plt.show()
```

This plot also appears to have issues with non-constant variance in the residuals.  As we traverse from left to right on this plot, the magnitude of the residuals appears to shrink or reduce.

Once again, the output of this model is not necessarily to predict the precise value of the number of votes a review comment will have, but instead to project how useful this vote would be perceived by new customers.  Although the issues with heteroscedasticity do exist for this model, we still find that the output of this model is potentially useful.

#### Normality in Residuals
```{python fig-qq-resid}
#| label: fig-qq-resid
#| fig-cap: Model Residuals are Abnormal

fig,ax = plt.subplots()
mu = x_points.mean()
sd = x_points.std()
norm = (x_points-mu) / sd
sm.qqplot(norm,ax=ax)
ax.axline((0,0),slope=1,color='black')
fig.suptitle('Q-Q Plot of Residuals')
plt.show()
```

From the above plot, it is clear that this model fails to adhere to the multiple linear regression requirement for residual normality.

#### Conclusion on MLR model

We conclude that the MLR model is *not* a statistically well-performing model for the purpose of *predicting* the number of *helpful votes* a review comment will receive.

That being said, in spite of its violations of the assumptions of multiple linear regression, we *do* find that the model is useful in predicting a relative usefulness score for a given review comment.  If all comments in a thread for a specific product were run against this model, we believe that the model will reflect the relative usefulness of a comment with respect to other comments in the same thread.  Sorting and organizing comments based on the maximum output value could bring the most useful comments to the top of the queue for review by potential shoppers.

We'll examine the per-product correlation next:

```{python MLR-per-product}
y_train_mlr,y_test_mlr = [np.array(train_frame['review_helpful_votes']),
    np.array(test_frame['review_helpful_votes'])]

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
#from sklearn.linear_model import LassoLarsIC

mlr = LinearRegression()

mlr.fit(X_train,y_train_mlr)

y_hat_base = mlr.predict(X_test)

beta_base = mlr.coef_
beta0_base = mlr.intercept_
R2_base = r2_score(y_hat_base,y_test_mlr)
MSPE_base = np.mean((y_hat_base - y_test_mlr)**2)



test_frame['MLR_prediction'] = mlr.predict(X_test)

import seaborn as sns

sns.scatterplot(
    data=test_frame,
    x='MLR_prediction',
    y='review_helpful_votes',
    hue='prodSiteID'
)

# plt.title('Multiple Linear Regression')
# plt.xlabel('Predicted Values')
# plt.ylabel('Observed Values')
# plt.scatter(
#     y_hat_base,y_test_mlr    
# )
# plt.show()
```

Having pursued the suggested research of @percUse and producing similar results, we will proceed to evaluate other models for detecting useful comments.

## Logistic Regression Classification

 Our next choice for examination was logistic regression.  The MLR called for use of only numeric or continuous variables.  Logisitic regression enables us to examine the inclusion of additional categorical variables as part of the regression consideration.

### Hyperparameter Tuning

Logistic regression is one of the best performing models in the project after SVM. There are 3 logistic regression models - one was tuned using the class weight hyperparameter to address the class imbalance present in the dataset. By assigning a higher weight to the minority class (useful level 1) and a lower weight to the majority class (useful level 0), the model was able to better capture the patterns associated with the minority class, leading to improved performance metrics.

### Oversampling techniques

Additionally, 2 more logistic regression models are trained using two oversampling techniques, namely ADASYN and SMOTE. ADASYN, which generates synthetic samples for the minority class based on their difficulty in learning regions, and SMOTE, which creates synthetic samples by interpolating between existing minority class samples, were used to address the class imbalance problem. These techniques help to provide the model with more balanced training data, allowing it to learn the characteristics of both classes more effectively.
 
It is a well know fact that oversampling techniques are employed if there a severe class imbalance if hyperparameter tuning does not improve the model performance. However, despite the heavy class imbalance, the tuned model and the SMOTE model achieve great result with 98% accuracy indicating that it is proficient at making correct predictions. 68% recall is decent, but indicates that the model maybe classifying the positive instances from minority class incorrectly. 

<!--try to tweak and tune this model for better performance--> 
### Logistic Regression Test Results

```

```

```{python LogRegBlock}
# need to divide into pieces to add figures / labels for each.
#Logmod hyperparameter
random.seed(123)

LogMod = LogisticRegression(class_weight={0:.2,1:.8})
LogMod.fit(X_train_pca,y_train)
y_pred = (LogMod.predict_proba(X_test_pca)[:,1]>0.25)

test_metrics.loc[len(test_metrics)] = {
    'Model':'Logistic Regression (TUNED)',
    'Useful Level': "above 0",
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}
# ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()
# display(test_metrics)

#### Logmod ADASYN

adasyn = ADASYN()
X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pca, y_train)
logmod_adasyn = LogisticRegression()
logmod_adasyn.fit(X_train_adasyn, y_train_adasyn)
y_pred_adasyn = logmod_adasyn.predict(X_test_pca)


test_metrics.loc[len(test_metrics)] = {
    'Model': 'Logistic Regression (ADASYN)',
    'Useful Level': "above 0",
    'Accuracy': accuracy_score(y_test, y_pred_adasyn),
    'F1': f1_score(y_test, y_pred_adasyn),
    'Precision': precision_score(y_test, y_pred_adasyn),
    'Recall': recall_score(y_test, y_pred_adasyn)
}

# ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_adasyn)).plot()
# display(test_metrics)

X_test_scaled = StandardScaler().fit_transform(X_test)
X_test_pca = pca.transform(X_test_scaled)

#### Logmod SMOTE

smote = SMOTE()
X_train_smote, y_train_smote = smote.fit_resample(X_train_pca, y_train)
logmod_smote = LogisticRegression()
logmod_smote.fit(X_train_smote, y_train_smote)

y_pred_smote = logmod_smote.predict(X_test_pca)

test_metrics.loc[len(test_metrics)] = {
    'Model': 'Logistic Regression (SMOTE)',
    'Useful Level': "above 0",
    'Accuracy': accuracy_score(y_test, y_pred_smote),
    'F1': f1_score(y_test, y_pred_smote),
    'Precision': precision_score(y_test, y_pred_smote),
    'Recall': recall_score(y_test, y_pred_smote)
}


# ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_smote)).plot()
# display(test_metrics)

test_metrics_df = pd.DataFrame(test_metrics)
display(test_metrics_df[:3].style.hide(axis='index'))


fig, axes = plt.subplots(3, 1, figsize=(18, 6))

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot(ax=axes[0])
axes[0].set_title('Tuned Model')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_adasyn)).plot(ax=axes[1])
axes[1].set_title('ADASYN')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_smote)).plot(ax=axes[2])
axes[2].set_title('SMOTE')

plt.tight_layout()
plt.show()
 
```

```{python UsefulComments-logreg}
# review_orig['pred_useful'] = y_pred_adasyn
# x = list(review_orig[(review_orig['is_helpful']==0) &(review_orig['pred_useful']==1)]['review_content'])#[['reviewer_name','verified_purchase','review_content','review_star_rating']]
# for i in x:
#     print(i)
#print(len(x))
```

## K-Nearest Neighbors Classification

K-Nearest Neighbors is used to learn and identify the target class instances. It makes predictions by calculating distance (usually, Euclidean distance) between a given instance and all other instances in the dataset in feature space. 

### Hyperparameter Tuning

The value of k is the most critial hyperparameter in the KNN Calssification algorithm. It determines the performance of the model. Usually a small k value leads to overly complex understanding of the data that might result into overfitting, however, a higher k values can lead to underfitting. 

We have looked at multiple values of k for our given data and compared a set of model metrics - accuracy, F1 score, precision, and recall to find the most optimal model to be the model with `k = 3`.

Looking the results shown below we can say that:

1. Just by looking at the accuracy for any given value of k, it ranges between 98-99% indicating that KNN might be a good model for the given data.

2. Among the evaluated models, KNN with `k = 3` achieves the highest F1 score of approximately 78.2%, indicating a balanced trade-off between precision and recall.

3. While all models have high precision and recall rates, KNN with higher k values (5, 7, and 9) demonstrate slightly lower F1 scores.

### KNN Test Results
<!--try to tweak and tune this model for better performance--> 
```{python KnnBlock}
# 3 Neighbors is best performing
# need to divide this into pieces to add figure labels.
random.seed(123)

knn = KNeighborsClassifier(n_neighbors=3, weights='distance')

knn.fit(X_train_smote,y_train_smote)
y_pred_3 = knn.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'KNN (k=3)',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_3),
    'F1':f1_score(y_test,y_pred_3),
    'Precision':precision_score(y_test,y_pred_3),
    'Recall':recall_score(y_test,y_pred_3)
}

# 5 Neighbors
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_smote,y_train_smote)
y_pred_5 = knn.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'KNN (k=5)',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_5),
    'F1':f1_score(y_test,y_pred_5),
    'Precision':precision_score(y_test,y_pred_5),
    'Recall':recall_score(y_test,y_pred_5)
}

# 7 Neighbors
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_smote,y_train_smote)
y_pred_7 = knn.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'KNN (k=7)',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_7),
    'F1':f1_score(y_test,y_pred_7),
    'Precision':precision_score(y_test,y_pred_7),
    'Recall':recall_score(y_test,y_pred_7)
}

# 9 Neighbors
knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train_smote,y_train_smote)
y_pred_9 = knn.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'KNN (k=9)',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_9),
    'F1':f1_score(y_test,y_pred_9),
    'Precision':precision_score(y_test,y_pred_9),
    'Recall':recall_score(y_test,y_pred_9)
}

# Display results
test_metrics_df = pd.DataFrame(test_metrics)
display(test_metrics_df[3:7].style.hide(axis='index'))

fig, axes = plt.subplots(4, 1, figsize=(11, 10))

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_3)).plot(ax=axes[0])
axes[0].set_title('KNN Model (k=3)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_5)).plot(ax=axes[1])
axes[1].set_title('KNN Model (k=5)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_7)).plot(ax=axes[2])
axes[2].set_title('KNN Model (k=7)')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_9)).plot(ax=axes[3])
axes[3].set_title('KNN Model (k=9)')

plt.tight_layout()
plt.show()
```

```{python UsefulComments-knn}
# review_orig['pred_useful'] = y_pred_3
# x = list(review_orig[(review_orig['is_helpful']==0) &(review_orig['pred_useful']==1)]['review_content'])#[['reviewer_name','verified_purchase','review_content','review_star_rating']]
# for i in x:
#     print(i)
#print(len(x))
```

## Support Vector Machine Classification

Support Vector Machines work on classification problems by finding an optimal hyperplane that best classifies the target classes in the given feature space. Because of its flexibility of moving the hyperplane and adapting to the intricacies of the data, SVM proves to be a powerful algorithm.

### Hyperparameter Tuning

The complexity of a SVM model is determined by the choice of kernel function that allows to capture all nuances of the data. Below are the implmentation results of the performance metrics of Support Vector Machine (SVM) models trained with different kernel functions: polynomial (SVM-Poly), radial basis function (SVM-RBF), and sigmoid (SVM-Sigmoid). 

To tune each version of this model, we leveraged a reduced set of principal components, and adjusted class weighting, as having useful votes for a product is a relative rarity across each of the e-commerce platforms.  To boost our recall, we elected to assign weights of 0.1 to class 0 (not useful) and 0.9 to class 1 (useful).

### SVM Test Results

```{python SVMBlock}
## need to divide this into pieces to add figure labels
# SVM Poly
random.seed(123)

spt_vector = svm.SVC(
    kernel='poly',degree=1,probability=True,class_weight={0:0.2,1:0.8}
).fit(X_train_pca,y_train)

y_pred_poly = spt_vector.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'SVM-Poly',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_poly),
    'F1':f1_score(y_test,y_pred_poly),
    'Precision':precision_score(y_test,y_pred_poly),
    'Recall':recall_score(y_test,y_pred_poly)
}

# SVM RBF
spt_vector_rbf = svm.SVC(
    kernel='rbf',degree=1,probability=True,class_weight={0:0.2,1:0.8}
).fit(X_train_pca,y_train)

y_pred_rbf = spt_vector_rbf.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'SVM-RBF',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_rbf),
    'F1':f1_score(y_test,y_pred_rbf),
    'Precision':precision_score(y_test,y_pred_rbf),
    'Recall':recall_score(y_test,y_pred_rbf)
}

# SVM Sigmoid
spt_vector_sigmoid = svm.SVC(
    kernel='sigmoid',degree=1,probability=True,class_weight={0:0.2,1:0.8}
).fit(X_train_pca,y_train)

y_pred_sigmoid = spt_vector_sigmoid.predict(X_test_pca)
test_metrics.loc[len(test_metrics)] = {
    'Model':'SVM-Sigmoid',
    'Useful Level':"above 0",
    'Accuracy':accuracy_score(y_test,y_pred_sigmoid),
    'F1':f1_score(y_test,y_pred_sigmoid),
    'Precision':precision_score(y_test,y_pred_sigmoid),
    'Recall':recall_score(y_test,y_pred_sigmoid)
}

# Display results
test_metrics_df = pd.DataFrame(test_metrics)
display(test_metrics_df[7:].style.hide(axis='index'))

fig, axes = plt.subplots(3, 1, figsize=(18, 6))

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_poly)).plot(ax=axes[0])
axes[0].set_title('SVM Poly Model')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rbf)).plot(ax=axes[1])
axes[1].set_title('SVM RBF Model')

ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_sigmoid)).plot(ax=axes[2])
axes[2].set_title('SVM Sigmoid Model')

plt.tight_layout()
plt.show()
```

Looking the results shown below we can say that:

1. SVM with a polynomial kernel (SVM-Poly) achieves the highest F1 score of approximately 81.09%. This model also exhibits perfect precision, indicating that it classifies all positive instances correctly.

2. SVM with a radial basis function kernel (SVM-RBF) follows closely with an F1 score of approximately 78.15%. Although it has slightly lower precision compared to SVM-Poly, it demonstrates a balanced trade-off between precision and recall. 

3. SVM with a sigmoid kernel (SVM-Sigmoid) exhibits the lowest F1 score of approximately 59.24%, indicating comparatively lower performance in capturing both precision and recall.

While all SVM models exhibit high accuracy, SVM with a polynomial kernel (SVM-Poly) emerges as the most favorable choice based on its highest F1 score and perfect precision. 

<!-- 
Is this what we want to say? if our goals are to instead identify potentially useful comments, not just predict whether or not it will have any votes against it, I think that the RBF or the sigmoid model may be best for our application

Here's an example - 

Here, the resuts of the sigmoid model are most promising for our outcomes, as the recall is high, and the model is predicting instances of potentially useful customer feedback comments that have no votes marking them as favorable.

Let's have a look at some of those predictions.

-->

```{python UsefulComments-svm-rbf}
# review_orig['pred_useful_prob'] = spt_vector_rbf.predict_proba(
#     X_test_pca
# )[:,1]
# review_orig['pred_useful'] = y_pred_rbf
# x = pd.DataFrame(review_orig[(review_orig['is_helpful']==0) &(review_orig['pred_useful']==1)].sort_values(by='pred_useful_prob',ascending=False)['review_content'])
# display(
#     x.head(10).style.hide(axis='index')
# )
```

RBF, unexpectedly, seems to associate being part of a promotion with a comment being useful. It's possible that other comments in this group have similar features and qualities to promotional review comments.  These results are sorted by the probability that they would be classified has having votes flagging them as being useful.  The highest 

What about sigmoid?
```{python UsefulComments-svm-sigmoid}
# review_orig['pred_useful_prob'] = spt_vector_sigmoid.predict_proba(
#     X_test_pca
# )[:,1]
# review_orig['pred_useful'] = spt_vector_sigmoid.predict(
#     X_test_pca
# )
# x = pd.DataFrame(review_orig[(review_orig['is_helpful']==0) &(review_orig['pred_useful']==1)].sort_values(by='pred_useful_prob',ascending=False)['review_content'])

# display(
#     x.head(10)
# )
# ##include product name?
```

Sigmoid seems to identify a wider variety of comments, some (subjectively) useful, some not, and identifies fewer items that were part of a promotion.

## Model Comparison

We examine the following table to compare and contrast our implemented models on our collected data.

```{python tbl-final-results}
#| label: tbl-final-results
#| tbl-cap: Summary Metrics (all evaluated models)
test_metrics.style.hide(axis='index')
```

Each of these models had similar performance in terms of accuracy and precision. A key consideration for us is within the realm of recall in that false positives are potentially beneficial to the generalization of this model to identify reviews that are useful, but currently possess no helpful votes.  None of the models delivered any false positives, but having high recall may support identification of new, useful comments.

The top 2 performing models were our Support Vector Machine and Logistic Regression implementations.  Between the two, SVM had higher recall, which is preferable in our use case.  That being said, on every metric between the two models, there was near equivalent performance.

With our preferences toward higher recall, the implementation of the SVM-Sigmoid and SVM-RBF models delivered best on our objectives, identifying the highest volume of true positives, minimizing false negatives, and providing a limited number of true positives that could identify comments with no votes that are useful to customers. 

From a stakeholder standpoint, these two models can both be beneficial, whether looking from the standpoint of the customer or the e-commerce platform.

The RBF model, with more exploration and examination, could be very beneficial to e-commerce platforms.  If the model could be modified and updated to identify comments that are closer in proximity to promotional comments (which potentially come at a cost to these platforms), it may allow them to present comments similar in quality to a paid promotional review without the platform having to pay for it.

The Sigmoid model, with additional training and tuning, could be of greater benefit to customers.  This model is less inclined to capture promomtional feedback comments and provide potentially useful feedback absent other outside influence.  At a subjective level, not all of the content contained in these classified reviews are useful, though many are. <!-- any additional writeup comments here-->