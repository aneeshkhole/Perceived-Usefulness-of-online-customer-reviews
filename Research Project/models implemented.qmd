# Models Implemented {#sec-models-implemented}

```{python importsBlock}
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import seaborn as sns
# import pingouin as pg
# from statsmodels.stats.multicomp import pairwise_tukeyhsd
# import statsmodels.api as sm
# from wordcloud import WordCloud
# import pingouin as pg 
from sklearn import tree
from sklearn.metrics import (
    accuracy_score,f1_score,
    precision_score,recall_score, 
    confusion_matrix, ConfusionMatrixDisplay,
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
import graphviz


product_data = pd.read_csv('../data/master_product_list.csv')
#review_data = pd.read_csv('../data/combined_review_table.csv')
review_data = pd.read_csv('../data/all_reviews_without_unicode.csv')
review_data_base = pd.DataFrame(review_data)
review_data = pd.DataFrame(
    review_data[
        (review_data['productID'].isin([1,2,3,5,6,8,9,11,12,13,14,15])) &
        (review_data['review_lang']=='en')
    ]
)
   
review_metrics = pd.read_csv('../data/combined_review_metrics.csv')

test_metrics = pd.DataFrame(
    {
        'Model':[],
        'Accuracy':[],
        'F1':[],
        'Precision':[],
        'Recall':[]
    }
)

```

Here's where we'll list out our models

<!--Note from Patrick: 


Started building some models based upon what's been taught in class.

The below are simple classification models, examining whether a review comment is or is not useful (added a boolean to say it's useful if it has one or more votes for being useful). 

May still need to look at MLR, haven't built or run it yet.  May be good to look at this in conjunction with the Logistic Regression.

Ran a decision tree

    * most impactful factors (in order) appear to be review star rating, review length, polarity

    * may need to include product subjectivity as well

Ran Logistic Regression (performs well)



Ran K-Nearest neighbors

Ran Support Vector Machine



Current modeling examines doing each of these algorithms against the items that match across all websites.

Each of these models runs may need to be modified such that they're also run against the external item(s).

Each of these models may also need some data modifications to include new variables, and to seek ways and means to avoid overfitting.

 -->

## Decision Tree
```{python data}
review_data_base.head()
review_data_base['is_helpful'] = review_data_base['review_helpful_votes'] > 0
X,y = [review_data_base[
    ['review_star_rating','verified_purchase','review_subjectivity',
    'review_polarity','review_length']
],review_data_base['is_helpful'].astype(int)]

t = pd.DataFrame(review_data_base.dropna())

```


```{python dTreeClassifier}


## research how to replace tree headers with
## names and values

X_train,X_test,y_train,y_test = train_test_split(
    X,y,test_size=0.2,random_state=1337
)
#clf = tree.DecisionTreeClassifier(criterion=)
clf = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=6)
clf=clf.fit(X_train,y_train)
#tree.plot_tree(clf)
dot_data = tree.export_graphviz(clf,out_file=None)
#graph = graphviz.Source(dot_data)
#display(graph)

y_pred = clf.predict(X_test)
test_metrics.loc[len(test_metrics)] = {
    'Model':'Decision Tree',
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()

display(test_metrics)

```

## Logistic Regression

```{python LogisitcRegressionClassifier}
### extract parameters...

mod = LogisticRegression()

#reduce the sample size to examine reduction in 
#overfit


X,y = [t[
    ['review_star_rating','verified_purchase','review_subjectivity',
    'review_polarity','review_length']
],t['is_helpful']]

X_train,X_test,y_train,y_test = train_test_split(
    X,
    y,
    test_size=0.8,
    random_state=1337
)

log_reg = mod.fit(X_train,y_train)
y_pred = mod.predict(X_test)

test_metrics.loc[len(test_metrics)] = {
    'Model':'Logistic Regression',
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()

display(test_metrics)
```


## KNN?

```{python KNearestNeighborsClassifier}
# X,y = [review_data_base[
#     ['review_star_rating','verified_purchase','review_subjectivity',
#     'review_polarity','review_length']
# ],review_data_base['is_helpful'].astype(int)]

# X_train,X_test,y_train,y_test = train_test_split(
#     X,
#     y,
#     test_size=0.8,
#     random_state=1337
# )
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)
test_metrics.loc[len(test_metrics)] = {
    'Model':'K-Nearest Neighbors',
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}
ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()

display(test_metrics)

```

## SVM Analysis

```{python SVMClassifier}
spt_vector = svm.SVC(kernel='linear').fit(X_train,y_train)

y_pred = spt_vector.predict(X_test)

test_metrics.loc[len(test_metrics)] = {
    'Model':'Support Vector Machine',
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()

display(test_metrics)
```

```{python SVMC2}
other_data = pd.DataFrame(review_data_base[(~review_data_base['productID'].isin([1,2,3,5,6,8,9,11,12,13,14,15]))&(review_data_base['review_lang']=='en')])
print(len(other_data))
X = other_data[
    ['review_star_rating','verified_purchase','review_subjectivity',
     'review_polarity','review_length']
]
y_test = other_data['is_helpful']
y_pred = spt_vector.predict(X)

test_metrics.loc[len(test_metrics)] = {
    'Model':'Support Vector Machine2',
    'Accuracy':accuracy_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred)
}

ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()

display(test_metrics)
```

## Examined Model - Multiple Linear Regression

### Base Model 

### Model Modifications

## Examined Model - Decision Tree

## Examined Model - Naive Bayes

## Examined Model - ?

<!--* Perceived Usefulness of Product Reviews

    - Modeling for search vs. experience goods as "mitigations" on reviews, examining their impact on perceived review usefulness

* Wealth of Data Paper on Customer Evaulation

    - Examination of 'Vine' customers from Amazon

    - Don't think that BestBuy or Target have the same.

    - Scope of this would be limited to Amazon, combining research from Perceived Usefulness paper (i.e. their modeling) with a customer "trustworthiness" score for Amazon Vine customers

        - May have challenges identifying products with Vine customers

        - Should also have a look at "verified purchase" customers, too.

    - Weaknesses (from authors)

        - Their NLP instance wasn't well trained for fully accurate

        - Their weighting system was manually (i.e. arbitrarily) set as opposed to having a labeled dataset

        - Can we fix these problems? If so - what data is needed?

    

## Potential Model for Search vs. Experience Weight (scale of 0-1)

### Jaccard Similarity Measure

https://www.sciencedirect.com/science/article/pii/S1567422318300450

* Can be used to produce a similarity between two items on a scale of 0 to 1

* May be able to use this for evaluation of a novel item against a pure-search good vs. a pure-experience good.

    * May require additional calculation / computation between these two values - maybe its arithmetic or harmonic mean between search and experience

    * Using that, we could potentially produce a weight.

    * Authors models

        * Model 1: $\text{Perceived Usefulness} = \beta_0 + \beta_1 \cdot \text{Review Content} + \beta_2 \cdot \text{Review Length} + \beta_3\cdot \text{Star Rating} + \beta_4 \cdot \text{Total Votes Received} + \epsilon_1$

        * Model 2 (product type as a moderator): $\text{Perceived Usefulness} = \beta_0 + \beta_1 \cdot \text{Review Content} + \beta_2 \cdot \text{Review Length} + \beta_3\cdot \text{Star Rating} + \beta_4 \cdot \text{Total Votes Received} + \beta_5 \cdot \text{Digital Music} + \beta_6 \cdot \text{Video Game} + \beta_7 \cdot \text{Review Content}\cdot \text{ Digital Music} + \beta_8\cdot\text{Review Content}\cdot\text{ Video Game} + \epsilon_2$

        * Our proposition #1 (may require some modification)
$$
\text{Perceived Usefulness} = \beta_0 + \beta_1 \cdot \text{Review Content} + \beta_2 \cdot \text{Review Length} + \beta_3\cdot \text{Star Rating} + \beta_4 \cdot \text{Total Votes Received} + \gamma\cdot\text{Review Content}
$$

or

$$
\text{Perceived Usefulness} = \beta_0 + \gamma\cdot\beta_1\cdot\text{Review Content} + \beta_2 \cdot \text{Review Length} + \beta_3\cdot \text{Star Rating} + \beta_4 \cdot \text{Total Votes Received}
$$

* Where $\gamma$ is the Jaccard Similarity Score between a given product and elements we are identifying as "pure" experience and "pure" search good.

* Operationalizaton of Jaccard Similarity score Variable (i.e. inputs)

    * Search good: those with attributes that can be evaluated prior to purchase or consumption. Consumers rely on prior experience, direct product inspection and other information search activities to locate information that assists in the evaluation process. Most products fall into the search goods category (e.g. clothing, office stationery, home furnishings).

    * Number of measurement specifications?

    * comment/review information?

    * What else could we gather that could be considered a "universal" tangible or intangible feature from a product online?  They need to be applicable to both search and experience goods.

* Experience good: those that can be accurately evaluated only after the product has been purchased and experienced. Many personal services fall into this category (e.g. restaurant, hairdresser, beauty salon, theme park, travel, holiday).

    * subjective descriptiveness vs. 

    * comment/review information?

* Are there other methods aside from Jaccard Similarity? -->

## Model Comparison

