# Introduction {#sec-intro}

```{python}
#print("hello world!")
import pandas as pd, numpy as np, matplotlib.pyplot as plt
#import scipy as sp
#import chart_studio.plotly as py
#from plotly.offline import iplot, init_notebook_mode
#import plotly.graph_objects as go
```

## Concept and Motivation
<!--PC-->
Customers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product.  According to research by @percUse, if a customer can gather and understand product quality before the purchase, it is considered a search good, while experience goods are those which must be purchased or experienced to evaluate them.  When a product is more in the directon of experience vs. search-based, other customers' experiences can shed light on its features and return on investment than information directly from the vendor can.  Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.

We seek to expound upon the research of [@percUse] to explore additional recommended research areas to improve upon and increase the general applicability of the model.

## What / Where
<!-- -->
@percUse provided the following areas for recommended additional research at the conclusion of their paper: <!--PC-->

1. Expand the number of products beyond 3 items (one search, one experience, one mixed) to better generalize the model.

1. Explore customer or reviewer metadata for classifying reviewer types to enhance model performance.

<!--Team - I don't know that we can actually go about getting customer metadata.  This may be a bridge too far for us unless the site(s) we identify can provide such information--> 

We seek to examine the above two above items, and to explore the possibility of assessing a scale for products to determine the extent to which they are a search or experience-based product.  We further seek to inspect additional potential modifiers to the underlying model for statistical and operational applicability; we've sought out work from other research teams to identify potential methods we can leverage to pursue these ends.

* Determining the polarity of a customer review by employing a classifier such as Naive Bayes. 

* Using Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.

* Attempting to assess the reliability of a customer's review based on star-rating and a 'sentiment score' of their textual feedback.  

Exploring methods employed within each of combinations of these research methods, we will pursue potential improvements on the models outlined in @percUse.  We will examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon).

## Why It Matters

<!--PC--> 

Feedback from customers can be beneficial to both vendors and consumers, but it is not always ordered by the most informative or beneficial feedback first.  Certain features of products, reviews, and reviewers (such as reviewer reliability, review quality, product quality, specificity and detail of the product, amongst others) can impact the usefulness of the feedback on a customer-by-customer basis.  Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers.  Leveraging metrics and data associated with a product, a review, and a reviewer together may allow for online vendors to improve consumer e-commerce experience, support identificaion of issues with product quality and sales, and enable vendors to adjust practices in product marketing, inventory, and manufacture.

Examining additional product types could support a generalization of the authors' methodology to other products.  Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals.  Producing a reliable scale and methods for classifying a products' degree of being experienced-based can inform vendors on:

* How to best sort product reviews

* Examine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment

* Adjust the product, its marketing, or future production based upon market efficacy

<!--not sure that this fits here...is this a separate paragraph talking about what else we're trying to do? Or do we re-word?-->
<!-- 

-->
* Understand the emotions a customer wants to express through a review is crucial as it will affect the "recommendation score" of that particular product or a different one from a similar category. <!--not sure that this fits here...-->

    * To contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.
    
    * Typically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. 
  <!--AK talk about Naive Bayes, Kansei Method--> 

* Determine which customer is trustworthy, meaning who has actually purchased the product versus a customer who gave a false review. Based on the ‘customer reputation score’, our aim is to classify customers into groups to judge reviewer reliability. This has two main aspects:

    * Star-rating score which is a discrete scale that tells the inclination of a customer.
    
    * Text review ‘sentiment score’ using NLP that explains customer opinions based on words.
    <!--UK - talk about Customer Reliability methods--> 

## Literature Survey

* **Paper(s) on user/consumer/reviewer classification**
<!--PC, UK, AK-->
* **All papers you've found, provide a summary of what they did and any key results** <!-- note: all of these are now listed in the refs.bib file and can be referenced using the examples below.  We can add more references as needed!-->

    * @Hu_Gong_Guo_2010 <!--Hu, W., Gong, Z., & Guo, J. (2010). Mining Product Features from Online Reviews. 2010 IEEE 7th International Conference on E-Business Engineering. doi:10.1109/icebe.2010.51.--> 
    
        * The proposed system employs a two-step process for opinion mining: identifying opinion sentences using a SentiWordNet-based algorithm and extracting product features from all reviews in the database. This feature extraction function focuses on identifying commonly expressed positive or negative opinions before extracting explicit and implicit product features.

    * @Rajeev_Rekha_2015 <!--Rajeev, P. V., & Rekha, V. S. (2015). Recommending products to customers using opinion mining of online product reviews and features. 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]. doi:10.1109/iccpct.2015.7159433. --> 
        
        * This paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.  

    * @Wang_Li_Tian_Wang_Cheng_2018 <!--Wang, W. M., Li, Z., Tian, Z. G., Wang, J. W., & Cheng, M. N. (2018). Extracting and summarizing affective features and responses from online product descriptions and reviews: A Kansei text mining approach. Engineering Applications of Artificial Intelligence, 73, 149–162. doi:10.1016/j.engappai.2018.05.005.-->

        * Authors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, POS tagging

## Research Questions

* Can the model from @percUse be generalized with

    - larger volume of products and product types from which to mine data?

    - a sliding scalar multiplier representing the degree to which a product is a "search" (0) or "experience" (1) product?

    - Adding modifiers to review content based upon:

        * Customer / Reviewer Reliability?

* Can the polarity of reviews be judged accurately by using a Naive Bayes classification model? @Hu_Gong_Guo_2010

    - What is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model? @Wang_Li_Tian_Wang_Cheng_2018

* Can products be classified on their degree of being search or experience based by examining product variables such as:

    * Degree of specificity in the product description? (e.g. level of detail, length, numeric values, descriptive values may suggest the product is more search than it is experience-based)

    * Whether the product is offered in brand-new condition only, or offered as new, used, or refurbished? (e.g. refurbished products may be more search products than they are experience products)

    * Which of the 5 senses the product engages? (e.g. engagement of more senses, or engagement of solely specific senses like hearing and vision may suggest more experience-based than search based; examine relationship between search and experience vs. senses engaged)

    * Item rarity (limited production or unique items vs. bulk-produced items)? (e.g. limited production products may be more experience-based than search-based)

* Can newer natrual language processing libraries provide a better fit for @percUse 's Review Content metrics?

* How does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product? @
<!-- AK 2 research questions -->

* Can we categorize customer reviews based on customer experience and sentiment?

* Do specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?

* Are specific quality descriptors in text-based reviews (e.g., 'enthusiastic', 'disappointed') strongly associated with certain rating levels, and how does this association affect product reputation?

<!-- UK 2 research questions -->

## Goals / Definition of Success

<!-- --> 

* Replicate similar results to @percUse with similar product types

* Expound upon @percUse with additional products, including:

    a. Original products from (paper): Digital Music, Video Game, and Grocery Item

    b. Additional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies

    c. Additional Proucts (Amazon, Target, BestBuy): Electronics

    d. Verify goodness of fit of original model

* Determing best metrics and/or modifiers for Review Content and Customer Reliability

* Achieving similar or better fit than original paper's modeling; extrapolate to other product types.

<!-- Adding success for other research questions --> 
* Determining strength of correlation metrics (support, confidence, lift) between Naive Bayes' classifier for review polarity @Hu_Gong_Guo_2010

    * Integrate with the model and test if Naive Bayes shows strong correlation metrics.

    * Compare and contrast the model with and without incorporation.

<!--NOTE: Need success metrics/plan for sentiment-->

## Project Schedule / Timeline

Below in @tbl-task-list, we lay out the major tasks, deliverables, and their respective due dates for this effort.  

```{python taskListing}
#| tbl-cap: Major Project Tasks
#| label: tbl-task-list

#note to reference a table or a figure in the doc - you do @label

task_list = [
    {'Task':'Milestone 1 Submission','Due Date':''},
    {'Task':'Product Identification and Selection','Due Date':''},
    {'Task':'Vendor Identification and Selection','Due Date':''},
    {'Task':'Data Collection','Due Date':''},
    {'Task':'Data Cleaning/Pre-Processing','Due Date':''},
    {'Task':'Review Classification (Naive Bayes, Sentiment, Kansei)','Due Date':''},
    {'Task':'Product Classification','Due Date':''},
    {'Task':'Milestone 2 Submission','Due Date':''},
    {'Task':'Exploratory Data Analysis','Due Date':''},
    {'Task':'Modeling - Search vs. Experience','Due Date':''},
    {'Task':'Model Reproduction - Original Paper','Due Date':''},
    {'Task':'Model Analysis for Selection','Due Date':''},
    {'Task':'Model Testing:','Due Date':''},
    {'Task':'Milestone 3 Submission','Due Date':''},
    {'Task':'Milestone 4 Submission','Due Date':''},
]

tbl_proj_tasks = pd.DataFrame(
    task_list
)
tbl_proj_tasks.set_index('Task')
tbl_proj_tasks.style.hide(axis='index')

```

<!--

This is where our abstract will go.

Here's an example of inserting a plot directly into our paper

```{python}
#| label: fig-a
#| fig-cap: a matplotlib plot
x = np.linspace(-5,5)
y = x**2
plt.scatter(
    x,y
)
```

Here's an example of pulling some data in from our github repo

```{python}
init_notebook_mode(connected=True)

#reference data directly that sits in the github repo
temp = pd.read_csv('../data/Fatalities.csv')

temp['ST'] = temp['state'].str.upper()

#add the ratio column to examine the plot by ratio of alcohol fatalitites to all fatalities
temp['afatal_ratio'] = temp['afatal'] / temp['fatal']

fig = go.Figure()

#collection point for data to be held from each slider position
slider_info = []

#slice the data by year 
for y in range(temp['year'].min(),temp['year'].max()+1):
    #get the current year's data
    year_data = temp[temp['year']==y]

    #build a dictionary to house the data for the year
    #we specify "choropleth" to feed the dictionary to plotly
    #when we have a slider change
    dict_year_data = {
        'type':'choropleth',
        'locations':year_data['ST'], #we want information by-state
        'z':year_data['afatal_ratio'], #plot the ratio of alcohol fatalities to all fatalities.
        'locationmode':'USA-states', #look at the US
        'colorbar':{'title':'alcohol-related fatalities (%)'} #label the color axis
    }
    #collect all the data to add to our slider information
    slider_info.append(dict_year_data)

steps = []
for i in range(len(slider_info)):
    step = dict(
        method='restyle',
        args=['visible',[False]*len(slider_info)],
        #start the slider index at 1982
        label='{}'.format(i+1982)
    )
    step['args'][1][i] = True
    steps.append(step)

sliders = [dict(active=len(steps)-1,pad={"t":1},steps=steps)]

layout = dict(
    title='Alcohol-Related Fatality Rate 1982-1988',
    geo=dict(scope='usa',projection={'type':'albers usa'}),
    sliders=sliders,
    width=1000,
    height=500
)

fig = dict(data=slider_info,layout=layout)

iplot(fig)

```-->