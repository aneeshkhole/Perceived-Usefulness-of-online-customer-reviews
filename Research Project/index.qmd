# Introduction {#sec-intro}

```{python}
#print("hello world!")
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import scipy as sp
#import chart_studio.plotly as py
from plotly.offline import iplot, init_notebook_mode
import plotly.graph_objects as go
```

## Concept and Motivation
<!--PC-->
Customers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product.  *(Information on search products vs. experience vs. mixed).*  When a product is more in the directon of experience vs. search-based, other customers' experiences can shed light on its features and return on investment than information directly from the vendor can.  Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.

We seek to expound upon the research of *paper reference here* to explore additional recommended research areas, in the direction

## What / Where
<!-- -->
*(Citation)* provided the following areas for recommended additional research at the conclusion of their paper: <!--PC-->

* Expand the number of products beyond 3 items (one search, one experience, one mixed)

* Explore customer metadata for classifying reviewer types <!--Team - I don't know that we can actually go about getting customer metadata.  This may be a bridge too far for us unless the site(s) we identify can provide such information--> 

We seek to explore Item #1 and #2 above and explore the possibility of assessing a scale for products to determine the extent to which they are a search-based or an experience-based product. We've explored additonal research from other teams to identify additional potential methods

* Determine the polarity of a customer review by employing a classifier such as Naive Bayes. <!--AK talk about Naive Bayes, Kansei Method-->

* Use Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.  

* <!--UK - talk about Customer Reliability methods--> 

We seek to examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon).

## Why It Matters

<!--PC--> 

Feedback from customers is beneficial, but it is not always ordered by the most informative or beneficial feedback first.  Certain features of data such as... can impact the usefulness of the feedback on a customer-by-customer basis.  Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers.  Were e-commerce 

Examining additional product types can enable the generalization of the authors' methodology to other products.  Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals.  Producing a reliable scale and methods for classifying a products' degree of being experienced-based can inform vendors on:

* How to best sort product reviews

* Examine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment

* Adjust the product, its marketing, or future production based upon market efficacy.

* Understanding the emotions a customer wants to express through a review is crucial as it will affect the "recommendation score" of that particular product or a different one from a similar category.

    * To contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.
    
    * Typically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. 
  <!--AK talk about Naive Bayes, Kansei Method--> 

* <!--UK - talk about Customer Reliability methods--> 

## Literature Survey

* **Additional commentary on original paper here**

* **Paper(s) on product classification (search-experience-mixed)**

* **Paper(s) on user/consumer/reviewer classification**
<!--PC, UK, AK-->
* **All papers you've found, provide a summary of what they did and any key results**

    * Hu, W., Gong, Z., & Guo, J. (2010). Mining Product Features from Online Reviews. 2010 IEEE 7th International Conference on E-Business Engineering. doi:10.1109/icebe.2010.51
    
        

    * Rajeev, P. V., & Rekha, V. S. (2015). Recommending products to customers using opinion mining of online product reviews and features. 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]. doi:10.1109/iccpct.2015.7159433 
        
        * This paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.  

    * Wang, W. M., Li, Z., Tian, Z. G., Wang, J. W., & Cheng, M. N. (2018). Extracting and summarizing affective features and responses from online product descriptions and reviews: A Kansei text mining approach. Engineering Applications of Artificial Intelligence, 73, 149–162. doi:10.1016/j.engappai.2018.05.005

        * Authors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, POS tagging



## Research Questions

* Can the model from *(original paper citation)* be generalized with

    - larger volume of products and product types?

    - a sliding scale weight for degree to which a product is a "search" (0) or "experience" (1) product?

    - Adding modifiers to review content based upon:

        * Customer / Reviewer Reliability?

    - Can the polarity of reviews be judged accurately by using a Naive Bayes classification model? 

        - What is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model?
     
    <!--AK add bullet(s) on Naive Bayes classifier-->

* Can products be classified on degree of search or experience *(need other research that can tell us what features of a product determine what makes a product a search vs. experience product)* using:
    <!--PC --->

    * Which of the 5 senses the product engages?

    * 

<!-- 

NEW: true/false.

Discounted / lower price?

Fiction/Non-Fiction book concept?

Rarity: 1-of-a-kind or limited production, vs. bulk produced?

Factory Produced: T/F

Environmentally Friendly? T/F

Touch: t/f

Hear: t/f 

Smell: t/f 

See: t/f (is the primary purpose to look at the item)

Taste: t/f 

ALL (): please list your ideas for true/false variables here

-->

    a. Feature 1

    b. Feature 2

    c. Some combination of features?

* Can different natrual language processing libraries provide a better fit for the Review Content metric itself?

* How does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product?
<!-- AK 2 research questions -->

* <!-- UK 2 research questions -->

## Goals / Definition of Success

<!-- --> 

* Replicate similar results to *(original paper citation)* with similar product types

* Expound upon *(original paper citation)* with additional products, including:

    a. Original products from (paper): Digital Music, Video Game, and Grocery Item

    b. Additional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies?

    c. Additional Proucts (Amazon, Target, BestBuy): Electronics.

    d. Verify goodness of fit of original model

* Determing best metrics and/or modifiers for Review Content and Customer Reliability

* Achieving similar or better fit than original paper's modeling; extrapolate to other product types.

<!-- * Perform analysis of multiple product types to produce a weighting for degree of how experienced-based the product is.

* Assess weighting of products in terms of degree of "experience" base, and use an updated formula on

    a. Original Model

    b. Updated Model

    c. Extrapolation on data outside the models.

* Success of our effort and this methodology is met if it produces as-good or better performance of the experience weight-based model vs. the original model from *(original paper citation here)* -->







This is where our abstract will go.

Here's an example of inserting a plot directly into our paper

```{python}
#| label: fig-a
#| fig-cap: a matplotlib plot
x = np.linspace(-5,5)
y = x**2
plt.scatter(
    x,y
)
```

Here's an example of pulling some data in from our github repo

```{python}
init_notebook_mode(connected=True)

#reference data directly that sits in the github repo
temp = pd.read_csv('../data/Fatalities.csv')

temp['ST'] = temp['state'].str.upper()

#add the ratio column to examine the plot by ratio of alcohol fatalitites to all fatalities
temp['afatal_ratio'] = temp['afatal'] / temp['fatal']

fig = go.Figure()

#collection point for data to be held from each slider position
slider_info = []

#slice the data by year 
for y in range(temp['year'].min(),temp['year'].max()+1):
    #get the current year's data
    year_data = temp[temp['year']==y]

    #build a dictionary to house the data for the year
    #we specify "choropleth" to feed the dictionary to plotly
    #when we have a slider change
    dict_year_data = {
        'type':'choropleth',
        'locations':year_data['ST'], #we want information by-state
        'z':year_data['afatal_ratio'], #plot the ratio of alcohol fatalities to all fatalities.
        'locationmode':'USA-states', #look at the US
        'colorbar':{'title':'alcohol-related fatalities (%)'} #label the color axis
    }
    #collect all the data to add to our slider information
    slider_info.append(dict_year_data)

steps = []
for i in range(len(slider_info)):
    step = dict(
        method='restyle',
        args=['visible',[False]*len(slider_info)],
        #start the slider index at 1982
        label='{}'.format(i+1982)
    )
    step['args'][1][i] = True
    steps.append(step)

sliders = [dict(active=len(steps)-1,pad={"t":1},steps=steps)]

layout = dict(
    title='Alcohol-Related Fatality Rate 1982-1988',
    geo=dict(scope='usa',projection={'type':'albers usa'}),
    sliders=sliders,
    width=1000,
    height=500
)

fig = dict(data=slider_info,layout=layout)

iplot(fig)

```