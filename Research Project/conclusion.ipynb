{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion {#sec-conclusions}\n",
        "\n",
        "\n",
        "<!-- #module imports\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "lg_pred = pd.read_csv('../data/LogisticRegressionPredUseful.csv')\n",
        "mlr_pred = pd.read_csv('../data/MLRPredUseful.csv')\n",
        "svm_pred = pd.read_csv('../data/SVMPolyPredUseful.csv')\n",
        "x = lg_pred.merge(right=mlr_pred[['review_id','rank']],how='inner',on=['review_id'],suffixes=['_lg','_mlr'])\n",
        "x = x.merge(right=svm_pred[['review_id','rank']],how='inner',on=['review_id'],suffixes=['_svm'])\n",
        "x.rename({'rank':'rank_svm'},inplace=True,axis=1)\n",
        "from sklearn.metrics import r2_score -->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Evaluating the performance our models in comparison to the source data, we find that our selected features are likely weak in the prediction of a feedback comment being useful.  There are substantial issues with recall, precision, and F1 scores for both the training and testing data.\n",
        "\n",
        "We attempted multiple iterations of examining our data, examining models and how to adjust them, and how to optimize performance.  Ultimately, our work did not deliver a system that could definitively be used to predict usefulness of a review.\n",
        "\n",
        "Despite of that, we were able to greatly answer the research questions that were initially laid out by us. This study can prove to be a basis of further examinations into how to better classify sentiment and assess the usefulness of reviews based on customer experience.\n",
        "\n",
        "<!-- * We leveraged python's TextBlob as a means to gather the subjectivity and polarity.  Other libraries and means for assessing compound sentiment may be more prudent.  Due to resource constraints, we were unable to explore some methods suggested by:\n",
        "\n",
        "    * @Hu_Gong_Guo_2010 for mitigating common positive and negative sentiment prior to computing sentiment scores\n",
        "\n",
        "    * @Rajeev_Rekha_2015 for exploring Naive Bayes classification of review polarity\n",
        "\n",
        "    * @Wang_Li_Tian_Wang_Cheng_2018 for text mining methods to further support feature extraction\n",
        "\n",
        "Taking our methods and applying some of the above could greatly support stronger features and model performance.\n",
        "\n",
        "-->\n",
        "\n",
        "\n",
        "<!-- TEAM - LET'S ANSWER ALL THIS IN A NON-TECHNICAL MANNER -->\n",
        "\n",
        "## Review of Research Questions\n",
        "\n",
        "At the start of this effort, we set out to answer the following questions:\n",
        "\n",
        "* Can the model from @percUse be generalized with\n",
        "\n",
        "    - **larger volume of products and product types from which to mine data?**  \n",
        "        - We find that a larger mix of products across multiple platforms does support a wider-reaching and more generalizable model.  Their model, with some adaptations, was able to identify (X) useful comments.  However, the methods we used may not be fully appropriate or have sufficient precision and recall to establish confidence in other models. We implemented Multiple Linear Regression, Logistic regression, K-Nearest-Neighbours and Support Vector Machine models. \n",
        "        - We ascertained that the Linear Regression model used in @percUse could not capture the underlying patterns in our case from the larger volume of products. It is found that the results from the tuned Logistic Regression and SVM-Poly models which have credible scores upto a certain threshold. These models were able to capture insights from the large data which was not seen from the logistic regression model\n",
        "\n",
        "    - **a sliding scalar multiplier representing the degree to which a product is a \"search\" (0) or \"experience\" (1) product?**  \n",
        "        It was found to be a useful strategy to use a sliding scalar multiplier to define the range of \"search\" and \"experience\" products. We successfully captured the essence of whether a product leans more toward being a simple \"search\" item or a complex \"experience\" one by employing a metric that gauges the level of subjectivity within product descriptions. By using the textblob library to compute subjectivity scores, we have been able to draw significant conclusions about how specific and concrete product descriptions are. This approach provides a useful way to comprehend and classify products across the consumer goods spectrum, facilitating more perceptive analysis and strategic decision-making across a range of industries.\n",
        "\n",
        "    - **Adding modifiers to review content based upon:**\n",
        "\n",
        "        * Customer / Reviewer reliability and reputation? \n",
        "        \n",
        "        This question could not be answered from our analysis as for all the websites - Amazon, Target and BestBuy, only a verified buyer was allowed to review a product. Due to this factor, we were forced to accept that every reviewer was reliable. As going around this was to go through eac and every review looking for jargon content which was not feasible.\n",
        "\n",
        "        * Review Polarity? \n",
        "\n",
        "        Positive, negative and neutral reviews all offer insightful information about the varying subjective experiences that people have with a product. Positive reviews are classified by positive attitudes and recommendations, and they are usually linked to products that provide remarkable experiences or successfully meet particular needs. On the other hand, unfavorable reviews could indicate shortcomings, contradictions, or mismatches between what customers expect from a product and how well it performs.\n",
        "\n",
        "* Can the polarity of reviews be judged accurately by using a Naive Bayes classification model? @Hu_Gong_Guo_2010\n",
        "\n",
        "    - **What is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model? @Wang_Li_Tian_Wang_Cheng_2018**\n",
        "\n",
        "        While formulating the problem statement, we listed using Naive Bayes classification for possible solutions but did not explore them in our analysis.\n",
        "\n",
        "* Can products be classified on their degree of being search or experience based by examining product variables such as:\n",
        "\n",
        "    * **Degree of specificity in the product description? (e.g. level of detail, length, numeric values, descriptive values may suggest the product is more search than it is experience-based)**  \n",
        "\n",
        "        - Products with extremely detailed descriptions typically offer precise and comprehensive details about their characteristics, features, and functionalities. Precise measurements, technical details, and explicit information about the attributes of the product are frequently included in these descriptions. The focus on specificity makes it easier for customers to look for and assess products according to their own requirements, tastes, and standards. Consequently, goods that have extremely detailed descriptions are usually linked to the \"search\" category, as consumers can readily find and assess them based on objective criteria.\n",
        "        - So indeed, depending on factors like the level of specificity in the product description, products can be categorized as \"search\" or \"experience\" in varying degrees. \n",
        "\n",
        "    * **Whether the product is offered in brand-new condition only, or offered as new, used, or refurbished? (e.g. refurbished products may be more search products than they are experience products)**  \n",
        "    \n",
        "        While we lacked comprehensive data on this aspect, generally, products offered in brand-new condition may incline more towards the search-based category, whereas items available in used or refurbished conditions could incline towards being more experience-based due to the potential history of priduct use associated with them.\n",
        "\n",
        "    * **Which of the 5 senses the product engages? (e.g. engagement of more senses, or engagement of solely specific senses like hearing and vision may suggest more experience-based than search based; examine relationship between search and experience vs. senses engaged)**  \n",
        "    \n",
        "        While the engagement of the senses is undoubtedly a crucial aspect of the consumer experience, it was not within the scope of our current investigation. However, examining the relationship between the engagement of the senses and the classification of products as \"search\" or \"experience\" goods could yield valuable insights\n",
        "\n",
        "    * **Item rarity (limited production or unique items vs. bulk-produced items)? (e.g. limited production products may be more experience-based than search-based)** \n",
        "\n",
        "        Most of the items in our research were common household products i.e. bulk-produced items. We did not give sufficient research time to dig into limited production or unique items. Solely based on human psychology, it might be possible that limited production or unique items typically exhibit more experiential qualities, as they offer distinctive and potentially rare experiences compared to bulk-produced items.\n",
        "\n",
        "* **Can newer natrual language processing libraries provide a better fit for Review Content metrics examined by @percUse?** \n",
        "\n",
        "    We found that there was no clear linear model from the data which could be an outcome of we how we processed sentiment as a numeric variable as opposed to a category. Based on this assumption, newer natural language processing libraries might provide an improved fit for analyzing review content metrics like sentiment. For instance, using these libraries could involve more nuanced sentiment analysis techniques that avoid treating sentiment solely as a numeric variable, potentially providing clearer insights into customer feedback.\n",
        "\n",
        "* **How does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product?**  \n",
        "    Sentiment in customer reviews, characterized by higher star ratings, tends to correlate positively with customer satisfaction metrics. Higher star ratings often reflect stronger polarity and subjectivity in sentiment, indicating greater overall satisfaction of the customer and potentially influencing purchasing decisions for future buyers.\n",
        "\n",
        "\n",
        "* **Can we categorize customer reviews based on customer experience and sentiment?**  \n",
        "\n",
        "    Our study results show that it is possible to categorize customer reviews based on customer experience and sentiment. One way of doing this is using machine learning algorithms like classifiers that inculcate subjectivity and polarity as model features. The results of the classifiers implemented in this project might not be robust but can act as a base for methodological adaptations to better classify sentiment and assess the usefulness of reviews based on customer experience.\n",
        "\n",
        "\n",
        "* **Do specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?** (we didn't get to dig in on reputation.  We should run some quick numbers on total reviews based on how many star ratings and average star rating and do a write-up here...)\n",
        "\n",
        "* **Are specific quality descriptors in text-based reviews (e.g., 'enthusiastic', 'disappointed') strongly associated with certain rating levels, and how does this association affect product reputation?** \n",
        "\n",
        "    Our findings suggest a positive trend between star ratings and positive polarity in reviews. In review content, specific quality descriptors like 'enthusiastic' or 'disappointed' seem to be associated with certain rating levels. However, further analysis is needed to assess how these associations impact overall product reputation, including how sentiment and specific descriptors contribute to consumer perceptions and purchasing decisions.\n",
        "\n",
        "\n",
        "## Interesting Findings, In Spite of Model Performance\n",
        "\n",
        "Despite what we perceive as weakness in the models, the degree of detail and specificity within some of the predicted \"useful\" comments is enlightening.  Let's have a look at a small selection of comments, near the top of the prediction list for MLR, SVM, and for Logistic Regression - \n",
        "\n",
        "![Comment on Dyson Vacuum Cleaner - predicted useful by Logistic Regression](./imgs/Comment.jpg){#fig-dyson-vacuum}\n",
        "\n",
        "A Dyson vacuum is an expensive product.  Hearing of someone's challenges, particularly an extreme case of having a vacuum *melt* on the user, may make one think twice about the investment.  While appearing negative in sentiment, the commenter appears to provide a degree of objectivity to their review - and does talk about both the good and the bad of the product.  These are all things we believe may be useful to a customer.\n",
        "\n",
        "![Comment on a Sofa - predicted useful by SVM](./imgs/Comment2.jpg){#fig-cheap-sofa}\n",
        "\n",
        "An approximately $400 sofa is also quite the investment, and knowing about issues with returns, issues with the product and its craftsmanship is likely a useful datapoint for a prospective customer.\n",
        "\n",
        "![Comment on a Pizza Oven - predicted useful by MLR](./imgs/Comment3.jpg){#fig-pizza-oven}\n",
        "\n",
        "This customer looks like they had a good experience - so good in fact that they went online to a commerce platform that they didn't even use to purchase the item, just so that they could post about their experience.  Knowing the settings used, how quickly the oven managed to cook their food, and the ease of use likely all give utility to a prospective customer.\n",
        "\n",
        "None of these reviews had any votes for the comment as being useful to other buyers.  That being said - it's all subjective in terms of determining utility.\n",
        "\n",
        "<!-- Another interesting outcome from this research is the correlation of predictions between our tuned SVM and Logistic Regression predictions.  For each prediction, we assigned it a rank based upon how likely the commment is to be useful as predicted by both models. -->\n",
        "\n",
        "\n",
        "<!--"
      ],
      "id": "508af253"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#| label: fig-SVM-LG-corr\n",
        "#| fig-cap: Examining rank predictions, SVM and Logistic Regression\n",
        "sns.scatterplot(\n",
        "    data=x,\n",
        "    x='rank_lg',\n",
        "    y='rank_svm'\n",
        ")\n",
        "plt.title(\"Correlation between Logisitic Regression and SVM Predictions\")\n",
        "plt.xlabel(\"Log Reg Predicted Rank\")\n",
        "plt.ylabel(\"SVM Predicted Rank\")\n",
        "plt.show()\n",
        "```-->\n",
        "\n",
        "<!-- The predictions had a correlation coefficient of `{python} round(r2_score(x['rank_lg'],x['rank_svm']),2)`\n",
        "\n",
        "The strength of this correlation is interesting and suggests the two models seek out similar results, as also shown within the examination of some of the top comments recommended by both.  Logistic Regression provided stronger scoring for all metrics (accuracy, F1, precision, and recall) compared to SVM-Poly, though the metrics were all quite similar. -->"
      ],
      "id": "14e96ae3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}