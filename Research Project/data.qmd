# Data Collection and Exploration {#sec-data}

```{python}
#module and data imports
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import seaborn as sns
import pingouin as pg
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import statsmodels.api as sm
from wordcloud import WordCloud
import pingouin as pg 

product_data = pd.read_csv('../data/master_product_list.csv')
#review_data = pd.read_csv('../data/combined_review_table.csv')
review_data = pd.read_csv('../data/all_reviews_without_unicode.csv')
review_data_base = pd.DataFrame(review_data)
review_data = pd.DataFrame(
    review_data[
        (review_data['productID'].isin([1,2,3,5,6,8,9,11,12,13,14,15])) &
        (review_data['review_lang']=='en')
    ]
)
   
review_metrics = pd.read_csv('../data/combined_review_metrics.csv')
```

## Data Collection Overview

The original efforts by @percUse selected three products, all listed on Amazon for sale.  In our efforts, we leveraged python Selenium, urllib, and Beautiful Soup to scrape data from 20 different products across multiple websites (Amazon, BestBuy, and Target).  Where possible, we sought to collect the exact same 20 products from each site and customer feedback associated with each.

As part of collection, to the greatest extent we were able, we cleaned information *during* the scraping process.  Doing this enabled us to have minimal cleaning efforts after collection.  Post collection, remaining items such as handling and removing special characters, unicode characters, addressing customer reviews written in foreign languages, and addressing misspellings remained necessary.

In terms of simplicity for scraping our data, we manually identified a list of products from each of the aforementioned sites.  Our team divided responsibilities to produce scraping code customized for each of the three websites.

## Data Collection Details

In collecting our data, in order to adhere to the model implemented by @percUse, we required the following data points: 

```{python dataReqsProducts}
#|tbl: tbl-data-reqs-products
data_reqs_product = {
    'Variable':['Product Title','Product Category*','Product Details/Specs', 'Product Cost'],
    'Data Type':['string','string','string','float']
}
tbl_data_reqs = pd.DataFrame(data_reqs_product)
tbl_data_reqs.set_index('Variable')
tbl_data_reqs.style.hide(axis='index')
```

For the product category variable - we may add our own manual categorization.  @percUse manually set the value for this variable.  Part of the intent of our research is to seek out means and methods to replace this variable with a continuous scale (ranging from 0 for a "search" good, to a 1 for an "experience" good).  

As an initial proxy for this variable and to operationalize it, we leverage a measure of subjectivity for the product - namely how subjective (e.g. how many adverb, adjective, and other word modifiers) are present within the details and specifications of a product.  A product that more aligns to a "search" product, we hypothesize, will have fewer modifying words and be oriented toward the facts of the object.  

For example, a desk has specific dimensions for length, width, and height, an associated weight, and material from which the desk is made, and possibly some warranty information - all of which are likely to be contained within the product description and specifications.  We would characterize such a good as a "search" good (or a 0 on our scale).  Leveraging existing language processing tools should allow us to calculate a value for subjectivity in the product's description and specifications.  

Initially, we'll explore product subjectivity in the combination of the specification and the description, though it may be necessary to explore product subjectivity solely within one of these fields or the other to pursue our modeling. 

```{python tbl-data-reqs-ratings}
#| label: tbl-data-reqs-ratings
#| tbl-cap: Review Data Required
data_reqs_rating = {
    'Variable':['Verified Purchase', 'Star Rating', 'Review Content', 'Useful Votes'],
    'Data Type':['boolean','float','string','integer']
}
tbl_data_reqs = pd.DataFrame(data_reqs_rating)
tbl_data_reqs.set_index('Variable')
tbl_data_reqs.style.hide(axis='index')
```

In @tbl-data-reqs-ratings, we outline the specific datapoints we sought out for reviews across each website.  @percUse leveraged star rating, review content (specifically the review length), and the number of votes for the review being useful as key measures in their research.  To further their work, we plan on exploring the impacts of verified product purchasers and the impact of verification on how useful a review may be to potential customers.

```{python tbl-data-calcs}
#| label: tbl-data-calcs
#| tbl-cap: Additional Calculated Columns, Post Data Collection
data_calcs = {
    'Variable':['Product Subjectivity','Review Length (Words)', 'Review Subjectivity','Review Polarity'],
    'Data Type':['float','integer','float','float']
}
#sentiment score, product type score, ...
tbl_data_calcs  = pd.DataFrame(data_calcs)
tbl_data_calcs.set_index('Variable')
tbl_data_calcs.style.hide(axis='index')
```

Post collection, we added the calculations listed in @tbl-data-calcs to our review data and product data (less reputation score).  Each of these calculations will allow us to better understand our underlying data and explore possibilities of where and how each may fit into models for review usefulness.

We have also established a master listing of all products for which we collected data and have associated arbitrary identifiers with the products.  In instances where we've successfully pulled data for *identical* products from multiple websites, it can allow us to explore the impact on product and review metrics and investigate the listing site as a treatment variable. 

For instance - exploring the impact of review subjectivity, polarity, length, and usefulness, based upon which site the product was listed.

## Data Collection Procedures

We wrote code to allow us to gather information from each website.  The general process for each e-commerce platform is similar.  To alleviate any unnecessary burden for any of these websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought.  This manual identification also allowed us to ensure, in most cases, that we were getting the *exact* same product during data capture.  This hybrid approach enabled higher certainty in getting the same product while also accelerating collection, structuring, and cleaning of product review information.

* Gathering from Amazon (All Products)

    * Product & Review data was scraped from Amazon's website using Python and Selenium. A Selenium WebDriver was utilized to automate web browser interactions. After navigating to product categories like electronics, home appliances, furniture, books, and grocery, Selenium's functions were employed to locate review elements. These elements were then parsed and collected, storing the data in a structured format i.e. a CSV file. Pagination handling was implemented to scrape reviews from multiple pages. 
    
        * Challenge: Amazon’s product “All Reviews” webpage HTML structure had 10 reviews per page with a “Next Page” navigation button that was clickable only up-to 10 review pages. This restricted our scope of the number of reviews being scraped per product to a maximum of 100. 

        * Solution: Instead of scraping based on the “All Reviews” webpage, we decided to scrape reviews based on “star-rating” thereby, increasing our scope from a total of 100 reviews per product to having a maximum of 100 reviews per star rating i.e. 5*100 = 500 reviews per product.

* Gathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)

    * Just like Target and Amazon, even BestBuy has dynamic content on its web page. We employed Python with Selenium to automate the exploration of product pages, unveiling hidden content, and harvesting essential data. Employing Selenium's functionalities, we initiated the traversal process, enabling the program to automatically expand pertinent sections to uncover additional information. By targeting elements such as product details and reviews, we orchestrated the seamless extraction of critical fields from each product's page. This automated approach allowed us to efficiently parse through an extensive array of reviews, ensuring a comprehensive analysis of user feedback for the products under scrutiny. We systematically stored the extracted data in our records tables for further analysis and reference.

* Gathering from Target (All products)

    * Target has dynamic content on their webpages.  We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data.  We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question. 

## Data Exploration

After collection and cleaning, we plan to explore our data via visualization, seeking to answer key research questions.

* Is the price of a product higher, given it's offered on Amazon, BestBuy, or Target? <!--Eliminate?-->

* Is a product's star rating affected by which e-commerce platform is selling it?

* Is there a substantial difference in number of product reviews on one e-commerce platform vs. another?

* Is one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of "this review is helpful" votes to total number of reviews)? <!--Eliminate?-->

* What is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?  

* Do certain product categories perform better on specific platforms? <!--Eliminate?-->

* Are users more likely to leave reviews on one platform over another?

* Do customers show different purchasing behaviors based on promotional strategies employed by platforms?

Structuring our data properly during the collection process will enable us to explore and answer these questions.

## Data Exploration and Visualization

For our data exploration, we plan to examine solely the reviews for which we have data from all of our websites.  Due to the nature of the vendors, not all offer the same products online.  We've included some unique products from each site (and may even gather more), but will exclude them from initial analysis. 

The common items between all 3 websites include the following:

```{python}
# pd.DataFrame(product_data[product_data['productID'].isin(
#     [1,2,3,5,6,8,9,11,12,13,14,15]
# )]['product_title'].unique()).style.hide(axis='index')
pd.DataFrame(
    product_data.loc[[1,2,3,5,6,8,9,11,12,13,14,15],'product_title']
).style.hide(axis='index')
```

The reason for only examining common products is to check for comparability and similarity of the products associated variables (e.g. product subjectivity, review subjectivity, review polarity, star rating, and so forth) between the websites.  If they are similar or comparable, it may mean that we could use single models to make predictions on the usefulness of customer feedback.  If they are substantially dissimilar, it may mean that modifiers are needed based upon the e-commerce platform in which the product is listed.

We'll start by looking at distributions of some of these key variables, and check some of the common trends between them, potentially moving on to hypothesis testing of these variables to check for statistically significant differences.

### Univariate Plots and Distributions

First, we want to examine the review content across all websites in a single, simple visual - a Wordcloud.  Seeing common words and phrases can prime us for what we might expect to see in more detailed statistical plots.

```{python fig-wordcloud}
#| label: fig-wordcloud
#| fig-cap: Wordcloud of review content

#1 plot type
#1 total plot
all_text = ' '.join(review_data['review_content'])

wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=300).generate(all_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud of Review Content')
plt.axis('off')
plt.show()
```

Examining the Wordcloud, some larger words stick out ("easy", "good", "love", "need" and "great").  There don't seem to be very many negative singular words here as it pertains to these reviews.  This may suggest that the content of reviews, generally, gravitates toward positivity in reviews.  We will proceed to examine this with appropriate statistical plots.

```{python fig-hist-plots-rating}
#| label: fig-hist-plots-rating
#| fig-cap: Histogram Plot (star-rating, by-site)

#2 plot types
#4 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
sites = ['Amazon','Target','BestBuy']
for i in range(len(sites)):
    sns.histplot(
        data=review_data[review_data['site']==sites[i]],
        x='review_star_rating',
        ax=ax[i]
    )
    ax[i].set_title(sites[i])
    if i > 0:
        ax[i].set_ylabel("")
plt.suptitle("Histogram Plots for Star Rating By Site")
plt.tight_layout()
plt.show()
```

Examining the histogram plots for star-rating by website, we can see that, generally, reviews tend to provide more positive than negative feedback for the selected products, supporting what we see coming out of @fig-wordcloud

```{python fig-qq-plots-subj}
#| label: fig-qq-plots-subj
#| fig-cap: Q-Q plots (star-rating, by-site)

#3 plot types
#7 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
sites = ['Amazon','Target','BestBuy']
for i in range(len(sites)):
    x = review_data[review_data['site']==sites[i]]['review_subjectivity']
    mu = x.mean()
    sig = x.std()
    norm = (x-mu)/sig
    sm.qqplot(norm,ax=ax[i])
    ax[i].axline((0,0),slope=1,color='black')
    ax[i].set_title("{}".format(sites[i]))
    if i > 0:
        ax[i].set_ylabel("")
#plt.tight_layout()
plt.suptitle("Q-Q Plots for Review Subjectivity")
plt.show()
```

Across all three websites, there appears to be consistency with adherence to, and issues with, the normal distribution for subjectivity.  These charts suggest sufficient normal distribution of review subjectivity (degree of inclusion of word modifiers such as adverbs and adjectives).  

Therre seems to be slight skewness in the tails of these Q-Q distributions.  Filtering off some of the outliers may grant us reasonable relevance and assurance to perform hypothesis testing and evaluation of these variables across sites (e.g. ANOVA, F-Testing, etc).

We'll try plotting the same Q-Q plot with outliers removed.  To remove outliers, initially, we leveraged the inter-quartile range of each variable and excluded any records for which the variable was more than $1.5\cdot \text{IQR}$ away from the 1st and 3rd quartiles.

Before we proceed to re-examining the Q-Q plot with outliers removed, we'll examine boxplots for these variables to examine the prevalence of outliers.

```{python fig-outliers-star-rating}
#| label: fig-outliers-star-rating
#| fig-cap: "Boxplots - Review Star Rating"


#4 plot types
#10 total plots
boxplot_star_rating = sns.FacetGrid(
    data=review_data,
    col='site'
)
boxplot_star_rating.map_dataframe(
    sns.boxplot,
    y='review_star_rating'
)
```

Boxplots for star ratings on both Target and Amazon are generally higher with outliers on the lower-end of the 1 to 5 scale.  Amazon, however, seems to have a wider spread of information

```{python fig-outliers-polarity}
#| label: fig-outliers-polarity
#| fig-cap: "Boxplots - Review Polarity"

#4 plot types
#13 total plots
boxplot_polarity = sns.FacetGrid(
    data=review_data,
    col='site'
)
boxplot_polarity.map_dataframe(
    sns.boxplot,
    y='review_polarity'
)
```

Boxplots for reviewe polarity suggest common threads between BestBuy and Target in terms of the number summary (min, max, quartiles, and outliers at the lower end).  More notably, the polarity (or how positive or negative the content of the reviews are) generally tends toward positive.  Amazon, on the otherhand, seems to show a lower center of mass and a narrower spread, with outliers to both extremes for positive and negative polarity.

Next, we'll examine subjectivity in the same fashion.

```{python fig-outliers-subjectivity}
#| label: fig-outliers-subjectivity
#| fig-cap: "Boxplots - Review Subjectivity"

#4 plot types
#16 total plots
boxplot_star_rating = sns.FacetGrid(
    data=review_data,
    col='site'
)
boxplot_star_rating.map_dataframe(
    sns.boxplot,
    y='review_subjectivity'
)
```

Subjectivity, generally, seems to follow the same trends as review polarity.  This suggests that these reviews could come from similar or the same population in terms of polarity and subjectivity.  Further statistical analysis would be needed to make a definitive determination here.

Now that we've examined the centers and spread for these variables and understand where some of their outliers may exist, we'll examine filtering those outliers from their Q-Q plots.

First - Subjectivity.

```{python fig-qq-plot-subj-no-out}
#| label: fig-qq-plot-subj-no-out
#| fig-cap: Q-Q plots (star-rating, by-site, outliers removed)

#4 plot types
#19 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
sites = ['Amazon','Target','BestBuy']
for i in range(len(sites)):
    x = review_data[(review_data['site']==sites[i])&(review_data['site_outlier']==0)]['review_subjectivity']
    mu = x.mean()
    sig = x.std()
    norm = (x-mu)/sig
    sm.qqplot(norm,ax=ax[i])
    ax[i].axline((0,0),slope=1,color='black')
    ax[i].set_title("{}".format(sites[i]))
    if i > 0:
        ax[i].set_ylabel("")
#plt.tight_layout()
plt.suptitle("Q-Q Plots for Review Subjectivity (outliers removed)")
plt.show()
```

It seems that our adjustment for outliers sufficiently made corrections for normality across the sites to better adhere to the normal distribution on the lower tail. We may need to make further adjustments on the upper tail to further refine data selection for our training dataset.  Amongst the over 34K reviews in the common dataset, approximately 25.4K reviews remain after removing these outliers using this method.  

After identifying additional means to filter the data, these methods should suffice in support of using review subjectivity as a feature within various models.

```{python fig-qq-plots-pol}
#| label: fig-qq-plots-pol
#| fig-cap: Q-Q plots (star-rating, by-site)

#4 plot types
#22 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
sites = ['Amazon','Target','BestBuy']
for i in range(len(sites)):
    x = review_data[review_data['site']==sites[i]]['review_polarity']
    mu = x.mean()
    sig = x.std()
    norm = (x-mu)/sig
    sm.qqplot(norm,ax=ax[i])
    ax[i].axline((0,0),slope=1,color='black')
    ax[i].set_title("{}".format(sites[i]))
    if i > 0:
        ax[i].set_ylabel("")
plt.suptitle("Q-Q Plots for Review Polarity")
#plt.tight_layout()
plt.show()
```

Similar to review subjectivity, review polarity has good adherence to the normal distribution (particularly on the quantile interval of \[-2,2\]).  There are similar issues in the tails of these distributions as exist for review subjectivity.

As such, reduction in outliers may enable us to perform hypothesis testing during our model design and implementation.  We'll examine the same methods of outlier removal as we did for review subjectivity.

```{python fig-qq-plots-pol-no-out}
#| label: fig-qq-plots-pol-no-out
#| fig-cap: Q-Q plots (polarity, by-site, oultiers removed)


#4 plot types
#25 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
sites = ['Amazon','Target','BestBuy']
for i in range(len(sites)):
    x = review_data[(review_data['site']==sites[i])&(review_data['site_outlier']==0)]['review_polarity']
    mu = x.mean()
    sig = x.std()
    norm = (x-mu)/sig
    sm.qqplot(norm,ax=ax[i])
    ax[i].axline((0,0),slope=1,color='black')
    ax[i].set_title("{}".format(sites[i]))
    if i > 0:
        ax[i].set_ylabel("")
plt.suptitle("Q-Q Plots for Review Polarity (Outliers Removed)")
#plt.tight_layout()
plt.show()
```

This method of removal seems to mirror that of review subjectivity, and as such, additional fitration of the dataset will be necessary to enable this feature's use within various models.

Another key distribution we must understand is that of our targeted response variable - how useful a review is, as voted by other customers.  We'll plot the response as a pure density plot to explore it's shape.

```{python fig-helpful-vote-dist}
#| label: fig-helpful-vote-dist
#| fig-cap: "Distribution of Review Helpful Votes"

#5 plot types
#28 total plots
fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
sites = ['Amazon','Target','BestBuy']
u_lims = [25,2,2]
for i in range(len(sites)):
    sns.kdeplot(
        data=review_data[review_data['site']==sites[i]],
        x='review_helpful_votes',ax=ax[i],color='green'
    )
    mu = review_data[review_data['site']==sites[i]]['review_helpful_votes'].mean()
    sns.kdeplot(
        np.random.exponential(size=1000,scale=mu*1.9),
        ax=ax[i],color='black'
    )
    ax[i].set_title('site = '+sites[i])
    ax[i].set_xlim(0,u_lims[i])

plt.tight_layout()
plt.show()
```

The black lines represent random samples from the exponential distribution (with $E[X] = 1.9\cdot\bar{V}$ with $\bar{V}$ being the mean for helpful votes within the distribution), and the green lines represent the distribution of helpful votes.  It seems that, roughly, the distribution of helpful votes does follow the exponential distribution in the case of Amazon and Target.

<!--
```{python fig-rand-exp-dist}
#| label: fig-rand-exp-dist
#| fig-cap: "Random Exponential Distributions with Means by Site"

fig,ax = plt.subplots(nrows=1,ncols=3)
fig.set_figheight(4)
fig.set_figwidth(8)
u_lims = [25,2,2]
for i in range(len(sites)):
    mu = review_data[review_data['site']==sites[i]]['review_helpful_votes'].mean()
    sns.kdeplot(
        np.random.exponential(size=1000,scale=mu),
        ax=ax[i],color='black'
    )
    ax[i].set_title("RExp("+str(round(mu,3))+")")
    ax[i].set_xlim(0,u_lims[i])

plt.tight_layout()
plt.show()
``` 
-->

Examining the plot of @fig-helpful-vote-dist, the distribution of helpful votes appears to be exponentially distributed on a per-website basis, with many reviews having an expected total count of helpful votes centered fairly low.

Knowing the distribution of our selected response variable will assist us in the modeling process.  The nature of the response variable's distribution may require us to perform transformations on features and responses (e.g. if we pursue a multiple linear regression model).



### Bi/Multivariate Plots

```{python fig-bivar-plot}
#| label: fig-bivar-plot
#| fig-cap: "Bivariate Plot for Sensitivity and Polarity, by Site (all purchases)"

#6 plot types
#28 total plots

bivar_subj_pol = sns.FacetGrid(
    data=review_data,
    col='site'
)
bivar_subj_pol.map_dataframe(
    sns.kdeplot,
    x='review_subjectivity',y='review_polarity'
)
```


```{python fig-bivar-plot2}
#| label: fig-bivar-plot2
#| fig-cap: "Bivariate Plot for Sensitivity and Polarity, by Site (verified purchases)"


#6 plot types
#32 total plots
bivar_subj_pol2 = sns.FacetGrid(
    data=review_data[review_data['verified_purchase']==1],
    col='site'
)
bivar_subj_pol2.map_dataframe(
    sns.kdeplot,
    x='review_subjectivity',y='review_polarity'
)
```


In @fig-bivar-plot and @fig-bivar-plot2, we observe the comparison of multiple features like review polarity, review subjectivity and verified purchases in the form of a Kernel Densoty Plot. The 2 different visuals depict the difference between the whole dataset and after the filter of `verified_purchase` = 1 is applied. This difference may lead to variations in the distribution and relationship between subjectivity and polarity of reviews across different sites, particularly if there are differences in the characteristics of verified and non-verified purchases.

We can see 2 major clusters at (0,0) which are mostly outliers where a review is very short in length. The second cluster around the area where subjectivity is about 0.5 suggests that as the review increases in subjectivity, i.e. the higher an opinionated a review is, the polarity also increases. 

The isolated data points or "islands" outside of the main clusters suggest outliers or unique instances within the dataset. These isolated points may represent reviews that deviate significantly from the overall patterns observed in the data. They could indicate rare or extreme cases that warrant further investigation. For instance, these outliers might correspond to highly subjective or polarized reviews that are not typical of the majority of reviews.


    * Box Plots
```{python fig-violin-plots1}
#| label:    fig-violin-plots1
#| fig-cap:  "Violin Plots of Review Star-Rating vs. Subjectivity, by Site"

#7 plot types
#35 total plots

vplot_polarity_stars = sns.FacetGrid(
    data = review_data,
    col='site'
)
vplot_polarity_stars.map_dataframe(
    sns.violinplot,
    x='review_star_rating',
    y='review_subjectivity'
)
plt.show()
```

```{python fig-violin-plots2}
#| label: fig-violin-plots2
#| fig-cap: "Violin Plots for Star Rating vs. Polarity, by Site"

#7 plot types
#38 total plots
vplot_polarity_stars = sns.FacetGrid(
    data = review_data,
    col='site'
)
vplot_polarity_stars.map_dataframe(
    sns.violinplot,
    x='review_star_rating',
    y='review_polarity'
)
plt.show()
```

Generally, in @fig-violin-plots1 and @fig-violin-plots2, we see a trend for the median polarity and subjectivity of each review to increase as the star rating increases.  We also see that, generally, the data suggest that we have a minimum of neutral polarity that tends towards positive as star rating increases.

Since both median subjectivity and polarity seem to increase with respect to star rating, such a correlation could be useful to us in multiple linear regression, and is generally useful to us for consideration when pursuing model development.

### Hypothesis Testing for Key Feature and Response Variables

Some key features we plan to explore in our modeling include review subjectivity and review polarity.  Knowing whether or not there is a significant difference for these features between the websites on which they're hosted will inform us during model selection, design, and implementation.  As such, we'll perform ANOVA and Tukey Honest Significant Difference Tests on these variables between each site.

#### ANOVA Testing

To perform our ANOVA testing, we'll evaluate each dataset's review polarity and subjectivity as the mean measure, and the website on which the review was posted as the treatment variable.  Prior to performing our one-way ANOVA, we'll filter the datasets down to eliminate outliers, such that the data may represent the outcomes depicted in @fig-qq-plot-subj-no-out and @fig-qq-plots-pol-no-out.  An assumption of ANOVA testing is that the source data (and its respective groups) adhere to the normal distribution.  

We are leveraging Welch ANOVA and operating under an assumption that the variances between the groups are not equal, as visually evidenced in @fig-outliers-polarity and @fig-outliers-subjectivity.

Hypotheses:

* Test 1: 
    * $H_0: \mu_{\text{Subj,Amazon}}=\mu_{\text{Subj,BestBuy}}=\mu_{\text{Subj,Target}}$

    * $H_1:$ at least one mean for review subjectivity is different.

* Test 2: 
    * $H_0: \mu_{\text{Polr,Amazon}}=\mu_{\text{Polr,BestBuy}}=\mu_{\text{Polr,Target}}$

    * $H_1:$ at least one mean for review polarity is different.

* For both tests, $\alpha=0.003$

```{python tbl-welch-anova-polarity}
#| label: tbl-welch-anova-polarity
#| tbl-cap: Welch ANOVA Results (Polarity)

#conducting a welch_anova test for 
#difference of treatment (e.g. website vs. score)
#import pingouin as pg 
#welch_anova
#pg.welch_anova(data=df, between='company',dv=star_rating|sentiment|polarity|total_star_rating)

filtered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])

display(
    pg.welch_anova(data=filtered_data,between='site',dv='review_polarity').style.hide(axis='index')
)
```

```{python tbl-welch-anova-subjectivity}
#| label: tbl-welch-anova-subjectivity
#| tbl-cap: Welch ANOVA Results (Subjectivity)

#conducting a welch_anova test for 
#difference of treatment (e.g. website vs. score)
#import pingouin as pg 
#welch_anova
#pg.welch_anova(data=df, between='company',dv=star_rating|sentiment|polarity|total_star_rating)

filtered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])

display(
    pg.welch_anova(data=filtered_data,between='site',dv='review_subjectivity').style.hide(axis='index')
)
```

The output of both Welch ANOVA tests suggests that the means for review subjectivity and review polarity, given the website on which it was posted, have a statistically significant difference.  We'll seek to visualize these differences using a plot of the Tukey Honest Significance Test.

#### Tukey Tests with E-Commerce Platform as Treatment

```{python fig-tukey-tests}
#| label: fig-tukey-tests
#| fig-cap: "Tukey Tests for Star Rating, Polarity, and Subjectivity, by-Site"

# star = pairwise_tukeyhsd(
#     endog=review_data['review_star_rating'],
#     groups=review_data['site'],
#     alpha=0.003
# )

#8 plot types
#40 total plots

filtered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])

pol = pairwise_tukeyhsd(
    endog=filtered_data['review_polarity'],
    groups=filtered_data['site'],
    alpha=0.003
)
subj = pairwise_tukeyhsd(
    endog=filtered_data['review_subjectivity'],
    groups=filtered_data['site'],
    alpha=0.003
)

fig,axes=plt.subplots(nrows=2,ncols=1)
fig.set_figwidth(7)
fig.set_figheight(6)

tests = [
    #(star,"Tukey Test - Star Rating (95%)"),
    (pol,"Tukey Test - Polarity (99.7%)"),
    (subj,"Tukey Test - Subjectivity (99.7%)")
]

for i in range(len(tests)):
    tests[i][0].plot_simultaneous(ax=axes[i])
    axes[i].set_title(tests[i][1])

plt.tight_layout()
plt.show()

```

The Tukey honest significance tests, depicted in @fig-tukey-tests suggest some interesting patterns between the three websites.  Namely, target and best buy seem to have higher polarity, and subjectivity than the same variables for Amazon!  Additionally, for each variable and each website, it seems there is no overlap in the variables at the 99.7% confidence level.

These statistically significant differences between the reviews, treated by website, indicate to us that we should proceed with caution in our modeling phase.  Namely, it may be necessary to include an explicit variable or feature accounting for the source website in our modeling process as a predictor for the response variable.

<!-- 
### Exploring Sentiments - Star Rating vs. Sentiment Variables

* Inter-Website Comparison of Product Reviews

    * Same Product 

        * Clustering? 

        * Distances?

    * All Products

    * Inspect the following, visually: 
    
        * Product Ratings
        
        * Customer Sentiments **try to score before plotting & turn-in**
        
        * Review Polarity **try to score and store before plotting**
        
        * Naive Bayes Classifier
        
        * Reliability estimates

        * Product description subjectivity scores **try to score and store before turn-in** 
        
        * Average / Spread of number of ratings per product, **try to score and store before turn-in** 
        
        * Average/Spread of Useful Votes per Product Review, **try to score and store before turn-in** 
        
        * Inspection of Data and / or Scoring using Kansei method.

* Will need to take note on if / how these variables conform to some form of statistical distribution (uniform, normal, exponential, etc)

-->

## Data Before / After

Much of our data cleaning occured during the collection process.  Our team took specific steps to pursue cleaning during collection to simplify the process of bringing all information together:

* Using regular expressions to extract key values from text blocks

* Leveraging XPATH, class names, and element IDs to identify HTML fields in which our desired data points resided

Post-scraping, we had to pursue some additional cleanup

* Removal of unicode characters from review content where possible through coding and scripting.

* Conversion of numbers, stored as strings, to integers (i.e. star ratings, cost/dollar amounts)

* Handling of missing values (i.e. no ratings, no star ratings, no cost listed)

A particular challenge we came across during the data cleaning process was the handling foreign language reviews, highly repetitive reviews, and misspelled reviews.  To better support our calculated measures for subjectivity and polarity, we leveraged the langdetect library to attempt to classify the languages of each of our 45,000+ reviews collected.

```{python tbl-foreign-lang-reviews}
#| label: tbl-foreign-lang-reviews
#| tbl-cap: Examples of reviews written in foreign languages
foreign_lang = pd.DataFrame(
    review_data_base[review_data_base['review_lang']!='en']
)
#pd.set_option('display.height',30)
# pd.set_option('display.min_rows',5)
# pd.set_option('display.max_rows',None)
#40:45, 40:45, 3:8
sliced_frame = pd.DataFrame(
    pd.concat([
        foreign_lang[foreign_lang['site']=='Amazon'][0:5][['site','reviewer_name','review_content']],
        foreign_lang[foreign_lang['site']=='BestBuy'][0:5][['site','reviewer_name','review_content']],
        foreign_lang[foreign_lang['site']=='Target'][0:5][['site','reviewer_name','review_content']]
    ])
)
pd.options.display.max_rows = 100
sliced_frame

# # display(
# #     sliced_frame.head(20)
# # )
# from IPython.display import Markdown
# Markdown(sliced_frame.to_markdown(index=False))
```

In some cases, the language classification by langdetect was a false negative (i.e. classified as a language other than english, when it was indeed English).  In our data exploration, we found that many of these false positives were outliers in other categories (whether for review length, review subjectivity, review polarity, or star rating).  As such, we find it prudent to exclude these reviews from our dataset when pursuing model development.  

In total, langdetect classified fewer than 440 reviews (accounting for less than 1% of our collected reviews) as being non-English, or being repeated words or gibberish.  Excluding these reviews should have minimal impact on the pursuit of model development.

TextBlob also offers us the ability to attempt to correct the spelling of reviews.  Due to the amount of time it would take us to pursue spelling corre

Here are some additional examples of gibberish or non-contributional text that impact calculations for review subjectivity and polarity.  While some of these could potentially provide value with deeper analysis, we find that these will not contribute significantly to our research.

```{python}
pd.DataFrame(
    review_data_base[review_data_base['review_lang']=='Unk'][['site','reviewer_name','review_content']]
)
```

We are retaining the totality of the data we've collected, and will filter the data based upon our findings here so as to keep the most relevant and supportive data in building our models.

## Insights from Collection and EDA 

* As in @percUse, online reviews across multiple websites tend toward positivity (reference polarity and star ratings here).

* Statistically significant differences for review subjectivity, polarity across each of the three websites.

* Exclusion of outliers for one or more categories could result in excluding lower star rating reviews, which could impact model.