{
  "hash": "ce69dce3a5c5df5d0251a281c534931b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Data Collection and Exploration {#sec-data}\n\n\n\n## Data Collection Overview\n\nLeverage python Selenium, urllib, and Beautiful Soup to scrape data from X products.\n\n* Sought to collect some data from 3 websites - target, amazon, and best buy.\n\n* Where possible, collect same entity from multiple sites\n\nAs part of collection, to the greatest extent we are able, we cleaned information *during* the scraping process.  We leveraged tools such as the python regular expression library and (*anything else?*) to pull the exact information we sought while scraping.  The only possible additional cleaning that may be required after scraping is the handling of unicode characters within product names, product reviews, and so forth.\n\nIn terms of simplicity for scraping our data, we will manually identify a list of products.  @percUse leveraged solely 3 products from Amazon.  Our team seeks to scrape between 20 and 30 products and all their reviews from Target, Amazon, and Best Buy.  By doing so, we are greatly increasing the sample size of data compared to the original work performed.\n\n## Data Collection Details\n\nIn collecting our data, in order to adhere to the model implemented by @percUse, we require the following data points, at a minimum:\n\n::: {#07c379b6 .cell tbl='tbl-data-reqs-products' execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_1bb90\">\n  <thead>\n    <tr>\n      <th id=\"T_1bb90_level0_col0\" class=\"col_heading level0 col0\" >Variable</th>\n      <th id=\"T_1bb90_level0_col1\" class=\"col_heading level0 col1\" >Data Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_1bb90_row0_col0\" class=\"data row0 col0\" >Product Title</td>\n      <td id=\"T_1bb90_row0_col1\" class=\"data row0 col1\" >string</td>\n    </tr>\n    <tr>\n      <td id=\"T_1bb90_row1_col0\" class=\"data row1 col0\" >Product Category*</td>\n      <td id=\"T_1bb90_row1_col1\" class=\"data row1 col1\" >string</td>\n    </tr>\n    <tr>\n      <td id=\"T_1bb90_row2_col0\" class=\"data row2 col0\" >Product Details/Specs</td>\n      <td id=\"T_1bb90_row2_col1\" class=\"data row2 col1\" >string</td>\n    </tr>\n    <tr>\n      <td id=\"T_1bb90_row3_col0\" class=\"data row3 col0\" >Product Cost</td>\n      <td id=\"T_1bb90_row3_col1\" class=\"data row3 col1\" >float</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {#2a12528b .cell tbl='tbl-data-reqs-ratings' execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_c7501\">\n  <thead>\n    <tr>\n      <th id=\"T_c7501_level0_col0\" class=\"col_heading level0 col0\" >Variable</th>\n      <th id=\"T_c7501_level0_col1\" class=\"col_heading level0 col1\" >Data Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_c7501_row0_col0\" class=\"data row0 col0\" >Verified Purchase</td>\n      <td id=\"T_c7501_row0_col1\" class=\"data row0 col1\" >boolean</td>\n    </tr>\n    <tr>\n      <td id=\"T_c7501_row1_col0\" class=\"data row1 col0\" >Star Rating</td>\n      <td id=\"T_c7501_row1_col1\" class=\"data row1 col1\" >float</td>\n    </tr>\n    <tr>\n      <td id=\"T_c7501_row2_col0\" class=\"data row2 col0\" >Review Content</td>\n      <td id=\"T_c7501_row2_col1\" class=\"data row2 col1\" >string</td>\n    </tr>\n    <tr>\n      <td id=\"T_c7501_row3_col0\" class=\"data row3 col0\" >Useful Votes</td>\n      <td id=\"T_c7501_row3_col1\" class=\"data row3 col1\" >integer</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nFrom our collected data, here are some of the calculations we'll need to run for building our models.  If time is available, we will run these calculations using Python's NLTK before the completion of this milestone.\n\n::: {#42073f95 .cell tbl='tbl-data-calcs' execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_b781b\">\n  <thead>\n    <tr>\n      <th id=\"T_b781b_level0_col0\" class=\"col_heading level0 col0\" >Variable</th>\n      <th id=\"T_b781b_level0_col1\" class=\"col_heading level0 col1\" >Data Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_b781b_row0_col0\" class=\"data row0 col0\" >Product Subjectivity</td>\n      <td id=\"T_b781b_row0_col1\" class=\"data row0 col1\" >float</td>\n    </tr>\n    <tr>\n      <td id=\"T_b781b_row1_col0\" class=\"data row1 col0\" >Review Length</td>\n      <td id=\"T_b781b_row1_col1\" class=\"data row1 col1\" >integer</td>\n    </tr>\n    <tr>\n      <td id=\"T_b781b_row2_col0\" class=\"data row2 col0\" >Sentiment Score</td>\n      <td id=\"T_b781b_row2_col1\" class=\"data row2 col1\" >float</td>\n    </tr>\n    <tr>\n      <td id=\"T_b781b_row3_col0\" class=\"data row3 col0\" >Reputation Score</td>\n      <td id=\"T_b781b_row3_col1\" class=\"data row3 col1\" >float</td>\n    </tr>\n    <tr>\n      <td id=\"T_b781b_row4_col0\" class=\"data row4 col0\" >Product Type Score</td>\n      <td id=\"T_b781b_row4_col1\" class=\"data row4 col1\" >float</td>\n    </tr>\n    <tr>\n      <td id=\"T_b781b_row5_col0\" class=\"data row5 col0\" >Polarity Score</td>\n      <td id=\"T_b781b_row5_col1\" class=\"data row5 col1\" >float</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nIn terms of structuring our stored data, we will have a central table and child tables.  Since we will seek, in some cases, to gather the *same* product from multiple websites, we must have a structure that identifies:\n\n* The full listing of products, assigned an arbitrary ID\n\n* A listing of specific products we intend to scrape from each of the websites we've identified.  Adding an additional ID of \\[company_name\\]-\\[arbitrary_product_id\\]\n\n* Site-specific product information\n\n* Site-specific review content and metadata\n\nTo wrangle the potential amount of data we may collect, we will partition review files into their own files under the following convention:  \\[company_name\\]-\\[arbitrary_product_id\\]-review_content.csv.  Using this method will allow us to capture thousands of reviews for multiple products without overrunning GitHub filesize limitations.  Following this convention will also allow us to easily script out the integration of all these files for analysis when we begin building and applying our models.\n\nAdditionally, the use of assigning products an arbitrary ID that is common between different vendors will allow us to *directly compare* review and product information across multiple vendors simultaneously to gather additional insights.  Effectively, if we group products by this identifier, we can see how the product performs overall in multiple e-commerce platforms.  Similarly, we can use this information to evaluate and run comparisons and tests on individual products, given the treatment of their offering on different platforms.  This can help generate even more insight before model application.  \n\nFor instance, we may be able to explore questions like:\n\n* Is the price of a product higher, given it's offered on Amazon, BestBuy, or Target?\n\n* Is a product's star rating affected by which e-commerce platform is selling it?\n\n* Is there a substantial difference in number of product reviews on one e-commerce platform vs. another?\n\n* Is one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of \"this review is helpful\" votes to total number of reviews)?\n\n* What is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?\n\n* Do certain product categories perform better on specific platforms?\n\n* Are users more likely to leave reviews on one platform over another?\n\n* Do customers show different purchasing behaviors based on promotional strategies employed by platforms?\n\nStructuring our data properly during the collection process will enable us to explore and answer these questions.\n\n## Collection Procedures?\n\nWe wrote code to allow us to (mostly) template out our gathering of information from each website.  The general process for each page is similar for data gathering.  To alleviate any unnecessary burden on the target websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought.  This hybrid approach saved us time and effort.\n\n* Gathering from Target (All products)\n\n    * Target has dynamic content on their webpages.  We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data.  We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question.  We extracted the fields listed above (reference here) to store in our records tables.\n\n* Gathering from Amazon (All Products)\n    * Product & Review data was scraped from Amazon's website using Python and Selenium. A Selenium WebDriver was utilized to automate web browser interactions. After navigating to product categories like electronics, home appliances, furniture, books, and grocery, Selenium's functions were employed to locate review elements. These elements were then parsed and collected, storing the data in a structured format i.e. a CSV file. Pagination handling was implemented to scrape reviews from multiple pages. \n\n* Gathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)\n\n    * Just like Target and Amazon, even BestBuy has dynamic content on its web page. We employed Python with Selenium to automate the exploration of product pages, unveiling hidden content, and harvesting essential data. Employing Selenium's functionalities, we initiated the traversal process, enabling the program to automatically expand pertinent sections to uncover additional information. By targeting elements such as product details and reviews, we orchestrated the seamless extraction of critical fields from each product's page. This automated approach allowed us to efficiently parse through an extensive array of reviews, ensuring a comprehensive analysis of user feedback for the products under scrutiny. We systematically stored the extracted data in our records tables for further analysis and reference.\n\n## Visualizations\n\n* We require a minimum of 10 unique EDA plots for this milestone.  We've outlined some of the below but need our data in order and unified prior to development.\n\n    * Scatter Plots\n\n::: {#cell-fig-violin-plots1 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Violin Plots of Review Star-Rating vs. Subjectivity, by Site](data_files/figure-html/fig-violin-plots1-output-1.png){#fig-violin-plots1 width=854 height=278}\n:::\n:::\n\n\n::: {#cell-fig-violin-plots2 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Violin Plots for Star Rating vs. Polarity, by Site](data_files/figure-html/fig-violin-plots2-output-1.png){#fig-violin-plots2 width=854 height=278}\n:::\n:::\n\n\nGenerally, in @fig-violin-plots1 and @fig-violin-plots2, we see a trend for the median polarity and subjectivity of each review to increase as the star rating increases.  We also see that, generally, the data suggest that we have a minimum of neutral polarity that tends towards positive as star rating increases.\n\n* Bar Plots \n\n    * Box Plots\n\n::: {#cell-fig-outliers-star-rating .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![Outliers for Review Star Rating](data_files/figure-html/fig-outliers-star-rating-output-1.png){#fig-outliers-star-rating width=854 height=278}\n:::\n:::\n\n\n::: {#cell-fig-outliers-polarity .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![Outliers for Review Polarity](data_files/figure-html/fig-outliers-polarity-output-1.png){#fig-outliers-polarity width=854 height=278}\n:::\n:::\n\n\n::: {#cell-fig-outliers-subjectivity .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Outliers for Review Subjectivity](data_files/figure-html/fig-outliers-subjectivity-output-1.png){#fig-outliers-subjectivity width=854 height=278}\n:::\n:::\n\n\n    * Violin Plots (e.g. same product, two different websites)\n\n    * review length\n\n        * vs star rating\n\n        * vs sentiment\n\n    * Product description / detail length\n\n        * Potentially explore \"specificity\" classifier\n\n    * Correlation analyses and linear regressions\n\n    * Heatmaps\n\n    * Tukey test \n\n::: {#cell-fig-tukey-tests .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![Tukey Tests for Star Rating, Polarity, and Subjectivity, by-Site](data_files/figure-html/fig-tukey-tests-output-1.png){#fig-tukey-tests width=950 height=566}\n:::\n:::\n\n\nThe Tukey honest significance tests, depicted in @fig-tukey-tests suggest some interesting patterns between the three websites.  Namely, target and best buy seem to have (across the board) higher star ratings, polarity, and subjectivity than the same variables for Amazon!  Additionally, for each variable and each website, it seems there is no overlap in the variables at the 95% confidence level.  The only exception here is for subjectivity between Target and BestBuy.  \n\n\n\n### Exploring Sentiments - Star Rating vs. Sentiment Variables\n\n* Inter-Website Comparison of Product Reviews\n\n    * Same Product \n\n        * Clustering? \n\n        * Distances?\n\n    * All Products\n\n    * Inspect the following, visually: \n    \n        * Product Ratings\n        \n        * Customer Sentiments **try to score before plotting & turn-in**\n        \n        * Review Polarity **try to score and store before plotting**\n        \n        * Naive Bayes Classifier\n        \n        * Reliability estimates\n\n        * Product description subjectivity scores **try to score and store before turn-in** \n        \n        * Average / Spread of number of ratings per product, **try to score and store before turn-in** \n        \n        * Average/Spread of Useful Votes per Product Review, **try to score and store before turn-in** \n        \n        * Inspection of Data and / or Scoring using Kansei method.\n\n* Will need to take note on if / how these variables conform to some form of statistical distribution (uniform, normal, exponential, etc)\n\n## Data Before / After\n\nMuch of our data cleaning occured during the collection process.  Our team took specific steps to pursue cleaning during collection to simplify the process of bringing all information together:\n\n* Using regular expressions to extract key values from text blocks\n\n* Leveraging XPATH, class names, and element IDs to identify HTML fields in which our desired data points resided\n\nPost-scraping, we had to pursue some additional cleanup\n\n* Removal of unicode characters from review content where possible through coding and scripting.\n\n* Conversion of numbers, stored as strings, to integers (i.e. star ratings, cost/dollar amounts)\n\n* Handling of missing values (i.e. no ratings, no star ratings, no cost listed)\n\nA particular challenge we came across during the data cleaning process was the handling foreign language reviews, highly repetitive reviews, and misspelled reviews.  To better support our calculated measures for subjectivity and polarity, we leveraged the langdetect library to attempt to classify the languages of each of our 45,000+ reviews collected.  \n\n::: {#tbl-foreign-lang-reviews .cell tbl-cap='Examples of reviews written in foreign languages' execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site</th>\n      <th>reviewer_name</th>\n      <th>review_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>495</th>\n      <td>Amazon</td>\n      <td>Kiti🌸</td>\n      <td>Muy oratuca y fácil de instalar</td>\n    </tr>\n    <tr>\n      <th>525</th>\n      <td>Amazon</td>\n      <td>julia</td>\n      <td>Impossible to set up so far</td>\n    </tr>\n    <tr>\n      <th>552</th>\n      <td>Amazon</td>\n      <td>Liuska Noda</td>\n      <td>Me está dando problemas para conectarla me dic...</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>Amazon</td>\n      <td>Arturo</td>\n      <td>Me compre la impresora para imprimir en la may...</td>\n    </tr>\n    <tr>\n      <th>721</th>\n      <td>Amazon</td>\n      <td>Hbolivar</td>\n      <td>funciono para 4 hojas .. problemas con wi-fi -...</td>\n    </tr>\n    <tr>\n      <th>872</th>\n      <td>Amazon</td>\n      <td>Suena muy bien me encantó ❤️🥰</td>\n      <td>Muy buena elección me encantó 🥰 suena bien</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>Amazon</td>\n      <td>Clara James</td>\n      <td>Love it!</td>\n    </tr>\n    <tr>\n      <th>901</th>\n      <td>Amazon</td>\n      <td>humberto a.</td>\n      <td>Excelente aparato para tan bajo precio, sonido...</td>\n    </tr>\n    <tr>\n      <th>904</th>\n      <td>Amazon</td>\n      <td>Betty Maronsky</td>\n      <td>El Bluetooth muy bueno escucho mi música</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <td>Amazon</td>\n      <td>Melissa Cannon</td>\n      <td>I love it</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIn some cases, the language classification by langdetect was a false negative (i.e. classified as a language other than english, when it was indeed English).  In our data exploration, we found that many of these false positives were outliers in other categories (whether for review length, review subjectivity, review polarity, or star rating).  As such, we find it prudent to exclude these reviews from our dataset when pursuing model development.  \n\nIn total, langdetect classified fewer than X reviews (accounting for X.XX% of our collected reviews) as being non-English.  Excluding these reviews should have minimal impact on the pursuit of model development.\n\n## Insights from Collection and EDA \n\n* Statistically significant differences for review star rating, subjectivity, polarity across each of the three websites.\n\n* Exclusion of outliers for one or more categories could result in excluding lower star rating reviews, which could impact model\n\n<!-- ```{r TryBothOnePaper}\nprint(\"Hello,world!\")\n``` -->\n\n",
    "supporting": [
      "data_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}