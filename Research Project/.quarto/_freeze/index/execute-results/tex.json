{
  "hash": "2ccd4e64167f74bbe4b75e19538d4923",
  "result": {
    "engine": "jupyter",
    "markdown": "# Introduction {#sec-intro}\n\n\n\n## Concept and Motivation\n<!--PC-->\nCustomers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product.  *(Information on search products vs. experience vs. mixed).*  When a product is more in the directon of experience vs. search-based, other customers' experiences can shed light on its features and return on investment than information directly from the vendor can.  Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.\n\nWe seek to expound upon the research of [@percUse] to explore additional recommended research areas to improve upon and increase the general applicability of the model.\n\n## What / Where\n<!-- -->\n@percUse provided the following areas for recommended additional research at the conclusion of their paper: <!--PC-->\n\n1. Expand the number of products beyond 3 items (one search, one experience, one mixed) to better generalize the model.\n\n1. Explore customer or reviewer metadata for classifying reviewer types to enhance model performance.\n\n<!--Team - I don't know that we can actually go about getting customer metadata.  This may be a bridge too far for us unless the site(s) we identify can provide such information--> \n\nWe seek to explore Item #1 and #2 above, and to explore the possibility of assessing a scale for products to determine the extent to which they are a search or experience-based product.  We've explored work from other research teams to identify potential methods we can leverage to pursue these ends.\n\n* Determine the polarity of a customer review by employing a classifier such as Naive Bayes. <!--AK talk about Naive Bayes, Kansei Method-->\n\n* Use Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.\n\n* Attempt to assess the reliability of a customer's review based on star-rating and a 'sentiment score' of their textual feedback.  <!--UK - talk about Customer Reliability methods--> \n\nExploring combinations of these research methods, we will pursue potential improvements on the models outlined in @percUse.  We will examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon).\n\n## Why It Matters\n\n<!--PC--> \n\nFeedback from customers is beneficial, but it is not always ordered by the most informative or beneficial feedback first.  Certain features of data such as... can impact the usefulness of the feedback on a customer-by-customer basis.  Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers.  Were e-commerce \n\nExamining additional product types can enable the generalization of the authors' methodology to other products.  Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals.  Producing a reliable scale and methods for classifying a products' degree of being experienced-based can inform vendors on:\n\n* How to best sort product reviews\n\n* Examine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment\n\n* Adjust the product, its marketing, or future production based upon market efficacy.\n\n* Understanding the emotions a customer wants to express through a review is crucial as it will affect the \"recommendation score\" of that particular product or a different one from a similar category.\n\n    * To contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.\n    \n    * Typically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. \n  <!--AK talk about Naive Bayes, Kansei Method--> \n\n* Determine which customer is trustworthy, meaning who has actually purchased the product versus a customer who gave a false review. Based on the ‘customer reputation score’, our aim is to classify customers into groups to judge reviewer reliability. This has two main aspects:\n\n    * Star-rating score which is a discrete scale that tells the inclination of a customer.\n    \n    * Text review ‘sentiment score’ using NLP that explains customer opinions based on words.\n    <!--UK - talk about Customer Reliability methods--> \n\n## Literature Survey\n\n* **Additional commentary on original paper here**\n\n* **Paper(s) on product classification (search-experience-mixed)**\n\n* **Paper(s) on user/consumer/reviewer classification**\n<!--PC, UK, AK-->\n* **All papers you've found, provide a summary of what they did and any key results**\n\n    * Hu, W., Gong, Z., & Guo, J. (2010). Mining Product Features from Online Reviews. 2010 IEEE 7th International Conference on E-Business Engineering. doi:10.1109/icebe.2010.51\n    \n        * The proposed system employs a two-step process for opinion mining: identifying opinion sentences using a SentiWordNet-based algorithm and extracting product features from all reviews in the database. This feature extraction function focuses on identifying commonly expressed positive or negative opinions before extracting explicit and implicit product features.\n\n    * Rajeev, P. V., & Rekha, V. S. (2015). Recommending products to customers using opinion mining of online product reviews and features. 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]. doi:10.1109/iccpct.2015.7159433 \n        \n        * This paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.  \n\n    * Wang, W. M., Li, Z., Tian, Z. G., Wang, J. W., & Cheng, M. N. (2018). Extracting and summarizing affective features and responses from online product descriptions and reviews: A Kansei text mining approach. Engineering Applications of Artificial Intelligence, 73, 149–162. doi:10.1016/j.engappai.2018.05.005\n\n        * Authors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, POS tagging\n\n\n## Research Questions\n\n* Can the model from @percUse be generalized with\n\n    - larger volume of products and product types from which to mine data?\n\n    - a sliding scalar multiplier representing the degree to which a product is a \"search\" (0) or \"experience\" (1) product?\n\n    - Adding modifiers to review content based upon:\n\n        * Customer / Reviewer Reliability?\n\n* Can the polarity of reviews be judged accurately by using a Naive Bayes classification model? \n\n    - What is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model?\n\n* Can products be classified on their degree of being search or experience based by examining product variables such as:\n\n    * Degree of specificity in the product description?\n\n    * Whether the product is offered in brand-new condition only, or offered as new, used, or refurbished?\n\n    * Which of the 5 senses the product engages?\n\n    * Item rarity (limited production or unique items vs. bulk-produced items)?\n\n* Can newer natrual language processing libraries provide a better fit for @percUse 's Review Content metrics?\n\n* How does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product?\n<!-- AK 2 research questions -->\n\n* Can we categorize customer reviews based on customer experience and sentiment?\n\n* Do specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?\n\n* Are specific quality descriptors in text-based reviews (e.g., 'enthusiastic', 'disappointed') strongly associated with certain rating levels, and how does this association affect product reputation?\n\n<!-- UK 2 research questions -->\n\n## Goals / Definition of Success\n\n<!-- --> \n\n* Replicate similar results to @percUse with similar product types\n\n* Expound upon @percUse with additional products, including:\n\n    a. Original products from (paper): Digital Music, Video Game, and Grocery Item\n\n    b. Additional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies\n\n    c. Additional Proucts (Amazon, Target, BestBuy): Electronics\n\n    d. Verify goodness of fit of original model\n\n* Determing best metrics and/or modifiers for Review Content and Customer Reliability\n\n* Achieving similar or better fit than original paper's modeling; extrapolate to other product types.\n\n<!-- Adding success for other research questions --> \n* Determining strength of correlation metrics (support, confidence, lift) between Naive Bayes' classifier for review polarity\n\n    * Integrate with the model and test if Naive Bayes shows strong correlation metrics.\n\n    * Compare and contrast the model with and without incorporation.\n\n* \n\n<!--\n\nThis is where our abstract will go.\n\nHere's an example of inserting a plot directly into our paper\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![a matplotlib plot](index_files/figure-pdf/fig-a-output-1.pdf){#fig-a}\n:::\n:::\n\n\nHere's an example of pulling some data in from our github repo\n\n\ninit_notebook_mode(connected=True)\n\n#reference data directly that sits in the github repo\ntemp = pd.read_csv('../data/Fatalities.csv')\n\ntemp['ST'] = temp['state'].str.upper()\n\n#add the ratio column to examine the plot by ratio of alcohol fatalitites to all fatalities\ntemp['afatal_ratio'] = temp['afatal'] / temp['fatal']\n\nfig = go.Figure()\n\n#collection point for data to be held from each slider position\nslider_info = []\n\n#slice the data by year \nfor y in range(temp['year'].min(),temp['year'].max()+1):\n    #get the current year's data\n    year_data = temp[temp['year']==y]\n\n    #build a dictionary to house the data for the year\n    #we specify \"choropleth\" to feed the dictionary to plotly\n    #when we have a slider change\n    dict_year_data = {\n        'type':'choropleth',\n        'locations':year_data['ST'], #we want information by-state\n        'z':year_data['afatal_ratio'], #plot the ratio of alcohol fatalities to all fatalities.\n        'locationmode':'USA-states', #look at the US\n        'colorbar':{'title':'alcohol-related fatalities (%)'} #label the color axis\n    }\n    #collect all the data to add to our slider information\n    slider_info.append(dict_year_data)\n\nsteps = []\nfor i in range(len(slider_info)):\n    step = dict(\n        method='restyle',\n        args=['visible',[False]*len(slider_info)],\n        #start the slider index at 1982\n        label='{}'.format(i+1982)\n    )\n    step['args'][1][i] = True\n    steps.append(step)\n\nsliders = [dict(active=len(steps)-1,pad={\"t\":1},steps=steps)]\n\nlayout = dict(\n    title='Alcohol-Related Fatality Rate 1982-1988',\n    geo=dict(scope='usa',projection={'type':'albers usa'}),\n    sliders=sliders,\n    width=1000,\n    height=500\n)\n\nfig = dict(data=slider_info,layout=layout)\n\niplot(fig)\n\n```-->\n\n",
    "supporting": [
      "index_files\\figure-pdf"
    ],
    "filters": []
  }
}