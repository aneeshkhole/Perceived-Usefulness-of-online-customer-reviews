{"title":"Data Collection and Exploration","markdown":{"headingText":"Data Collection and Exploration","headingAttr":{"id":"sec-data","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{python}\n#module and data imports\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport seaborn as sns\nimport pingouin as pg\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nimport statsmodels.api as sm\nfrom wordcloud import WordCloud\nimport pingouin as pg \n\nproduct_data = pd.read_csv('../data/master_product_list.csv')\n#review_data = pd.read_csv('../data/combined_review_table.csv')\nreview_data = pd.read_csv('../data/all_reviews_without_unicode.csv')\nreview_data_base = pd.DataFrame(review_data)\nreview_data = pd.DataFrame(\n    review_data[\n        (review_data['productID'].isin([1,2,3,5,6,8,9,11,12,13,14,15])) &\n        (review_data['review_lang']=='en')\n    ]\n)\n   \nreview_metrics = pd.read_csv('../data/combined_review_metrics.csv')\n```\n\n## Data Collection Overview\n\nThe original efforts by @percUse selected three products, all listed on Amazon for sale.  In our efforts, we leveraged python Selenium, urllib, and Beautiful Soup to scrape data from 20 different products across multiple websites (Amazon, BestBuy, and Target).  Where possible, we sought to collect the exact same 20 products from each site and customer feedback associated with each.\n\nAs part of collection, to the greatest extent we were able, we cleaned information *during* the scraping process.  Doing this enabled us to have minimal cleaning efforts after collection.  Post collection, remaining items such as handling and removing special characters, unicode characters, addressing customer reviews written in foreign languages, and addressing misspellings remained necessary.\n\nIn terms of simplicity for scraping our data, we manually identified a list of products from each of the aforementioned sites.  Our team divided responsibilities to produce scraping code customized for each of the three websites.\n\n## Data Collection Details\n\nIn collecting our data, in order to adhere to the model implemented by @percUse, we required the following data points: \n\n```{python dataReqsProducts}\n#|tbl: tbl-data-reqs-products\ndata_reqs_product = {\n    'Variable':['Product Title','Product Category*','Product Details/Specs', 'Product Cost'],\n    'Data Type':['string','string','string','float']\n}\ntbl_data_reqs = pd.DataFrame(data_reqs_product)\ntbl_data_reqs.set_index('Variable')\ntbl_data_reqs.style.hide(axis='index')\n```\n\nFor the product category variable - we may add our own manual categorization.  @percUse manually set the value for this variable.  Part of the intent of our research is to seek out means and methods to replace this variable with a continuous scale (ranging from 0 for a \"search\" good, to a 1 for an \"experience\" good).  \n\nAs an initial proxy for this variable and to operationalize it, we leverage a measure of subjectivity for the product - namely how subjective (e.g. how many adverb, adjective, and other word modifiers) are present within the details and specifications of a product.  A product that more aligns to a \"search\" product, we hypothesize, will have fewer modifying words and be oriented toward the facts of the object.  \n\nFor example, a desk has specific dimensions for length, width, and height, an associated weight, and material from which the desk is made, and possibly some warranty information - all of which are likely to be contained within the product description and specifications.  We would characterize such a good as a \"search\" good (or a 0 on our scale).  Leveraging existing language processing tools should allow us to calculate a value for subjectivity in the product's description and specifications.  \n\nInitially, we'll explore product subjectivity in the combination of the specification and the description, though it may be necessary to explore product subjectivity solely within one of these fields or the other to pursue our modeling. \n\n```{python tbl-data-reqs-ratings}\n#| label: tbl-data-reqs-ratings\n#| tbl-cap: Review Data Required\ndata_reqs_rating = {\n    'Variable':['Verified Purchase', 'Star Rating', 'Review Content', 'Useful Votes'],\n    'Data Type':['boolean','float','string','integer']\n}\ntbl_data_reqs = pd.DataFrame(data_reqs_rating)\ntbl_data_reqs.set_index('Variable')\ntbl_data_reqs.style.hide(axis='index')\n```\n\nIn @tbl-data-reqs-ratings, we outline the specific datapoints we sought out for reviews across each website.  @percUse leveraged star rating, review content (specifically the review length), and the number of votes for the review being useful as key measures in their research.  To further their work, we plan on exploring the impacts of verified product purchasers and the impact of verification on how useful a review may be to potential customers.\n\n```{python tbl-data-calcs}\n#| label: tbl-data-calcs\n#| tbl-cap: Additional Calculated Columns, Post Data Collection\ndata_calcs = {\n    'Variable':['Product Subjectivity','Review Length (Words)', 'Review Subjectivity','Review Polarity'],\n    'Data Type':['float','integer','float','float']\n}\n#sentiment score, product type score, ...\ntbl_data_calcs  = pd.DataFrame(data_calcs)\ntbl_data_calcs.set_index('Variable')\ntbl_data_calcs.style.hide(axis='index')\n```\n\nPost collection, we added the calculations listed in @tbl-data-calcs to our review data and product data (less reputation score).  Each of these calculations will allow us to better understand our underlying data and explore possibilities of where and how each may fit into models for review usefulness.\n\nWe have also established a master listing of all products for which we collected data and have associated arbitrary identifiers with the products.  In instances where we've successfully pulled data for *identical* products from multiple websites, it can allow us to explore the impact on product and review metrics and investigate the listing site as a treatment variable. \n\nFor instance - exploring the impact of review subjectivity, polarity, length, and usefulness, based upon which site the product was listed.\n\n## Data Collection Procedures\n\nWe wrote code to allow us to gather information from each website.  The general process for each e-commerce platform is similar.  To alleviate any unnecessary burden for any of these websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought.  This manual identification also allowed us to ensure, in most cases, that we were getting the *exact* same product during data capture.  This hybrid approach enabled higher certainty in getting the same product while also accelerating collection, structuring, and cleaning of product review information.\n\n* Gathering from Amazon (All Products)\n\n    * Product & Review data was scraped from Amazon's website using Python and Selenium. A Selenium WebDriver was utilized to automate web browser interactions. After navigating to product categories like electronics, home appliances, furniture, books, and grocery, Selenium's functions were employed to locate review elements. These elements were then parsed and collected, storing the data in a structured format i.e. a CSV file. Pagination handling was implemented to scrape reviews from multiple pages. \n    \n        * Challenge: Amazon’s product “All Reviews” webpage HTML structure had 10 reviews per page with a “Next Page” navigation button that was clickable only up-to 10 review pages. This restricted our scope of the number of reviews being scraped per product to a maximum of 100. \n\n        * Solution: Instead of scraping based on the “All Reviews” webpage, we decided to scrape reviews based on “star-rating” thereby, increasing our scope from a total of 100 reviews per product to having a maximum of 100 reviews per star rating i.e. 5*100 = 500 reviews per product.\n\n* Gathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)\n\n    * Just like Target and Amazon, even BestBuy has dynamic content on its web page. We employed Python with Selenium to automate the exploration of product pages, unveiling hidden content, and harvesting essential data. Employing Selenium's functionalities, we initiated the traversal process, enabling the program to automatically expand pertinent sections to uncover additional information. By targeting elements such as product details and reviews, we orchestrated the seamless extraction of critical fields from each product's page. This automated approach allowed us to efficiently parse through an extensive array of reviews, ensuring a comprehensive analysis of user feedback for the products under scrutiny. We systematically stored the extracted data in our records tables for further analysis and reference.\n\n* Gathering from Target (All products)\n\n    * Target has dynamic content on their webpages.  We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data.  We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question. \n\n## Data Exploration\n\nAfter collection and cleaning, we plan to explore our data via visualization, seeking to answer key research questions.\n\n* Is the price of a product higher, given it's offered on Amazon, BestBuy, or Target? <!--Eliminate?-->\n\n* Is a product's star rating affected by which e-commerce platform is selling it?\n\n* Is there a substantial difference in number of product reviews on one e-commerce platform vs. another?\n\n* Is one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of \"this review is helpful\" votes to total number of reviews)? <!--Eliminate?-->\n\n* What is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?  \n\n* Do certain product categories perform better on specific platforms? <!--Eliminate?-->\n\n* Are users more likely to leave reviews on one platform over another?\n\n* Do customers show different purchasing behaviors based on promotional strategies employed by platforms?\n\nStructuring our data properly during the collection process will enable us to explore and answer these questions.\n\n## Data Exploration and Visualization\n\nFor our data exploration, we plan to examine solely the reviews for which we have data from all of our websites.  Due to the nature of the vendors, not all offer the same products online.  We've included some unique products from each site (and may even gather more), but will exclude them from initial analysis. \n\nThe common items between all 3 websites include the following:\n\n```{python}\n# pd.DataFrame(product_data[product_data['productID'].isin(\n#     [1,2,3,5,6,8,9,11,12,13,14,15]\n# )]['product_title'].unique()).style.hide(axis='index')\npd.DataFrame(\n    product_data.loc[[1,2,3,5,6,8,9,11,12,13,14,15],'product_title']\n).style.hide(axis='index')\n```\n\nThe reason for only examining common products is to check for comparability and similarity of the products associated variables (e.g. product subjectivity, review subjectivity, review polarity, star rating, and so forth) between the websites.  If they are similar or comparable, it may mean that we could use single models to make predictions on the usefulness of customer feedback.  If they are substantially dissimilar, it may mean that modifiers are needed based upon the e-commerce platform in which the product is listed.\n\nWe'll start by looking at distributions of some of these key variables, and check some of the common trends between them, potentially moving on to hypothesis testing of these variables to check for statistically significant differences.\n\n### Univariate Plots and Distributions\n\nFirst, we want to examine the review content across all websites in a single, simple visual - a Wordcloud.  Seeing common words and phrases can prime us for what we might expect to see in more detailed statistical plots.\n\n```{python fig-wordcloud}\n#| label: fig-wordcloud\n#| fig-cap: Wordcloud of review content\n\n#1 plot type\n#1 total plot\nall_text = ' '.join(review_data['review_content'])\n\nwordcloud = WordCloud(width=800, height=400, background_color='white', max_words=300).generate(all_text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Word Cloud of Review Content')\nplt.axis('off')\nplt.show()\n```\n\nExamining the Wordcloud, some larger words stick out (\"easy\", \"good\", \"love\", \"need\" and \"great\").  There don't seem to be very many negative singular words here as it pertains to these reviews.  This may suggest that the content of reviews, generally, gravitates toward positivity in reviews.  We will proceed to examine this with appropriate statistical plots.\n\n```{python fig-hist-plots-rating}\n#| label: fig-hist-plots-rating\n#| fig-cap: Histogram Plot (star-rating, by-site)\n\n#2 plot types\n#4 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nsites = ['Amazon','Target','BestBuy']\nfor i in range(len(sites)):\n    sns.histplot(\n        data=review_data[review_data['site']==sites[i]],\n        x='review_star_rating',\n        ax=ax[i]\n    )\n    ax[i].set_title(sites[i])\n    if i > 0:\n        ax[i].set_ylabel(\"\")\nplt.suptitle(\"Histogram Plots for Star Rating By Site\")\nplt.tight_layout()\nplt.show()\n```\n\nExamining the histogram plots for star-rating by website, we can see that, generally, reviews tend to provide more positive than negative feedback for the selected products, supporting what we see coming out of @fig-wordcloud\n\n```{python fig-qq-plots-subj}\n#| label: fig-qq-plots-subj\n#| fig-cap: Q-Q plots (star-rating, by-site)\n\n#3 plot types\n#7 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nsites = ['Amazon','Target','BestBuy']\nfor i in range(len(sites)):\n    x = review_data[review_data['site']==sites[i]]['review_subjectivity']\n    mu = x.mean()\n    sig = x.std()\n    norm = (x-mu)/sig\n    sm.qqplot(norm,ax=ax[i])\n    ax[i].axline((0,0),slope=1,color='black')\n    ax[i].set_title(\"{}\".format(sites[i]))\n    if i > 0:\n        ax[i].set_ylabel(\"\")\n#plt.tight_layout()\nplt.suptitle(\"Q-Q Plots for Review Subjectivity\")\nplt.show()\n```\n\nAcross all three websites, there appears to be consistency with adherence to, and issues with, the normal distribution for subjectivity.  These charts suggest sufficient normal distribution of review subjectivity (degree of inclusion of word modifiers such as adverbs and adjectives).  \n\nTherre seems to be slight skewness in the tails of these Q-Q distributions.  Filtering off some of the outliers may grant us reasonable relevance and assurance to perform hypothesis testing and evaluation of these variables across sites (e.g. ANOVA, F-Testing, etc).\n\nWe'll try plotting the same Q-Q plot with outliers removed.  To remove outliers, initially, we leveraged the inter-quartile range of each variable and excluded any records for which the variable was more than $1.5\\cdot \\text{IQR}$ away from the 1st and 3rd quartiles.\n\nBefore we proceed to re-examining the Q-Q plot with outliers removed, we'll examine boxplots for these variables to examine the prevalence of outliers.\n\n```{python fig-outliers-star-rating}\n#| label: fig-outliers-star-rating\n#| fig-cap: \"Boxplots - Review Star Rating\"\n\n\n#4 plot types\n#10 total plots\nboxplot_star_rating = sns.FacetGrid(\n    data=review_data,\n    col='site'\n)\nboxplot_star_rating.map_dataframe(\n    sns.boxplot,\n    y='review_star_rating'\n)\n```\n\nBoxplots for star ratings on both Target and Amazon are generally higher with outliers on the lower-end of the 1 to 5 scale.  Amazon, however, seems to have a wider spread of information\n\n```{python fig-outliers-polarity}\n#| label: fig-outliers-polarity\n#| fig-cap: \"Boxplots - Review Polarity\"\n\n#4 plot types\n#13 total plots\nboxplot_polarity = sns.FacetGrid(\n    data=review_data,\n    col='site'\n)\nboxplot_polarity.map_dataframe(\n    sns.boxplot,\n    y='review_polarity'\n)\n```\n\nBoxplots for reviewe polarity suggest common threads between BestBuy and Target in terms of the number summary (min, max, quartiles, and outliers at the lower end).  More notably, the polarity (or how positive or negative the content of the reviews are) generally tends toward positive.  Amazon, on the otherhand, seems to show a lower center of mass and a narrower spread, with outliers to both extremes for positive and negative polarity.\n\nNext, we'll examine subjectivity in the same fashion.\n\n```{python fig-outliers-subjectivity}\n#| label: fig-outliers-subjectivity\n#| fig-cap: \"Boxplots - Review Subjectivity\"\n\n#4 plot types\n#16 total plots\nboxplot_star_rating = sns.FacetGrid(\n    data=review_data,\n    col='site'\n)\nboxplot_star_rating.map_dataframe(\n    sns.boxplot,\n    y='review_subjectivity'\n)\n```\n\nSubjectivity, generally, seems to follow the same trends as review polarity.  This suggests that these reviews could come from similar or the same population in terms of polarity and subjectivity.  Further statistical analysis would be needed to make a definitive determination here.\n\nNow that we've examined the centers and spread for these variables and understand where some of their outliers may exist, we'll examine filtering those outliers from their Q-Q plots.\n\nFirst - Subjectivity.\n\n```{python fig-qq-plot-subj-no-out}\n#| label: fig-qq-plot-subj-no-out\n#| fig-cap: Q-Q plots (star-rating, by-site, outliers removed)\n\n#4 plot types\n#19 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nsites = ['Amazon','Target','BestBuy']\nfor i in range(len(sites)):\n    x = review_data[(review_data['site']==sites[i])&(review_data['site_outlier']==0)]['review_subjectivity']\n    mu = x.mean()\n    sig = x.std()\n    norm = (x-mu)/sig\n    sm.qqplot(norm,ax=ax[i])\n    ax[i].axline((0,0),slope=1,color='black')\n    ax[i].set_title(\"{}\".format(sites[i]))\n    if i > 0:\n        ax[i].set_ylabel(\"\")\n#plt.tight_layout()\nplt.suptitle(\"Q-Q Plots for Review Subjectivity (outliers removed)\")\nplt.show()\n```\n\nIt seems that our adjustment for outliers sufficiently made corrections for normality across the sites to better adhere to the normal distribution on the lower tail. We may need to make further adjustments on the upper tail to further refine data selection for our training dataset.  Amongst the over 34K reviews in the common dataset, approximately 25.4K reviews remain after removing these outliers using this method.  \n\nAfter identifying additional means to filter the data, these methods should suffice in support of using review subjectivity as a feature within various models.\n\n```{python fig-qq-plots-pol}\n#| label: fig-qq-plots-pol\n#| fig-cap: Q-Q plots (star-rating, by-site)\n\n#4 plot types\n#22 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nsites = ['Amazon','Target','BestBuy']\nfor i in range(len(sites)):\n    x = review_data[review_data['site']==sites[i]]['review_polarity']\n    mu = x.mean()\n    sig = x.std()\n    norm = (x-mu)/sig\n    sm.qqplot(norm,ax=ax[i])\n    ax[i].axline((0,0),slope=1,color='black')\n    ax[i].set_title(\"{}\".format(sites[i]))\n    if i > 0:\n        ax[i].set_ylabel(\"\")\nplt.suptitle(\"Q-Q Plots for Review Polarity\")\n#plt.tight_layout()\nplt.show()\n```\n\nSimilar to review subjectivity, review polarity has good adherence to the normal distribution (particularly on the quantile interval of \\[-2,2\\]).  There are similar issues in the tails of these distributions as exist for review subjectivity.\n\nAs such, reduction in outliers may enable us to perform hypothesis testing during our model design and implementation.  We'll examine the same methods of outlier removal as we did for review subjectivity.\n\n```{python fig-qq-plots-pol-no-out}\n#| label: fig-qq-plots-pol-no-out\n#| fig-cap: Q-Q plots (polarity, by-site, oultiers removed)\n\n\n#4 plot types\n#25 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nsites = ['Amazon','Target','BestBuy']\nfor i in range(len(sites)):\n    x = review_data[(review_data['site']==sites[i])&(review_data['site_outlier']==0)]['review_polarity']\n    mu = x.mean()\n    sig = x.std()\n    norm = (x-mu)/sig\n    sm.qqplot(norm,ax=ax[i])\n    ax[i].axline((0,0),slope=1,color='black')\n    ax[i].set_title(\"{}\".format(sites[i]))\n    if i > 0:\n        ax[i].set_ylabel(\"\")\nplt.suptitle(\"Q-Q Plots for Review Polarity (Outliers Removed)\")\n#plt.tight_layout()\nplt.show()\n```\n\nThis method of removal seems to mirror that of review subjectivity, and as such, additional fitration of the dataset will be necessary to enable this feature's use within various models.\n\nAnother key distribution we must understand is that of our targeted response variable - how useful a review is, as voted by other customers.  We'll plot the response as a pure density plot to explore it's shape.\n\n```{python fig-helpful-vote-dist}\n#| label: fig-helpful-vote-dist\n#| fig-cap: \"Distribution of Review Helpful Votes\"\n\n#5 plot types\n#28 total plots\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nsites = ['Amazon','Target','BestBuy']\nu_lims = [25,2,2]\nfor i in range(len(sites)):\n    sns.kdeplot(\n        data=review_data[review_data['site']==sites[i]],\n        x='review_helpful_votes',ax=ax[i],color='green'\n    )\n    mu = review_data[review_data['site']==sites[i]]['review_helpful_votes'].mean()\n    sns.kdeplot(\n        np.random.exponential(size=1000,scale=mu*1.9),\n        ax=ax[i],color='black'\n    )\n    ax[i].set_title('site = '+sites[i])\n    ax[i].set_xlim(0,u_lims[i])\n\nplt.tight_layout()\nplt.show()\n```\n\nThe black lines represent random samples from the exponential distribution (with $E[X] = 1.9\\cdot\\bar{V}$ with $\\bar{V}$ being the mean for helpful votes within the distribution), and the green lines represent the distribution of helpful votes.  It seems that, roughly, the distribution of helpful votes does follow the exponential distribution in the case of Amazon and Target.\n\n<!--\n```{python fig-rand-exp-dist}\n#| label: fig-rand-exp-dist\n#| fig-cap: \"Random Exponential Distributions with Means by Site\"\n\nfig,ax = plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(4)\nfig.set_figwidth(8)\nu_lims = [25,2,2]\nfor i in range(len(sites)):\n    mu = review_data[review_data['site']==sites[i]]['review_helpful_votes'].mean()\n    sns.kdeplot(\n        np.random.exponential(size=1000,scale=mu),\n        ax=ax[i],color='black'\n    )\n    ax[i].set_title(\"RExp(\"+str(round(mu,3))+\")\")\n    ax[i].set_xlim(0,u_lims[i])\n\nplt.tight_layout()\nplt.show()\n``` \n-->\n\nExamining the plot of @fig-helpful-vote-dist, the distribution of helpful votes appears to be exponentially distributed on a per-website basis, with many reviews having an expected total count of helpful votes centered fairly low.\n\nKnowing the distribution of our selected response variable will assist us in the modeling process.  The nature of the response variable's distribution may require us to perform transformations on features and responses (e.g. if we pursue a multiple linear regression model).\n\n\n\n### Bi/Multivariate Plots\n\n```{python fig-bivar-plot}\n#| label: fig-bivar-plot\n#| fig-cap: \"Bivariate Plot for Sensitivity and Polarity, by Site (all purchases)\"\n\n#6 plot types\n#28 total plots\n\nbivar_subj_pol = sns.FacetGrid(\n    data=review_data,\n    col='site'\n)\nbivar_subj_pol.map_dataframe(\n    sns.kdeplot,\n    x='review_subjectivity',y='review_polarity'\n)\n```\n\n\n```{python fig-bivar-plot2}\n#| label: fig-bivar-plot2\n#| fig-cap: \"Bivariate Plot for Sensitivity and Polarity, by Site (verified purchases)\"\n\n\n#6 plot types\n#32 total plots\nbivar_subj_pol2 = sns.FacetGrid(\n    data=review_data[review_data['verified_purchase']==1],\n    col='site'\n)\nbivar_subj_pol2.map_dataframe(\n    sns.kdeplot,\n    x='review_subjectivity',y='review_polarity'\n)\n```\n\n\nIn @fig-bivar-plot and @fig-bivar-plot2, we observe the comparison of multiple features like review polarity, review subjectivity and verified purchases in the form of a Kernel Densoty Plot. The 2 different visuals depict the difference between the whole dataset and after the filter of `verified_purchase` = 1 is applied. This difference may lead to variations in the distribution and relationship between subjectivity and polarity of reviews across different sites, particularly if there are differences in the characteristics of verified and non-verified purchases.\n\nWe can see 2 major clusters at (0,0) which are mostly outliers where a review is very short in length. The second cluster around the area where subjectivity is about 0.5 suggests that as the review increases in subjectivity, i.e. the higher an opinionated a review is, the polarity also increases. \n\nThe isolated data points or \"islands\" outside of the main clusters suggest outliers or unique instances within the dataset. These isolated points may represent reviews that deviate significantly from the overall patterns observed in the data. They could indicate rare or extreme cases that warrant further investigation. For instance, these outliers might correspond to highly subjective or polarized reviews that are not typical of the majority of reviews.\n\n```{python fig-violin-plots1}\n#| label:    fig-violin-plots1\n#| fig-cap:  \"Violin Plots of Review Star-Rating vs. Subjectivity, by Site\"\n\n#7 plot types\n#35 total plots\n\nvplot_polarity_stars = sns.FacetGrid(\n    data = review_data,\n    col='site'\n)\nvplot_polarity_stars.map_dataframe(\n    sns.violinplot,\n    x='review_star_rating',\n    y='review_subjectivity'\n)\nplt.show()\n```\n\n```{python fig-violin-plots2}\n#| label: fig-violin-plots2\n#| fig-cap: \"Violin Plots for Star Rating vs. Polarity, by Site\"\n\n#7 plot types\n#38 total plots\nvplot_polarity_stars = sns.FacetGrid(\n    data = review_data,\n    col='site'\n)\nvplot_polarity_stars.map_dataframe(\n    sns.violinplot,\n    x='review_star_rating',\n    y='review_polarity'\n)\nplt.show()\n```\n\nGenerally, in @fig-violin-plots1 and @fig-violin-plots2, we see a trend for the median polarity and subjectivity of each review to increase as the star rating increases.  We also see that, generally, the data suggest that we have a minimum of neutral polarity that tends towards positive as star rating increases.\n\nSince both median subjectivity and polarity seem to increase with respect to star rating, such a correlation could be useful to us in multiple linear regression, and is generally useful to us for consideration when pursuing model development.\n\n### Hypothesis Testing for Key Feature and Response Variables\n\nSome key features we plan to explore in our modeling include review subjectivity and review polarity.  Knowing whether or not there is a significant difference for these features between the websites on which they're hosted will inform us during model selection, design, and implementation.  As such, we'll perform ANOVA and Tukey Honest Significant Difference Tests on these variables between each site.\n\n#### ANOVA Testing\n\nTo perform our ANOVA testing, we'll evaluate each dataset's review polarity and subjectivity as the mean measure, and the website on which the review was posted as the treatment variable.  Prior to performing our one-way ANOVA, we'll filter the datasets down to eliminate outliers, such that the data may represent the outcomes depicted in @fig-qq-plot-subj-no-out and @fig-qq-plots-pol-no-out.  An assumption of ANOVA testing is that the source data (and its respective groups) adhere to the normal distribution.  \n\nWe are leveraging Welch ANOVA and operating under an assumption that the variances between the groups are not equal, as visually evidenced in @fig-outliers-polarity and @fig-outliers-subjectivity.\n\nHypotheses:\n\n* Test 1: \n    * $H_0: \\mu_{\\text{Subj,Amazon}}=\\mu_{\\text{Subj,BestBuy}}=\\mu_{\\text{Subj,Target}}$\n\n    * $H_1:$ at least one mean for review subjectivity is different.\n\n* Test 2: \n    * $H_0: \\mu_{\\text{Polr,Amazon}}=\\mu_{\\text{Polr,BestBuy}}=\\mu_{\\text{Polr,Target}}$\n\n    * $H_1:$ at least one mean for review polarity is different.\n\n* For both tests, $\\alpha=0.003$\n\n```{python tbl-welch-anova-polarity}\n#| label: tbl-welch-anova-polarity\n#| tbl-cap: Welch ANOVA Results (Polarity)\n\n#conducting a welch_anova test for \n#difference of treatment (e.g. website vs. score)\n#import pingouin as pg \n#welch_anova\n#pg.welch_anova(data=df, between='company',dv=star_rating|sentiment|polarity|total_star_rating)\n\nfiltered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])\n\ndisplay(\n    pg.welch_anova(data=filtered_data,between='site',dv='review_polarity').style.hide(axis='index')\n)\n```\n\n```{python tbl-welch-anova-subjectivity}\n#| label: tbl-welch-anova-subjectivity\n#| tbl-cap: Welch ANOVA Results (Subjectivity)\n\n#conducting a welch_anova test for \n#difference of treatment (e.g. website vs. score)\n#import pingouin as pg \n#welch_anova\n#pg.welch_anova(data=df, between='company',dv=star_rating|sentiment|polarity|total_star_rating)\n\nfiltered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])\n\ndisplay(\n    pg.welch_anova(data=filtered_data,between='site',dv='review_subjectivity').style.hide(axis='index')\n)\n```\n\nThe output of both Welch ANOVA tests suggests that the means for review subjectivity and review polarity, given the website on which it was posted, have a statistically significant difference.  We'll seek to visualize these differences using a plot of the Tukey Honest Significance Test.\n\n#### Tukey Tests with E-Commerce Platform as Treatment\n\n```{python fig-tukey-tests}\n#| label: fig-tukey-tests\n#| fig-cap: \"Tukey Tests for Star Rating, Polarity, and Subjectivity, by-Site\"\n\n# star = pairwise_tukeyhsd(\n#     endog=review_data['review_star_rating'],\n#     groups=review_data['site'],\n#     alpha=0.003\n# )\n\n#8 plot types\n#40 total plots\n\nfiltered_data = pd.DataFrame(review_data[review_data['site_outlier']==0])\n\npol = pairwise_tukeyhsd(\n    endog=filtered_data['review_polarity'],\n    groups=filtered_data['site'],\n    alpha=0.003\n)\nsubj = pairwise_tukeyhsd(\n    endog=filtered_data['review_subjectivity'],\n    groups=filtered_data['site'],\n    alpha=0.003\n)\n\nfig,axes=plt.subplots(nrows=2,ncols=1)\nfig.set_figwidth(7)\nfig.set_figheight(6)\n\ntests = [\n    #(star,\"Tukey Test - Star Rating (95%)\"),\n    (pol,\"Tukey Test - Polarity (99.7%)\"),\n    (subj,\"Tukey Test - Subjectivity (99.7%)\")\n]\n\nfor i in range(len(tests)):\n    tests[i][0].plot_simultaneous(ax=axes[i])\n    axes[i].set_title(tests[i][1])\n\nplt.tight_layout()\nplt.show()\n\n```\n\nThe Tukey honest significance tests, depicted in @fig-tukey-tests suggest some interesting patterns between the three websites.  Namely, target and best buy seem to have higher polarity, and subjectivity than the same variables for Amazon!  Additionally, for each variable and each website, it seems there is no overlap in the variables at the 99.7% confidence level.\n\nThese statistically significant differences between the reviews, treated by website, indicate to us that we should proceed with caution in our modeling phase.  Namely, it may be necessary to include an explicit variable or feature accounting for the source website in our modeling process as a predictor for the response variable.\n\n<!-- \n### Exploring Sentiments - Star Rating vs. Sentiment Variables\n\n* Inter-Website Comparison of Product Reviews\n\n    * Same Product \n\n        * Clustering? \n\n        * Distances?\n\n    * All Products\n\n    * Inspect the following, visually: \n    \n        * Product Ratings\n        \n        * Customer Sentiments **try to score before plotting & turn-in**\n        \n        * Review Polarity **try to score and store before plotting**\n        \n        * Naive Bayes Classifier\n        \n        * Reliability estimates\n\n        * Product description subjectivity scores **try to score and store before turn-in** \n        \n        * Average / Spread of number of ratings per product, **try to score and store before turn-in** \n        \n        * Average/Spread of Useful Votes per Product Review, **try to score and store before turn-in** \n        \n        * Inspection of Data and / or Scoring using Kansei method.\n\n* Will need to take note on if / how these variables conform to some form of statistical distribution (uniform, normal, exponential, etc)\n\n-->\n\n## Data Before / After\n\nMuch of our data cleaning occured during the collection process.  Our team took specific steps to pursue cleaning during collection to simplify the process of bringing all information together:\n\n* Using regular expressions to extract key values from text blocks\n\n* Leveraging XPATH, class names, and element IDs to identify HTML fields in which our desired data points resided\n\nPost-scraping, we had to pursue some additional cleanup\n\n* Removal of unicode characters from review content where possible through coding and scripting.\n\n* Conversion of numbers, stored as strings, to integers (i.e. star ratings, cost/dollar amounts)\n\n* Handling of missing values (i.e. no ratings, no star ratings, no cost listed)\n\nA particular challenge we came across during the data cleaning process was the handling foreign language reviews, highly repetitive reviews, and misspelled reviews.  To better support our calculated measures for subjectivity and polarity, we leveraged the langdetect library to attempt to classify the languages of each of our 45,000+ reviews collected.\n\n```{python tbl-foreign-lang-reviews}\n#| label: tbl-foreign-lang-reviews\n#| tbl-cap: Examples of reviews written in foreign languages\nforeign_lang = pd.DataFrame(\n    review_data_base[review_data_base['review_lang']!='en']\n)\n#pd.set_option('display.height',30)\n# pd.set_option('display.min_rows',5)\n# pd.set_option('display.max_rows',None)\n#40:45, 40:45, 3:8\nsliced_frame = pd.DataFrame(\n    pd.concat([\n        foreign_lang[foreign_lang['site']=='Amazon'][0:5][['site','reviewer_name','review_content']],\n        foreign_lang[foreign_lang['site']=='BestBuy'][0:5][['site','reviewer_name','review_content']],\n        foreign_lang[foreign_lang['site']=='Target'][0:5][['site','reviewer_name','review_content']]\n    ])\n)\npd.options.display.max_rows = 100\nsliced_frame\n\n# # display(\n# #     sliced_frame.head(20)\n# # )\n# from IPython.display import Markdown\n# Markdown(sliced_frame.to_markdown(index=False))\n```\n\nIn some cases, the language classification by langdetect was a false negative (i.e. classified as a language other than english, when it was indeed English).  In our data exploration, we found that many of these false positives were outliers in other categories (whether for review length, review subjectivity, review polarity, or star rating).  As such, we find it prudent to exclude these reviews from our dataset when pursuing model development.  \n\nIn total, langdetect classified fewer than 440 reviews (accounting for less than 1% of our collected reviews) as being non-English, or being repeated words or gibberish.  Excluding these reviews should have minimal impact on the pursuit of model development.\n\nTextBlob also offers us the ability to attempt to correct the spelling of reviews.  Due to the amount of time it would take us to pursue spelling corre\n\nHere are some additional examples of gibberish or non-contributional text that impact calculations for review subjectivity and polarity.  While some of these could potentially provide value with deeper analysis, we find that these will not contribute significantly to our research.\n\n```{python}\npd.DataFrame(\n    review_data_base[review_data_base['review_lang']=='Unk'][['site','reviewer_name','review_content']]\n)\n```\n\nWe are retaining the totality of the data we've collected, and will filter the data based upon our findings here so as to keep the most relevant and supportive data in building our models.\n\n## Insights from Collection and EDA \n\nAs in @percUse, we find that online reviews, this time across multiple websites, tend toward positivity.  The values for star rating tend toward the 3 to 5 out of 5 star range, the polarity tends toward positive as star rating increases, and subjectivity (and one might argue, expressiveness) increases with star rating as well.  These correlations can prove useful to us in our research.\n\nWe've also witnessed, tested, and verified that there is a statistically significant difference for review subjectivity and polarity, given the comment was hosted on a specific website.  In light of these significant test results, we believe it may be necessary to account for the specific website from whence a review originates in training, testing, and validation data.  The stark differences, without adjustment, could negatively impact the performance of any models if we fail to account for these differences therein.  \n\nThe nature of the distribution of useful votes for a review poses a potential challenge to our research.  The exponential distribution of useful votes could prove difficult to predict, as more and more useful votes become exceedingly rare for a given customer comment.  As such, we may have an easier time with *categorizing* a review as being useful or not useful, in lieu of *predicting* a numeric value to represent how useful a comment is (e.g. predict the number of votes in favor of a comment as being useful to other customers).\n\nFurthermore, exclusion of outliers could also pose challenges to our research.  When excluding outliers, since the distribution of star rating tends toward the more positive reviews and results (reference @fig-outliers-star-rating), we could inadvertently build models that perform the same way and are less able or unable to effectively categorize the usefulness of a lower star-rated review comment.  With the nature of these outliers and the fact that having a high number of useful votes in and of itself is an outlier, we may need to examine building models upon the normalized data (e.g. the filters applied in @fig-qq-plot-subj-no-out and @fig-qq-plots-pol-no-out) as well as the negation of that normalization, focusing on the outliers, so as to ensure that all the cases for our models are covered.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"output-file":"data.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","bibliography":["refs.bib"],"nocite":"@*\n","theme":["cosmo","custom.scss"],"page-layout":"full","smooth-scroll":true,"citations-hover":true,"link-citations":true,"grid":{"body-width":"1000px","sidebar-width":"150px"},"quarto-required":">= 1.4.0"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":"../docs","link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"output-file":"data.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["refs.bib"],"nocite":"@*\n","quarto-required":">= 1.4.0","documentclass":"scrreprt","number-depth":4,"geometry":["margin=0.5in"],"link-citations":true,"header-includes":["\\usepackage{float}","\\usepackage{booktabs, caption, longtable, colortbl, array}","\\floatplacement{table}{H}","\\floatplacement{image}{H}"],"hyperrefoptions":["linktoc=all"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}