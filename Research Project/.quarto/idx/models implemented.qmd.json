{"title":"Models Implemented","markdown":{"headingText":"Models Implemented","headingAttr":{"id":"sec-models-implemented","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{python importsBlock}\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import tree\nfrom sklearn.metrics import (\n    accuracy_score,f1_score,\n    precision_score,recall_score, \n    confusion_matrix, ConfusionMatrixDisplay,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import svm\nfrom imblearn.over_sampling import ADASYN, SMOTE\nimport random\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport statsmodels.api as sm\n\n```\n\n```{python DataImports}\nproduct_data = pd.read_csv('../data/master_product_list.csv')\nreview_orig = pd.read_csv('../data/reviews_with_stats.csv')\nreviews = pd.read_csv('../data/reviews_outlier_adjusted.csv')\nreviews['prodSiteID'] = reviews['prodSiteID'].str.upper()\nreview_orig['prodSiteID'] = reviews['prodSiteID'].str.upper()\nproduct_data['prodSiteID'] = product_data['prodSiteID'].str.upper()\n\nreviews = reviews.merge(\n    right=pd.DataFrame(\n      product_data[\n        [\n            'product_price','prodSiteID',\n            'prod_subjectivity','total_star_rating','site'\n        ]\n      ]\n    ),\n    on='prodSiteID',\n    how='left'\n)\n\nreview_orig = review_orig.merge(\n    right=pd.DataFrame(\n      product_data[\n        [\n            'product_price','prodSiteID',\n            'prod_subjectivity','total_star_rating','site'\n        ]\n      ]\n    ),\n    on='prodSiteID',\n    how='left'\n)\n\nreviews['is_helpful'] = np.select(\n    [\n        reviews['review_helpful_votes'] > 0,\n        (reviews['review_helpful_votes'] == 0) | (reviews['review_helpful_votes'].isna()),\n    ],\n    [\n        1,\n        0\n    ]\n)\n\nreview_orig['is_helpful'] = np.select(\n    [\n        review_orig['review_helpful_votes'] > 0,\n        (review_orig['review_helpful_votes'] == 0 )| (reviews['review_helpful_votes'].isna()),\n    ],\n    [\n        1,\n        0\n    ]\n)\n\nreviews['site'] = np.select(\n    [\n        reviews['site_y'].str.lower() == 'amazon',\n        reviews['site_y'].str.lower() == 'target',\n        reviews['site_y'].str.lower() == 'bestbuy'\n    ],\n    [\n        1/3,2/3,1\n    ]\n)\n\nreview_orig['site'] = np.select(\n    [\n        review_orig['site_y'].str.lower() == 'amazon',\n        review_orig['site_y'].str.lower() == 'target',\n        review_orig['site_y'].str.lower() == 'bestbuy'\n    ],\n    [\n        1/3,2/3,1\n    ]\n)\n\ntest_metrics = pd.DataFrame(\n    {\n        'Model':[],\n        #'Useful Level':[],\n        'Accuracy':[],\n        'F1':[],\n        'Precision':[],\n        'Recall':[]\n    }\n)\n\nreviews = reviews.dropna(subset=[\n    'review_star_rating_adjusted', 'review_helpful_votes_adjusted',\n    'review_subjectivity_adjusted', 'review_polarity_adjusted',\n    'review_length_adjusted','prod_subjectivity','total_star_rating','site'\n],axis=0)\nreview_orig = review_orig.dropna(subset=[\n    'review_star_rating', 'review_helpful_votes',\n    'review_subjectivity', 'review_polarity',\n    'review_length','prod_subjectivity','total_star_rating','site'\n],axis=0)\n\n# tmp = pd.DataFrame(\n#     reviews[reviews['productID'].isin(list(range(1,16)))]\n# )\n\ntmp = reviews[reviews['productID'].isin(list(range(1,16)))].copy()\n\ntrain_frame = pd.DataFrame(\n    tmp.sample(n=int(.8*len(tmp)),random_state=1337)\n)\n\ntest_frame = pd.DataFrame(\n    tmp.loc[~tmp.index.isin(train_frame.index)]\n)\n\nadditional_data = review_orig[~review_orig['prodSiteID'].isin(train_frame['prodSiteID'].unique())]\n\ntest_frame = pd.concat([test_frame, additional_data], ignore_index=True)\n\ntest_frame = test_frame.sample(n=int(.2*len(tmp)),random_state=1337)\n\n# test_frame = pd.DataFrame(\n#     review_orig[~review_orig['prodSiteID'].isin(train_frame['prodSiteID'].unique())]\n# )\n```\n\n```{python test}\n\n# print(test_frame.shape)\n# print(train_frame.shape)\n```\n\n```{python DataTransforms}\nX_train,X_test,y_train,y_test= [\n    StandardScaler().fit_transform(np.array(train_frame[[\n        'review_star_rating_adjusted', #'review_helpful_votes_adjusted',\n        'review_subjectivity_adjusted', 'review_polarity_adjusted',\n        'review_length_adjusted','prod_subjectivity','total_star_rating','site'\n    ]])),\n    StandardScaler().fit_transform(np.array(test_frame[[\n        'review_star_rating', #'review_helpful_votes',\n        'review_subjectivity', 'review_polarity',\n        'review_length','prod_subjectivity','total_star_rating','site'\n    ]])),\n    np.array(train_frame['is_helpful']),\n    np.array(test_frame['is_helpful'])\n]\n```\n\n\n```{python PCATransforms}\npca = PCA()\npcs = pca.fit_transform(X_train)\npr_df = pd.DataFrame(data=pcs)\npr_df.columns = [\"PC{}\".format(i) for i in range(1,len(pca.components_)+1)]\nexp_var = pca.explained_variance_ratio_\ncum_var = np.cumsum(exp_var)\ncum_var_df = pd.DataFrame({\n    'Principal Component':[f'PC{i+1}' for i in range(len(cum_var))],\n    'Cumulative Variance':cum_var,\n    'Explained Variance':exp_var\n})\n#display(cum_var_df)\npca = PCA(n_components=6)\nw=pca.fit(X_train)\nX_train_pca=pca.transform(X_train)\nX_test_pca=pca.transform(X_test)\n```\n\nIn our modeling of the collected data, we seek to investigate several models for the generalization of the work performed by @percUse.\n\nWe will examine, compare, and contrast the use of the following models: \n\n* Multiple Linear Regression Prediction\n\n* Logistic Regression Classification\n\n* K-Nearest Neighbors Classification\n\n* Support Vector Machine Classification\n\n## Included Variables\n\n```{python tbl-var-selection}\n#| label: tbl-var-selection\n#| tbl-cap: Selected Variables for Model Training\n\npd.DataFrame({\n    'Variable':['review_star_rating', #'review_helpful_votes',\n        'review_subjectivity', 'review_polarity',\n        'review_length','prod_subjectivity','total_star_rating', 'site', 'review_helpful_votes'],\n    'Pre-Transformation Type':['int','float','float','int','float','float','dummy float','int'], #'int',\n    'Post-Transformation Type':['PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float','PCA scaled float'],\n    'Purpose':['feature','feature','feature','feature','feature','feature','feature','response'],\n    'Reason for Inclusion':['literature survey','literature survey','literature survey','literature survey','intuition','literature survey', 'intuition from EDA','literature survey']\n}).style.hide(axis='index')\n```\n\n## Data Adjustments\n\nAs noted in our exploratory data analysis, each individual site has statistically significant differences in key variables we're considering in our modeling.  To mitigate the potential for under or overfitting, and misrepresentation due to variable scale we perform the following transformations to our data:\n\n<!--NOTE - need information here on specific variables under consideration for our modeling-->\n\n1. Variable outlier adjustment.  We noted in our EDA that each of the e-commerce platforms had high volumes of outliers with respect to the inter-quartile range.  We applied a transformation to our data to map any outlier variable value on a per-website basis from its value to $\\mu+3\\cdot sd(\\text{variable})$ for high-end outliers, and $\\mu-3\\cdot sd(\\text{variable})$ for low-end outliers.  In the event that either of these values exceeded the minimum or maximum value of the dataset, we mapped the value to the minimum or maximum accordingly.\n\n2. Standard scaling of variables.  After adjusting outliers, we re-mapped all of our feature variables to be on the scale of the standard normal distribution $N\\sim(0,1)$\n\n3. Response variable transformation to binary value.  We denoted a single useful vote as meaning that the review was useful to customers, and mapped the value to True/1, and False/0 otherwise.\n\nHere is a sample (first 10 observations) of our data prior to the transformation:\n\n```{python tbl-pre-xform-data}\n#| label: tbl-pre-xform-data\n#| tbl-cap: Data (pre-transformation)\n\n#Let's look at this from the perspective of\n#key variables we're exploring...\nreviews[['review_star_rating', \n        'review_subjectivity', \n        'review_polarity',\n        'review_length',\n        'prod_subjectivity',\n        'total_star_rating','site']].head(10).style.hide(axis='index')\n```\n\nAnd here is a sample of our data after the applied transformations:\n\n* Snippet of first 10 observations in our training dataset, post-transformation:\n\n```{python tbl-post-xform-data}\n#| label: tbl-post-xform-data\n#| tbl-cap: Data (post-transformation)\n\npd.DataFrame(X_train, columns=[\n    'review_star_rating', #'review_helpful_votes',\n    'review_subjectivity', \n    'review_polarity',\n    'review_length',\n    'prod_subjectivity',\n    'total_star_rating','site']).head(10).style.hide(axis='index')\n\n\n```\n\nClasses in the response variable (the number of helpful votes a review received) set were mapped as follows for all non-linear models:\n\n* 0: if the review had no helful votes\n\n* 1: if the review had one or more than 1 helpful votes\n\nOur reasoning for this transformation is that, across the totality of our data, a comment receiving a vote as being useful is exceedingly rare, as uncovered during our exploratory data analysis.  As such, even a single vote for being useful should put the comment in the running for being considered useful. \n\n### Dimensionality Reduction\n\nFor all models outside of the multiple linear regression, we performed a principal component analysis on the scaled data.  \n\n```{python tbl-PCA-Info}\n#| label: tbl-PCA-Info\n#| tbl-cap: Cumulative Variance of Principal Components (selected features)\ndisplay(cum_var_df.style.hide(axis='index'))\n```\n\nTo reduce dimensionality for Logistic Regression, Support Vector Machine, and K-Nearest Neighbor models, we elected to reduce from 7 principal commponents to 6.  From the above table, we see that these 6 components explain approximately 95% of the variation within the training data.  We projected our training and testing data from their 7-dimensional feature space to a reduced 6-dimensional principal component vector space.\n\n### Training Data\n\nTo train our dataset, we leveraged the data post-transformation to train each of our models, including the adjustments of outlier datapoints to being within 3 standard deviations of the mean of each variable.  We selected an 80% sample of this data and leveraged the same dataset to train each model.\n\nIn the interest of generalization, we sought to take our training dataset solely from products that were common across all three e-commerce platforms.  By working with common data from each site, the data and the models may be able to formulate a more generalized construct of how certain variables behave within the different contexts of each platforms, and assist in better prediction and classification of comments as being useful or not.\n\n### Testing Data\n\nFor testing, we evaluated each model against transformed data, omitting the transformation of outliers to being within 3 standard deviations of the mean.  We performed this action to enable a fair comparison of each model against one another when working with real-world data.\n\n### Desired Outcomes and Objectives\n\nPredictive modeling for the usefulness of a user comment on a product, in and of itself, cannot be conducted 100% objectively.  We are interested in the exploration of misclassifications - particularly of false positives.  \n\nOur data exploration revealed that having even a single vote for a comment as being useful was exceedingly rare, with a median and mean number of votes hovering at or about 0 regardless of the website on which the comment was posted.\n\nAdditionally, from a technological perspective, the ability of a web user interface programmer or designer to simply filter or arrange comments by the number of votes they received is trivial.\n\nWith the above considerations in mind, we are interested in a model that provides reasonable accuracy while also having an apprpriate amount of recall and a reasonable F1 score.  Having a model that perfectly maps predictions to actual outcomes is not useful to this research.  Having a number of classified false positives for exploration and subjective evaluation is what interests us.\n\n## Examination of the Original Multiple Linear Regression\n\n### Linear Model Construction\n\nWe leveraged a similar formulation to that which was used within the research of @percUse.\n\nOur version of the linear model is designed as follows:\n$\\hat{y} = \\beta_0+\\beta_1X_{rsr}+\\beta_2X_{rs}+\\beta_3X_{rp}+\\beta_4X_{rl}+\\beta_5X_{ps}$ $+\\beta_6X_{tsr}+\\beta_7X_{s}$\n\nWhere each of the following variables have been standard-scaled to a range between 0 and 1 for the input data:\n\n* $X_{rsr}$ corresponds to the individual review's star rating\n\n* $X_{rs}$ corresponds to the review's subjectivity score\n\n* $X_{rp}$ corresponds to the review's polarity score\n\n* $X_{rl}$ corresponds to the review's length (in words)\n\n* $X_{ps}$ corresponds to the product description subjectivity score \n\n* $X_{tsr}$ is the overall star rating for the product.\n\n* $X_{s}$ is the site on which the comment was found (converted to a dummy variable for each website).\n\nWe leveraged $X_{ps}$ as a proxy for the previous model's binary attribute for whether or not a good was search-based or experienced based.  @percUse leveraged a set of binary variables to classify a good as being search, experience, or mixed products.  Our intuition was that, given the subjectivity of a product's description and/or specifications, a higher subjectivity score would correspond to an experience-based good, a lower subjectivity would correspond to a search-based good, and everything in-between would be a mixed product.  This construction allows for any product to have a continuous potential range, and for most products to be mixed products (some tending more toward experience or search).\n\n<!-- \nThe methods of @percUse leveraged a binary class feature to denote whether a product was a search good, an experience good, or a mixed good.  Our exploration leveraged a continuous scale variable from 0 to 1, being the subjectivity of the product description and specifications.  Our intent was to examine whether their model could be simplified using this single continuous variable.  In theory, a product within an e-commerce platform could be considered a search good if it has more specificity (thus less subjectivity), and an experience good if it has greater subjectivity, and any product could lie anywhere within that spectrum of being a degree of search and a degree of experience. \n-->\n\n### Inspection of Linear Model Assumptions\n\nGenerally, our examination of the multiple linear regression performed by @percUse failed to meet the assumptions of linear regression (normality of residuals, linear pattern in fitted vs. observed values, and constant variance of residuals). \n\n#### Linearity of the Model\n\nOur model failed to achieve any clear form of linearity between fitted and observed values.\n\n```{python fig-mlr-results}\n#| label: fig-mlr-resultss\n#| fig-cap: Linear Relationship Between Predictors and Response\n\n##train - test\n\ny_train_mlr,y_test_mlr = [np.array(train_frame['review_helpful_votes']),\n    np.array(test_frame['review_helpful_votes'])]\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n#from sklearn.linear_model import LassoLarsIC\n\nmlr = LinearRegression()\n\nmlr.fit(X_train,y_train_mlr)\n\ny_hat_base = mlr.predict(X_test)\n\nbeta_base = mlr.coef_\nbeta0_base = mlr.intercept_\nR2_base = r2_score(y_test_mlr,y_hat_base)\nMSPE_base = np.mean((y_hat_base - y_test_mlr)**2)\n\n\nplt.title('Multiple Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Observed Values')\nplt.scatter(\n    y_hat_base,y_test_mlr    \n)\nplt.show()\nn,p = X_train.shape\naR2 = 1-(1-R2_base) * ((n-1)/(n-p))\n#display(R2_base,aR2)\n```\n\nThe fitted vs. observed values for this plot are not indicative of a linear pattern between the feature and response variables.  It provides a high mean square prediction error of `{python} round(MSPE_base,4)`, an $R^2$ of `{python} round(R2_base,4)` and adjusted $R^2$ value of `{python} round(aR2,4)`.  The lack of even a moderate correlation here suggests one of the following:\n\n* The wider spread of data from multiple websites and wider range of products reduced the correlation found by @percUse\n\n* The linear model is not generalizable.\n\n* The model is no better than randomly guessing the number of votes a comment could or should have associated with it.\n\n<!--Our testing produced the following coefficients:\n\n$\\beta_0$: `{python} round(mlr.intercept_,3)`, $\\beta_1$: `{python} round(mlr.coef_[0],3)`,$\\beta_2$: `{python} round(mlr.coef_[1],3)`,$\\beta_3$: `{python} round(mlr.coef_[2],3)`,$\\beta_4$: `{python} round(mlr.coef_[3],3)`,$\\beta_5$: `{python} round(mlr.coef_[4],3)`,$\\beta_6$: `{python} round(mlr.coef_[5],3)`\n\nOn new training data, the model appears to underpredict the output value, across the board, for the new data.  That being said, there is still a strong linear trend between the predictors and the response.  The intent of this model is not to predict the actual number of votes that a review might have, but whether or not that review would be considered useful to customers.  \n\nGenerally examining this result, we would conclude that there is a linear trend between these features, their coefficients, and their corresponding response.-->\n\n#### Homoscedasticity on Normalized Data\n\nThis model has substantial challenges with heteroscedasticity.  Let's examine a plot of fitted vs. residuals in the model:\n\n```{python fig-non-constant-var-1}\n#| label: fig-non-constant-var-1\n#| fig-cap: Heteroscedasticity in Model\n\nplt.scatter(\n    y_hat_base,y_hat_base - y_test_mlr\n)\nplt.xlabel('Predicted Value')\nplt.ylabel('Model Residuals')\nplt.title('Evaluating Constant Variance - Predicted vs. Residual Values')\nplt.show()\n```\n\nAs expected for a model that does not have a clear linear pattern, the residuals for this linear model are heteroscedastic.  We should expect to see a constant variance in a plot of predicted vs. residual values, with no correlation between the errors and the predictions.  Here, we witness this issue directly, lending to the idea that the model is a poor fit for the data, and the nature of the model (e.g. additional data, additional features, or other feature/response transformations) would need to change substantially to produce effective predictive results.\n\n#### Normality in Residuals\n```{python fig-qq-resid}\n#| label: fig-qq-resid\n#| fig-cap: Model Residuals are Abnormal\n\nx_points = y_hat_base\nfig,ax = plt.subplots()\nmu = x_points.mean()\nsd = x_points.std()\nnorm = (x_points-mu) / sd\nsm.qqplot(norm,ax=ax)\nax.axline((0,0),slope=1,color='black')\nfig.suptitle('Q-Q Plot of Residuals')\nplt.show()\n```\n\nFrom the above plot, it is clear that this model fails to adhere to the multiple linear regression requirement for residual normality.\n\n#### Conclusion on MLR model\n\nExpanding the Multiple Linear Regression beyond the scope of the study performed by @percUse seems to fail all of the assumptions of a linear model, and does not appear to produce a tracable predictive pattern that may support classification of a review comment as being useful or not to prospective customers.\n\n## Logistic Regression Classification\n\nGiven the aforementioned challenges with the linear model, our next choice for examination was logistic regression.  The MLR called for use of only numeric or continuous variables.  Logisitic regression enables us to examine the inclusion of additional categorical variables as part of the regression consideration.\n\n### Hyperparameter Tuning\n\nLogistic regression is one of the best performing models in the project after SVM <!--with respect to which metrics?-->. We inspected 3 logistic regression models - one was tuned using the class weight hyperparameter to address the class imbalance present in the dataset. By assigning a higher weight to the minority class (useful level 1) and a lower weight to the majority class (useful level 0), the model was able to better capture the patterns associated with the minority class, leading to improved performance metrics.\n\n### Oversampling techniques\n\nAdditionally, 2 more logistic regression models are trained using two oversampling techniques, namely ADASYN and SMOTE. ADASYN, which generates synthetic samples for the minority class based on their difficulty in learning regions, and SMOTE, which creates synthetic samples by interpolating between existing minority class samples, were used to address the class imbalance problem. These techniques help to provide the model with more balanced training data, allowing it to learn the characteristics of both classes more effectively.\n \nIt is a well know fact that oversampling techniques are employed if there a severe class imbalance if hyperparameter tuning does not improve the model performance. However, despite the heavy class imbalance, the tuned model and the SMOTE model achieve great result with 98% accuracy indicating that it is proficient at making correct predictions. <!--adjust this?-->68% recall is decent, but indicates that the model maybe classifying the positive instances from minority class incorrectly. \n\n### Logistic Regression Test Results\n\n```{python tbl-log-reg-results}\n#| label: tbl-log-reg-results\n#| tbl-cap: Test Scores for Logistic Regression\n\n# need to divide into pieces to add figures / labels for each.\n#Logmod hyperparameter\nrandom.seed(123)\n\nLogMod = LogisticRegression(class_weight={0:.1,1:.9})\nLogMod.fit(X_train_pca,y_train)\ny_pred = LogMod.predict(X_test_pca)\n#y_pred = (LogMod.predict_proba(X_test_pca)[:,1]>0.25)\n\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'Logistic Regression (TUNED)',\n    #'Useful Level': \"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred)\n}\n# ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot()\n# display(test_metrics)\n\n#### Logmod ADASYN\n\nadasyn = ADASYN()\nX_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pca, y_train)\nlogmod_adasyn = LogisticRegression()\nlogmod_adasyn.fit(X_train_adasyn, y_train_adasyn)\ny_pred_adasyn = logmod_adasyn.predict(X_test_pca)\n\n\ntest_metrics.loc[len(test_metrics)] = {\n    'Model': 'Logistic Regression (ADASYN)',\n    #'Useful Level': \"above 0\",\n    'Accuracy': accuracy_score(y_test, y_pred_adasyn),\n    'F1': f1_score(y_test, y_pred_adasyn),\n    'Precision': precision_score(y_test, y_pred_adasyn),\n    'Recall': recall_score(y_test, y_pred_adasyn)\n}\n\n# ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_adasyn)).plot()\n# display(test_metrics)\n\nX_test_scaled = StandardScaler().fit_transform(X_test)\nX_test_pca = pca.transform(X_test_scaled)\n\n#### Logmod SMOTE\n\nsmote = SMOTE()\nX_train_smote, y_train_smote = smote.fit_resample(X_train_pca, y_train)\nlogmod_smote = LogisticRegression()\nlogmod_smote.fit(X_train_smote, y_train_smote)\n\ny_pred_smote = logmod_smote.predict(X_test_pca)\n\ntest_metrics.loc[len(test_metrics)] = {\n    'Model': 'Logistic Regression (SMOTE)',\n    #'Useful Level': \"above 0\",\n    'Accuracy': accuracy_score(y_test, y_pred_smote),\n    'F1': f1_score(y_test, y_pred_smote),\n    'Precision': precision_score(y_test, y_pred_smote),\n    'Recall': recall_score(y_test, y_pred_smote)\n}\n\n\n# ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_smote)).plot()\n# display(test_metrics)\n\n# test_metrics_df = pd.DataFrame(test_metrics)\n# display(test_metrics_df[:3].style.hide(axis='index'))\n\ndisplay(\n    test_metrics[test_metrics['Model'].str.contains('Logistic')].style.hide(axis='index')\n)\n\n```\n\n```{python fig-cm-logreg}\n#| label: fig-cm-logreg\n#| fig-cap: Logistic Regression Confusion Matrices\n#| fig-align: center\n\nfig, axes = plt.subplots(3, 1, figsize=(18, 6))\n\nConfusionMatrixDisplay(confusion_matrix(y_test,y_pred)).plot(ax=axes[0])\naxes[0].set_title('Tuned Model')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_adasyn)).plot(ax=axes[1])\naxes[1].set_title('ADASYN')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_smote)).plot(ax=axes[2])\naxes[2].set_title('SMOTE')\n\nplt.tight_layout()\nplt.show()\n```\n\nAmongst these models, it appears that the tuned model provides the highest F1 score.  The recall and precision are the lowest...meaning...\n\n<!--Let's examine some of its outputs.  We'll predict the probability that the comment belongs to the \"useful\" class and sort in descending order, then filter out any comments that are flagged as not useful.  This may introduce to us new comments that are useful to prospective buyers.-->\n\n```{python UsefulComments-logreg}\n# review_orig['pred_useful'] = y_pred_adasyn\n# x = list(review_orig[(review_orig['is_helpful']==0) &(review_orig['pred_useful']==1)]['review_content'])#[['reviewer_name','verified_purchase','review_content','review_star_rating']]\n# for i in x:\n#     print(i)\n#print(len(x))\ntest_frame['pred_useful'] = LogMod.predict_proba(X_test_pca)[:,1]\ntest_frame['product'] = test_frame.apply(lambda row:product_data[product_data['prodSiteID']==row['prodSiteID']]['product_title'].iloc[0],axis=1)\ndat = test_frame[test_frame['is_helpful']==0].sort_values(by='pred_useful',ascending=False)[['site_y','product','product_price','review_header','reviewer_name','review_content','review_star_rating','review_helpful_votes','pred_useful']].copy().reset_index()\n\ndat.to_csv('../data/LogisticRegressionPredUseful.csv')\n# for i,row in dat.iterrows():\n#     print(\n#         \"Review #{} ({})\\n{}\\n\".format(i+1,row['product'],row['review_content'])\n#     )\n    #print(\"Review #{}:\\n{}\".format(i,dat[i]))\n# for comment in dat:\n#     print(comment+\"\\n\")\n\n```\n\n<!-- These comments, holding high probability from the Logistic Regression-Tuned model, seem to have the following qualities:\n\n* Longer comments with descriptive detail\n\n* ...\n\nSubjectively, if our research team were prospective buyers of these products, we find that these comments would be beneficial to us in pursuing a decision as to whether or not to purchase the product. -->\n\nComments, sorted by probabability of being useful in descending order, are located here (link).\n\n## K-Nearest Neighbors Classification\n\nK-Nearest Neighbors is used to learn and identify the target class instances. It makes predictions by calculating distance (usually, Euclidean distance) between a given instance and all other instances in the dataset in feature space. \n\n### Hyperparameter Tuning\n\nThe value of k is the most critial hyperparameter in the KNN Calssification algorithm. It determines the performance of the model. Usually a small k value leads to overly complex understanding of the data that might result into overfitting, however, a higher k values can lead to underfitting. \n\nWe have looked at multiple values of k for our given data and compared a set of model metrics - accuracy, F1 score, precision, and recall to find the most optimal model to be the model with `k = 3`.\n\nLooking the results shown below we can say that:\n\n1. KNN at each neighbor level performs poorer in comparison to logistic regression.  The accuracy and precision do not reach sufficient levels.  Furthermore, the total percent prediction of positive cases (ranging from 21.5-25.6%) far exceed the percent of true positive cases in the testing dataset (approximately 17%).  These over-optimistic prediction levels (in combination with a high number of false negatives) suggest we may not be meeting the mark with this model, as once again, votes for a comment as being useful is relatively rare in this dataset.\n\n2. For KNN with n=3 neighbors, we are the closest to the actual percent of positives in the source dataset.  This model, however, has poor performance for precision, recall, and F1.\n\n3. For KNN with n=9 neighbors, we improve the F1 score, but the precision and recall remain substantially low.\n\n4. KNN, for any number of neighbors and given our selected features, may be an insufficient model for our use case.\n\n<!--1. Just by looking at the accuracy for any given value of k, it ranges between 98-99% indicating that KNN might be a good model for the given data.\n\n2. Among the evaluated models, KNN with `k = 3` achieves the highest F1 score of approximately 78.2%, indicating a balanced trade-off between precision and recall.\n\n3. While all models have high precision and recall rates, KNN with higher k values (5, 7, and 9) demonstrate slightly lower F1 scores.-->\n\n### KNN Test Results\n\n<!--try to tweak and tune this model for better performance--> \n```{python tbl-knn-test-results}\n#| label: tbl-knn-test-results\n#| tbl-cap: Test Scores for K-Nearest Neighbors\n\n# 3 Neighbors is best performing\n# need to divide this into pieces to add figure labels.\nrandom.seed(123)\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(X_train_smote,y_train_smote)\ny_pred_3 = knn.predict(X_test_pca)\n\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'KNN (k=3)',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_3),\n    'F1':f1_score(y_test,y_pred_3),\n    'Precision':precision_score(y_test,y_pred_3),\n    'Recall':recall_score(y_test,y_pred_3)\n}\n\n# 5 Neighbors\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train_smote,y_train_smote)\ny_pred_5 = knn.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'KNN (k=5)',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_5),\n    'F1':f1_score(y_test,y_pred_5),\n    'Precision':precision_score(y_test,y_pred_5),\n    'Recall':recall_score(y_test,y_pred_5)\n}\n\n# 7 Neighbors\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train_smote,y_train_smote)\ny_pred_7 = knn.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'KNN (k=7)',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_7),\n    'F1':f1_score(y_test,y_pred_7),\n    'Precision':precision_score(y_test,y_pred_7),\n    'Recall':recall_score(y_test,y_pred_7)\n}\n\n# 9 Neighbors\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train_smote,y_train_smote)\ny_pred_9 = knn.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'KNN (k=9)',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_9),\n    'F1':f1_score(y_test,y_pred_9),\n    'Precision':precision_score(y_test,y_pred_9),\n    'Recall':recall_score(y_test,y_pred_9)\n}\n\n# Display results\n# test_metrics_df = pd.DataFrame(test_metrics)\n# display(test_metrics_df[3:7].style.hide(axis='index'))\ndisplay(\n    test_metrics[test_metrics['Model'].str.contains('KNN')].style.hide(axis='index')\n)\n\n```\n\n```{python fig-cm-knn}\n#| label: fig-cm-knn\n#| fig-cap: Confusion Matrices for KNN Models\n#| fig-align: center\n\nfig, axes = plt.subplots(4, 1, figsize=(11, 10))\n\nConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_3)).plot(ax=axes[0])\naxes[0].set_title('KNN Model (k=3)')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_5)).plot(ax=axes[1])\naxes[1].set_title('KNN Model (k=5)')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_7)).plot(ax=axes[2])\naxes[2].set_title('KNN Model (k=7)')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_9)).plot(ax=axes[3])\naxes[3].set_title('KNN Model (k=9)')\n\nplt.tight_layout()\nplt.show()\n```\n\nSimply examining the above confusion matrices, especially the upper-right hand corner false posistives, we see that KNN is likely over-optimistic about the usefulness of customer feedback.\n\n```{python UsefulComments-knn}\ntest_frame['pred_useful'] = y_pred_3\ndat = test_frame[test_frame['is_helpful']==0].sort_values(by='pred_useful',ascending=False)[['site_y','product','product_price','review_header','reviewer_name','review_content','review_star_rating','review_helpful_votes','pred_useful']].copy().reset_index()\n\n# for i,row in dat.iterrows():\n#     print(\n#         \"Review #{} ({})\\n{}\\n\".format(i+1,row['product'],row['review_content'])\n#     )\n\ndat.to_csv('../data/KNNPredUseful.csv')\n```\n\n## Support Vector Machine Classification\n\nSupport Vector Machines work on classification problems by finding an optimal hyperplane that best classifies the target classes in the given feature space. Because of its flexibility of moving the hyperplane and adapting to the intricacies of the data, SVM could be a useful and powerful algorithm for our use case.\n\n### Hyperparameter Tuning\n\nThe complexity of a SVM model is determined by the choice of kernel function that supports the capturing of nuances within the data. Below are our implmentation results of the performance metrics of Support Vector Machine (SVM) models trained with different kernel functions: polynomial (SVM-Poly), radial basis function (SVM-RBF), and sigmoid (SVM-Sigmoid). \n\nTo tune each version of this model, we leveraged a reduced set of principal components, and adjusted class weighting, as having useful votes for a product is a relative rarity across each of the e-commerce platforms.  To boost our recall, we elected to assign weights of 0.2 to class 0 (not useful) and 0.8 to class 1 (useful).\n\n### SVM Test Results\n\n```{python tbl-svm-test-results}\n#| label: tbl-svm-test-results\n#| tbl-cap: Test Scores for Support Vector Machine\n\n## need to divide this into pieces to add figure labels\n# SVM Poly\nrandom.seed(123)\n\nspt_vector = svm.SVC(\n    kernel='poly',degree=1,class_weight={0:0.2,1:0.8},probability=True,\n).fit(X_train_pca,y_train)\n\ny_pred_poly = spt_vector.predict_proba(X_test_pca)[:,1] > 0.1 #spt_vector.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'SVM-Poly',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_poly),\n    'F1':f1_score(y_test,y_pred_poly),\n    'Precision':precision_score(y_test,y_pred_poly),\n    'Recall':recall_score(y_test,y_pred_poly)\n}\n\n# SVM RBF\nspt_vector_rbf = svm.SVC(\n    kernel='rbf',degree=1,class_weight={0:0.2,1:0.8}#,probability=True\n).fit(X_train_pca,y_train)\n\ny_pred_rbf = spt_vector_rbf.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'SVM-RBF',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_rbf),\n    'F1':f1_score(y_test,y_pred_rbf),\n    'Precision':precision_score(y_test,y_pred_rbf),\n    'Recall':recall_score(y_test,y_pred_rbf)\n}\n\n# SVM Sigmoid\nspt_vector_sigmoid = svm.SVC(\n    kernel='sigmoid',degree=1,class_weight={0:0.2,1:0.8}#,probability=True\n).fit(X_train_pca,y_train)\n\ny_pred_sigmoid = spt_vector_sigmoid.predict(X_test_pca)\ntest_metrics.loc[len(test_metrics)] = {\n    'Model':'SVM-Sigmoid',\n    #'Useful Level':\"above 0\",\n    'Accuracy':accuracy_score(y_test,y_pred_sigmoid),\n    'F1':f1_score(y_test,y_pred_sigmoid),\n    'Precision':precision_score(y_test,y_pred_sigmoid),\n    'Recall':recall_score(y_test,y_pred_sigmoid)\n}\n\n# Display results\n# test_metrics_df = pd.DataFrame(test_metrics)\n# display(test_metrics_df[7:].style.hide(axis='index'))\n\ndisplay(\n    test_metrics[\n        test_metrics['Model'].str.contains('SVM')\n    ].style.hide(axis='index')\n)\n\n```\n\n```{python fig-cm-svm}\n#| label: fig-cm-svm\n#| fig-cap: Confusion Matrices for Support Vector Machines\n#| fig-align: center\n\nfig, axes = plt.subplots(3, 1, figsize=(18, 6))\n\nConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_poly)).plot(ax=axes[0])\naxes[0].set_title('SVM Poly Model')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rbf)).plot(ax=axes[1])\naxes[1].set_title('SVM RBF Model')\n\nConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_sigmoid)).plot(ax=axes[2])\naxes[2].set_title('SVM Sigmoid Model')\n\nplt.tight_layout()\nplt.show()\n```\n\n```{python UsefulComments-knn}\ntest_frame['pred_useful'] = y_pred_poly\ndat = test_frame[test_frame['is_helpful']==0].sort_values(by='pred_useful',ascending=False)[['site_y','product','product_price','review_header','reviewer_name','review_content','review_star_rating','review_helpful_votes','pred_useful']].copy().reset_index()\n\n# for i,row in dat.iterrows():\n#     print(\n#         \"Review #{} ({})\\n{}\\n\".format(i+1,row['product'],row['review_content'])\n#     )\n\ndat.to_csv('../data/SVMPolyPredUseful.csv')\n```\n\nLooking the results shown below we can say that:\n\n1. SVM with a polynomial kernel (set to degree 1 or linear) predicts a total percent of true positives close to the underlying source data (approximately 17%).  \n\n2. SVM with radial basis function kernel appears to under-predict positive cases.\n\n3. SVM with sigmoid kernel strongly over-predicts false positives\n\nSVM Poly appears to achieve reasonable accuracy while hitting an appropriate level for F1 and Recall for our use case.  The presence of false positives gives us something to subjectively examine for its usefulness as a potential customer. \n\n<!--1. SVM with a polynomial kernel (SVM-Poly) achieves the highest F1 score of approximately 81.09%. This model also exhibits perfect precision, indicating that it classifies all positive instances correctly.\n\n2. SVM with a radial basis function kernel (SVM-RBF) follows closely with an F1 score of approximately 78.15%. Although it has slightly lower precision compared to SVM-Poly, it demonstrates a balanced trade-off between precision and recall. \n\n3. SVM with a sigmoid kernel (SVM-Sigmoid) exhibits the lowest F1 score of approximately 59.24%, indicating comparatively lower performance in capturing both precision and recall.\n\nWhile all SVM models exhibit high accuracy, SVM with a polynomial kernel (SVM-Poly) emerges as the most favorable choice based on its highest F1 score and perfect precision. -->\n\n\n## Model Comparison\n\nWe examine the following table to compare and contrast our implemented models on our collected data.\n\n```{python tbl-final-results}\n#| label: tbl-final-results\n#| tbl-cap: Summary Metrics (all evaluated models)\ntest_metrics.style.hide(axis='index')\n```\n\nThe most interesting results, we find, come from the tuned Logistic Regression model and SVM-Poly models. These seem to hit a sweet spot when it comes to F1 and recall scores.  Exceeding certain thresholds (at or around 0.5), seems to have too high a percentage of false positives.  \n\n50% of our oversampled training data held votes as being useful comments, and our regular training data contained samples with approximately 7% being voted as useful.\n\nFor our testing data, approximately 17% of the records held votes for being useful.\n\nThe Tuned Logistic Regression predicted a total number of positive (true and false positives) of 1603, or 18% of the available samples.  This result is very close in proximity to the actual total for true positives within the data.\n\nThe SVM Poly model also produced a prediction of approximately 17.7% of the testing data being useful (comparable to the actual value of 17%).\n\nNone of the other model formulations or permutations achieved a percentage of total positive prediction rate as in close proximity to the actual underlying data.\n\nThe false positives for SVM-Poly and Tuned Logistic Regression are located here:\n\n* link to Logisitic Regression file here\n\n* link to SVM-Poly here\n\n<!--\nEach of these models had similar performance in terms of accuracy and precision. A key consideration for us is within the realm of recall in that false positives are potentially beneficial to the generalization of this model to identify reviews that are useful, but currently possess no helpful votes.  None of the models delivered any false positives, but having high recall may support identification of new, useful comments.\n\nThe top 2 performing models were our Support Vector Machine and Logistic Regression implementations.  Between the two, SVM had higher recall, which is preferable in our use case.  That being said, on every metric between the two models, there was near equivalent performance.\n\nWith our preferences toward higher recall, the implementation of the SVM-Sigmoid and SVM-RBF models delivered best on our objectives, identifying the highest volume of true positives, minimizing false negatives, and providing a limited number of true positives that could identify comments with no votes that are useful to customers. \n\nFrom a stakeholder standpoint, these two models can both be beneficial, whether looking from the standpoint of the customer or the e-commerce platform.\n\nThe RBF model, with more exploration and examination, could be very beneficial to e-commerce platforms.  If the model could be modified and updated to identify comments that are closer in proximity to promotional comments (which potentially come at a cost to these platforms), it may allow them to present comments similar in quality to a paid promotional review without the platform having to pay for it.\n\nThe Sigmoid model, with additional training and tuning, could be of greater benefit to customers.  This model is less inclined to capture promomtional feedback comments and provide potentially useful feedback absent other outside influence.  At a subjective level, not all of the content contained in these classified reviews are useful, though many are. <!-- any additional writeup comments here-->","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","number-sections":true,"output-file":"models implemented.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","bibliography":["refs.bib"],"nocite":"@*\n","theme":["cosmo","custom.scss"],"page-layout":"full","smooth-scroll":true,"citations-hover":true,"link-citations":true,"grid":{"body-width":"1000px","sidebar-width":"150px"},"quarto-required":">= 1.4.0"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":"../docs","link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"output-file":"models implemented.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["refs.bib"],"nocite":"@*\n","quarto-required":">= 1.4.0","documentclass":"scrreprt","number-depth":4,"geometry":["margin=0.5in"],"link-citations":true,"header-includes":["\\usepackage{float}","\\usepackage{booktabs, caption, longtable, colortbl, array}","\\floatplacement{table}{H}","\\floatplacement{image}{H}"],"hyperrefoptions":["linktoc=all"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}