[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 5302 - Final Project",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "CSCI 5302 - Final Project",
    "section": "1.1 Abstract",
    "text": "1.1 Abstract\nWe explore the research performed in Guha Majumder, Dutta Gupta, and Paul (2022), and seek to further it via their recommendations for predicting the perceived usefulness of online customer reviews to potential customers. Our work focuses on the expansion and generalization of their multiple linear regression model. To check the model’s general applicability, we collect additional products and reviews, and do so for the same products from multiple e-commerce websites to examine whether such models are applicable to any platform, or if the models may be platform-specific. Furthermore, we explore use of additional features and coefficients, and use of other prediction and classification models to assess the degree to which a customer review is useful to future customers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#concept-and-motivation",
    "href": "index.html#concept-and-motivation",
    "title": "CSCI 5302 - Final Project",
    "section": "1.2 Concept and Motivation",
    "text": "1.2 Concept and Motivation\n\nCustomers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product. According to research by Guha Majumder, Dutta Gupta, and Paul (2022), if a customer can gather and understand product quality before the purchase, it is considered a search good, while experience goods are those which must be purchased or experienced to evaluate them. When a product is more in the directon of experience vs. search-based, other customers’ experiences can shed light on its features and return on investment than information directly from the vendor can. Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.\nWe seek to expound upon the research of (Guha Majumder, Dutta Gupta, and Paul 2022) to explore additional recommended research areas to improve upon and increase the general applicability of the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#our-research-plan",
    "href": "index.html#our-research-plan",
    "title": "CSCI 5302 - Final Project",
    "section": "1.3 Our Research Plan",
    "text": "1.3 Our Research Plan\nGuha Majumder, Dutta Gupta, and Paul (2022) provided the following summary model for what aspects and features they took into consideration in predicting the perceived usefulness of a customer review in Figure 1.1.\n\n\n\n\n\n\nFigure 1.1: Model Overview\n\n\n\nFurthermore, the authors provided the following areas for recommended additional research at the conclusion of their paper: \n\nExpand the number of products beyond 3 items (one search, one experience, one mixed) to better generalize the model.\nExplore customer or reviewer metadata for classifying reviewer types to enhance model performance.\n\nWe seek to examine the above two above items, and to explore the possibility of assessing a scale for products to determine the extent to which they are a search or experience-based product. We further seek to inspect additional potential modifiers to the underlying model for statistical and operational applicability; we’ve sought out work from other research teams to identify potential methods we can leverage to pursue these ends.\n\nDetermining the polarity of a customer review by employing a classifier such as Naive Bayes.\nUsing Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.\nAttempting to assess the reliability of a customer’s review based on star-rating and a ‘sentiment score’ of their textual feedback.\n\nExploring methods employed within each of combinations of these research efforts, we will pursue potential improvements on the models outlined in Guha Majumder, Dutta Gupta, and Paul (2022). We will examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon). A summary of our explorations are depicted in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: Model Modification Goals\n\n\n\nThis is not final, but what we plan to explore. If any metrics or measurements are found to not be significant in analysis and prediction of usefulness of a review, we will seek to explain the relationships (or lack thereof) and modify the final model accordingly. By incorporating these additional measures, we may be able to improve upon and generalize the original model to multiple product types across multiple e-commerce vendors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "CSCI 5302 - Final Project",
    "section": "1.4 Why It Matters",
    "text": "1.4 Why It Matters\n\nFeedback from customers can be beneficial to both vendors and consumers, but it is not always ordered by the most informative or beneficial feedback first. Certain features of products, reviews, and reviewers (such as reviewer reliability, review quality, product quality, specificity and detail of the product, amongst others) can impact the usefulness of the feedback on a customer-by-customer basis. Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers. Leveraging metrics and data associated with a product, a review, and a reviewer together may allow for online vendors to improve consumer e-commerce experience, support identificaion of issues with product quality and sales, and enable vendors to adjust practices in product marketing, inventory, and manufacture.\nExamining additional product types could support a generalization of the authors’ methodology to other products. Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals. Producing a reliable scale and methods for classifying a products’ degree of being experienced-based can inform vendors on:\n\nHow to best sort product reviews.\nExamine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment.\nAdjust the product, its marketing, or future production based upon market efficacy.\n\n\n\n\nUnderstand the emotions a customer wants to express through a review is crucial as it will affect the “recommendation score” of that particular product or a different one from a similar category. \n\nTo contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.\nTypically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. \n\nDetermine which customer is trustworthy, meaning who has actually purchased the product versus a customer who gave a false review. Based on the ‘customer reputation score’, our aim is to classify customers into groups to judge reviewer reliability. This has two main aspects:\n\nStar-rating score which is a discrete scale that tells the inclination of a customer.\nText review ‘sentiment score’ using NLP that explains customer opinions based on words.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#literature-survey",
    "href": "index.html#literature-survey",
    "title": "CSCI 5302 - Final Project",
    "section": "1.5 Literature Survey",
    "text": "1.5 Literature Survey\n\nGuha Majumder, Dutta Gupta, and Paul (2022)\n\nExamined multiple-linear regression modeling to calculate the usefulness of an online review based upon type of product (search vs. experience), review sentiment, review star rating, review length, and number of votes for the review as being “useful”. Suggested exploration using larger number of products as well as customer/reviewer metadata.\n\nHu, Gong, and Guo (2010) \n\nThe proposed system employs a two-step process for opinion mining: identifying opinion sentences using a SentiWordNet-based algorithm and extracting product features from all reviews in the database. This feature extraction function focuses on identifying commonly expressed positive or negative opinions before extracting explicit and implicit product features.\n\nRajeev and Rekha (2015) \n\nThis paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.\n\nWang et al. (2018) \n\nAuthors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, and POS tagging.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "CSCI 5302 - Final Project",
    "section": "1.6 Research Questions",
    "text": "1.6 Research Questions\n\nCan the model from Guha Majumder, Dutta Gupta, and Paul (2022) be generalized with:\n\nlarger volume of products and product types from which to mine data?\na sliding scalar multiplier representing the degree to which a product is a “search” (0) or “experience” (1) product?\nAdding modifiers to review content based upon:\n\nCustomer / Reviewer reliability and reputation?\nReview Polarity?\n\n\nCan the polarity of reviews be judged accurately by using a Naive Bayes classification model? Hu, Gong, and Guo (2010)\n\nWhat is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model? Wang et al. (2018)\n\nCan products be classified on their degree of being search or experience based by examining product variables such as:\n\nDegree of specificity in the product description? (e.g. level of detail, length, numeric values, descriptive values may suggest the product is more search than it is experience-based)\nWhether the product is offered in brand-new condition only, or offered as new, used, or refurbished? (e.g. refurbished products may be more search products than they are experience products)\nWhich of the 5 senses the product engages? (e.g. engagement of more senses, or engagement of solely specific senses like hearing and vision may suggest more experience-based than search based; examine relationship between search and experience vs. senses engaged)\nItem rarity (limited production or unique items vs. bulk-produced items)? (e.g. limited production products may be more experience-based than search-based)\n\nCan newer natrual language processing libraries provide a better fit for Review Content metrics examined by Guha Majumder, Dutta Gupta, and Paul (2022)?\nHow does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product? \nCan we categorize customer reviews based on customer experience and sentiment?\nDo specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?\nAre specific quality descriptors in text-based reviews (e.g., ‘enthusiastic’, ‘disappointed’) strongly associated with certain rating levels, and how does this association affect product reputation?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#goals-definition-of-success",
    "href": "index.html#goals-definition-of-success",
    "title": "CSCI 5302 - Final Project",
    "section": "1.7 Goals / Definition of Success",
    "text": "1.7 Goals / Definition of Success\n\n\nReplicate similar results to Guha Majumder, Dutta Gupta, and Paul (2022) with similar product types\nExpound upon Guha Majumder, Dutta Gupta, and Paul (2022) with additional products, including:\n\nOriginal products from (paper): Digital Music, Video Game, and Grocery Item\nAdditional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies\nAdditional Proucts (Amazon, Target, BestBuy): Electronics\nVerify goodness of fit of original model\n\nDeterming best metrics and/or modifiers for Review Content and Customer Reliability\nAchieving similar or better fit than original paper’s modeling; extrapolate to other product types.\n\n\n\nDetermining strength of correlation metrics (support, confidence, lift) between Naive Bayes’ classifier for review polarity Hu, Gong, and Guo (2010)\n\nIntegrate with the model and test if Naive Bayes shows strong correlation metrics.\nCompare and contrast the model with and without incorporation.\n\nSuccessful computation of reputation scores for reviewers\n\nCheck applicability across all sites used for determination of validity within the model.\nIf valid and applicable, execute model against testing data set to ensure it holds.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-schedule-timeline",
    "href": "index.html#project-schedule-timeline",
    "title": "CSCI 5302 - Final Project",
    "section": "1.8 Project Schedule / Timeline",
    "text": "1.8 Project Schedule / Timeline\nBelow in Table 1.1, we lay out the major tasks, deliverables, and their respective due dates for this effort.\n\n\n\n\nTable 1.1: Major Project Tasks\n\n\n\n\n\n\n\n\nTask\nDue Date\n\n\n\n\nMilestone 1 Submission\nFeb 26 2024\n\n\nProduct Identification and Selection\nFeb 28 2024\n\n\nVendor Identification and Selection\nFeb 28 2024\n\n\nData Collection\nMar 8 2024\n\n\nData Cleaning/Pre-Processing\nMar 17 2024\n\n\nMilestone 2 Submission\nMar 20 2024\n\n\nReview Classification (Naive Bayes, Kansei)\nMar 27 2024\n\n\nProduct Classification\nMar 27 2024\n\n\nReputation Classification\nMar 27 2024\n\n\nExploratory Data Analysis\nMar 31 2024\n\n\nMilestone 3 Submission\nUnknown\n\n\nModel Selection\nApril 7 2024\n\n\nModel Testing:\nApril 11 2024\n\n\nComplete Final Paper / Milestone 4\nApril 17 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Collection and Exploration",
    "section": "",
    "text": "2.1 Data Collection Overview\nThe original efforts by Guha Majumder, Dutta Gupta, and Paul (2022) selected three products, all listed on Amazon for sale. In our efforts, we leveraged python Selenium, urllib, and Beautiful Soup to scrape data from 20 different products across multiple websites (Amazon, BestBuy, and Target). Where possible, we sought to collect the exact same 20 products from each site and customer feedback associated with each.\nAs part of collection, to the greatest extent we were able, we cleaned information during the scraping process. Doing this enabled us to have minimal cleaning efforts after collection. Post collection, remaining items such as handling and removing special characters, unicode characters, addressing customer reviews written in foreign languages, and addressing misspellings remained necessary.\nIn terms of simplicity for scraping our data, we manually identified a list of products from each of the aforementioned sites. Our team divided responsibilities to produce scraping code customized for each of the three websites.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-collection-details",
    "href": "data.html#data-collection-details",
    "title": "2  Data Collection and Exploration",
    "section": "2.2 Data Collection Details",
    "text": "2.2 Data Collection Details\nIn collecting our data, in order to adhere to the model implemented by Guha Majumder, Dutta Gupta, and Paul (2022), we required the following data points:\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Title\nstring\n\n\nProduct Category*\nstring\n\n\nProduct Details/Specs\nstring\n\n\nProduct Cost\nfloat\n\n\n\n\n\nFor the product category variable - we may add our own manual categorization. Guha Majumder, Dutta Gupta, and Paul (2022) manually set the value for this variable. Part of the intent of our research is to seek out means and methods to replace this variable with a continuous scale (ranging from 0 for a “search” good, to a 1 for an “experience” good).\nAs an initial proxy for this variable and to operationalize it, we leverage a measure of subjectivity for the product - namely how subjective (e.g. how many adverb, adjective, and other word modifiers) are present within the details and specifications of a product. A product that more aligns to a “search” product, we hypothesize, will have fewer modifying words and be oriented toward the facts of the object.\nFor example, a desk has specific dimensions for length, width, and height, an associated weight, and material from which the desk is made, and possibly some warranty information - all of which are likely to be contained within the product description and specifications. We would characterize such a good as a “search” good (or a 0 on our scale). Leveraging existing language processing tools should allow us to calculate a value for subjectivity in the product’s description and specifications.\nInitially, we’ll explore product subjectivity in the combination of the specification and the description, though it may be necessary to explore product subjectivity solely within one of these fields or the other to pursue our modeling.\n\n\n\n\nTable 2.1: Review Data Required\n\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nVerified Purchase\nboolean\n\n\nStar Rating\nfloat\n\n\nReview Content\nstring\n\n\nUseful Votes\ninteger\n\n\n\n\n\n\n\n\nIn Table 2.1, we outline the specific datapoints we sought out for reviews across each website. Guha Majumder, Dutta Gupta, and Paul (2022) leveraged star rating, review content (specifically the review length), and the number of votes for the review being useful as key measures in their research. To further their work, we plan on exploring the impacts of verified product purchasers and the impact of verification on how useful a review may be to potential customers.\n\n\n\n\nTable 2.2: Additional Calculated Columns, Post Data Collection\n\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Subjectivity\nfloat\n\n\nReview Length (Words)\ninteger\n\n\nReview Subjectivity\nfloat\n\n\nReview Polarity\nfloat\n\n\n\n\n\n\n\n\nPost collection, we added the calculations listed in Table 2.2 to our review data and product data (less reputation score). Each of these calculations will allow us to better understand our underlying data and explore possibilities of where and how each may fit into models for review usefulness.\nWe have also established a master listing of all products for which we collected data and have associated arbitrary identifiers with the products. In instances where we’ve successfully pulled data for identical products from multiple websites, it can allow us to explore the impact on product and review metrics and investigate the listing site as a treatment variable.\nFor instance - exploring the impact of review subjectivity, polarity, length, and usefulness, based upon which site the product was listed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-exploration",
    "href": "data.html#data-exploration",
    "title": "2  Data Collection and Exploration",
    "section": "2.3 Data Exploration",
    "text": "2.3 Data Exploration\nAfter collection and cleaning, we plan to explore our data via visualization, seeking to answer key research questions.\n\nIs the price of a product higher, given it’s offered on Amazon, BestBuy, or Target? \nIs a product’s star rating affected by which e-commerce platform is selling it?\nIs there a substantial difference in number of product reviews on one e-commerce platform vs. another?\nIs one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of “this review is helpful” votes to total number of reviews)? \nWhat is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?\nDo certain product categories perform better on specific platforms? \nAre users more likely to leave reviews on one platform over another?\nDo customers show different purchasing behaviors based on promotional strategies employed by platforms?\n\nStructuring our data properly during the collection process will enable us to explore and answer these questions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#collection-procedures",
    "href": "data.html#collection-procedures",
    "title": "2  Data Collection and Exploration",
    "section": "2.4 Collection Procedures?",
    "text": "2.4 Collection Procedures?\nWe wrote code to allow us to gather information from each website. The general process for each e-commerce platform is similar. To alleviate any unnecessary burden for any of these websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought. This manual identification also allowed us to ensure, in most cases, that we were getting the exact same product during data capture. This hybrid approach enabled higher certainty in getting the same product while also accelerating collection, structuring, and cleaning of product review information.\n\nGathering from Amazon (All Products)\n\nProduct & Review data was scraped from Amazon’s website using Python and Selenium. A Selenium WebDriver was utilized to automate web browser interactions. After navigating to product categories like electronics, home appliances, furniture, books, and grocery, Selenium’s functions were employed to locate review elements. These elements were then parsed and collected, storing the data in a structured format i.e. a CSV file. Pagination handling was implemented to scrape reviews from multiple pages.\n\nGathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)\n\nJust like Target and Amazon, even BestBuy has dynamic content on its web page. We employed Python with Selenium to automate the exploration of product pages, unveiling hidden content, and harvesting essential data. Employing Selenium’s functionalities, we initiated the traversal process, enabling the program to automatically expand pertinent sections to uncover additional information. By targeting elements such as product details and reviews, we orchestrated the seamless extraction of critical fields from each product’s page. This automated approach allowed us to efficiently parse through an extensive array of reviews, ensuring a comprehensive analysis of user feedback for the products under scrutiny. We systematically stored the extracted data in our records tables for further analysis and reference.\n\nGathering from Target (All products)\n\nTarget has dynamic content on their webpages. We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data. We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-exploration-and-visualization",
    "href": "data.html#data-exploration-and-visualization",
    "title": "2  Data Collection and Exploration",
    "section": "2.5 Data Exploration and Visualization",
    "text": "2.5 Data Exploration and Visualization\nFor our data exploration, we plan to examine solely the reviews for which we have data from all of our websites. Due to the nature of the vendors, not all offer the same products online. We’ve included some unique products from each site (and may even gather more), but will exclude them from initial analysis.\nThe reason for only examining common products is to check for comparability and similarity of the products associated variables (e.g. product subjectivity, review subjectivity, review polarity, star rating, and so forth) between the websites. If they are similar or comparable, it may mean that we could use single models to make predictions on the usefulness of customer feedback. If they are substantially dissimilar, it may mean that modifiers are needed based upon the e-commerce platform in which the product is listed.\nWe’ll start by looking at distributions of some of these key variables, and check some of the common trends between them, potentially moving on to hypothesis testing of these variables to check for statistically significant differences.\n\n2.5.1 Univariate Plots and Distributions\n\n\n\n\n\n\n\n\nFigure 2.1: Histogram Plot (star-rating, by-site)\n\n\n\n\n\nExamining the histogram plots for star-rating by website, we can see that, generally, reviews tend to provide positive feedback for the selected products.\n\n\n\n\n\n\n\n\nFigure 2.2: Q-Q plots (star-rating, by-site)\n\n\n\n\n\nAcross all three websites, there appears to be consistency with adherence to, and issues with, the normal distribution for subjectivity. These charts suggest sufficient normal distribution of review subjectivity (degree of inclusion of word modifiers such as adverbs and adjectives).\nTherre seems to be slight skewness in the tails of these Q-Q distributions. Filtering off some of the outliers may grant us reasonable relevance and assurance to perform hypothesis testing and evaluation of these variables across sites (e.g. ANOVA, F-Testing, etc)\n\n\n\n\n\n\n\n\n\n\n\n(a) Q-Q plots (star-rating, by-site)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 2.3\n\n\n\n\nSimilar to review subjectivity, review polarity has good adherence to the normal distribution (particularly on the quantile interval of [-2,2]). There are similar issues in the tails of these distributions as exist for review subjectivity. As such, reduction in outliers may enable us to perform hypothesis testing during our model design and implementation.\n\n\n\n\n\n\n\n\nFigure 2.4: Distribution of Review Helpful Votes\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Random Exponential Distributions with Means by Site\n\n\n\n\n\nExamining the plots of Figure 2.4 and Figure 2.5, the distribution of helpful votes, per website, appears to be exponentially distributed, with many reviews having an expected total count of helpful votes centered fairly low. Namely, for amazon, the expected value is approximately 3.69 helpful votes, for Target 0.18 helpful votes, and BestBuy with 0.15 helpful votes.\nKnowing the distribution of these predictions will assist us in the modeling process and may require us to perform variable transformations (e.g. if we pursue a multiple linear regression model).\n\n\n\n\n\n\n\n\nFigure 2.6: Outliers for Review Star Rating\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Outliers for Review Polarity\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: Outliers for Review Subjectivity\n\n\n\n\n\n* Violin Plots\n\nBar Plots\n\n\n\n2.5.2 Bi/Multivariate Plots\n\n\n\n\n\n\n\n\nFigure 2.9: Bivariate Plot for Sensitivity and Polarity, by Site\n\n\n\n\n\n* Box Plots\n\n\n\n\n\n\n\n\nFigure 2.10: Violin Plots of Review Star-Rating vs. Subjectivity, by Site\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: Violin Plots for Star Rating vs. Polarity, by Site\n\n\n\n\n\nGenerally, in Figure 2.10 and Figure 2.11, we see a trend for the median polarity and subjectivity of each review to increase as the star rating increases. We also see that, generally, the data suggest that we have a minimum of neutral polarity that tends towards positive as star rating increases.\nSince both median subjectivity and polarity seem to increase with respect to star rating, such a correlation could be useful to us in multiple linear regression, and is generally useful to us for consideration when pursuing model development.\n\n\n2.5.3 Hypothesis Testing for Key Feature and Response Variables\nSome key features we plan to explore in our modeling include review subjectivity and review polarity. Knowing whether or not there is a significant difference for these features between the websites on which they’re hosted will inform us during model selection, design, and implementation. As such, we’ll perform ANOVA and Tukey Honest Significant Difference Tests on these variables between each site.\n\n2.5.3.1 ANOVA Testing\n\n\n2.5.3.2 Tukey Tests with E-Commerce Platform as Treatment\n\n\n\n\n\n\n\n\nFigure 2.12: Tukey Tests for Star Rating, Polarity, and Subjectivity, by-Site\n\n\n\n\n\nThe Tukey honest significance tests, depicted in Figure 2.12 suggest some interesting patterns between the three websites. Namely, target and best buy seem to have (across the board) higher star ratings, polarity, and subjectivity than the same variables for Amazon! Additionally, for each variable and each website, it seems there is no overlap in the variables at the 95% confidence level. The only exception here is for subjectivity between Target and BestBuy holding no statistically significant difference.\n\n\n\n2.5.4 Exploring Sentiments - Star Rating vs. Sentiment Variables\n\nInter-Website Comparison of Product Reviews\n\nSame Product\n\nClustering?\nDistances?\n\nAll Products\nInspect the following, visually:\n\nProduct Ratings\nCustomer Sentiments try to score before plotting & turn-in\nReview Polarity try to score and store before plotting\nNaive Bayes Classifier\nReliability estimates\nProduct description subjectivity scores try to score and store before turn-in\nAverage / Spread of number of ratings per product, try to score and store before turn-in\nAverage/Spread of Useful Votes per Product Review, try to score and store before turn-in\nInspection of Data and / or Scoring using Kansei method.\n\n\nWill need to take note on if / how these variables conform to some form of statistical distribution (uniform, normal, exponential, etc)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-before-after",
    "href": "data.html#data-before-after",
    "title": "2  Data Collection and Exploration",
    "section": "2.6 Data Before / After",
    "text": "2.6 Data Before / After\nMuch of our data cleaning occured during the collection process. Our team took specific steps to pursue cleaning during collection to simplify the process of bringing all information together:\n\nUsing regular expressions to extract key values from text blocks\nLeveraging XPATH, class names, and element IDs to identify HTML fields in which our desired data points resided\n\nPost-scraping, we had to pursue some additional cleanup\n\nRemoval of unicode characters from review content where possible through coding and scripting.\nConversion of numbers, stored as strings, to integers (i.e. star ratings, cost/dollar amounts)\nHandling of missing values (i.e. no ratings, no star ratings, no cost listed)\n\nA particular challenge we came across during the data cleaning process was the handling foreign language reviews, highly repetitive reviews, and misspelled reviews. To better support our calculated measures for subjectivity and polarity, we leveraged the langdetect library to attempt to classify the languages of each of our 45,000+ reviews collected.\n\n\n\n\nTable 2.3: Examples of reviews written in foreign languages\n\n\n\n\n\n\n\n\n\n\nsite\nreviewer_name\nreview_content\n\n\n\n\n11\nAmazon\nMoldea muy bien, me gustó mucho! Es cómodo de ...\nEn perfectas condiciones, 100% el estado de la...\n\n\n13\nAmazon\nDiego Sanchez\nTodo estuvo muy bien\n\n\n22\nAmazon\nDaniel831\nLlevo un da usndolo y aparecer funciona bien y...\n\n\n29\nAmazon\nCarlos Tocto\nExcelente producto y llego bien embalado\n\n\n46\nAmazon\nRocio castrellon\nMe encanto llego en muy buen estado\\nLa vida d...\n\n\n20063\nBestBuy\nIris\nI love Apple, amazing calidad camera and perfe...\n\n\n20087\nBestBuy\nErickL\nAmazing phone:.::::::::::::::..\n\n\n20458\nBestBuy\nsenti\ngreat............................................\n\n\n20569\nBestBuy\nJaimerecios25\nVery Good Printer yessssssssssssssssssssssssss...\n\n\n20709\nBestBuy\nSilviaC\nExcellent product. Excellent price. Will recom...\n\n\n5482\nTarget\nDo it\nGreat upgrade for my teens\n\n\n7284\nTarget\ndaisy78228\ngood for now.....................................\n\n\n7303\nTarget\nPut Jesus first\nI don't want to say anything, thank you. I don...\n\n\n9575\nTarget\nyuenkai\nuse it all the time, kind of simple to me.aaaa...\n\n\n11164\nTarget\nA\nI just got it it is awesome\n\n\n\n\n\n\n\n\n\n\nIn some cases, the language classification by langdetect was a false negative (i.e. classified as a language other than english, when it was indeed English). In our data exploration, we found that many of these false positives were outliers in other categories (whether for review length, review subjectivity, review polarity, or star rating). As such, we find it prudent to exclude these reviews from our dataset when pursuing model development.\nIn total, langdetect classified fewer than 440 reviews (accounting for less than 1% of our collected reviews) as being non-English, or being repeated words or gibberish. Excluding these reviews should have minimal impact on the pursuit of model development.\nTextBlob also offers us the ability to attempt to correct the spelling of reviews. Due to the amount of time it would take us to pursue spelling corre\nHere are some additional examples of gibberish or non-contributional text that impact calculations for review subjectivity and polarity. While some of these could potentially provide value with deeper analysis, we find that these will not contribute significantly to our research.\n\n\n\n\n\n\n\n\n\nsite\nreviewer_name\nreview_content\n\n\n\n\n962\nAmazon\nKiran Kumar\nNaN\n\n\n980\nAmazon\nErvey Gomez\n's s s s ! s s s !\n\n\n1907\nAmazon\nKathya De Alvarenga\nNaN\n\n\n2691\nAmazon\nCristopher Leyva\n10/10\n\n\n2892\nAmazon\nAmazon Customer\n10/10\n\n\n2997\nAmazon\nJoe Zuppardo\nNaN\n\n\n3567\nAmazon\nHAMZAH ALGHAMDI\nNaN\n\n\n21293\nBestBuy\nAndy\n: ) : ) : ) : ) ...\n\n\n23207\nBestBuy\nAndy\n: ) : ) : ) : ) ...\n\n\n\n\n\n\n\nWe are retaining the totality of the data we’ve collected, and will filter the data based upon our findings here so as to keep the most relevant and supportive data in building our models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#insights-from-collection-and-eda",
    "href": "data.html#insights-from-collection-and-eda",
    "title": "2  Data Collection and Exploration",
    "section": "2.7 Insights from Collection and EDA",
    "text": "2.7 Insights from Collection and EDA\n\nStatistically significant differences for review subjectivity, polarity across each of the three websites.\nExclusion of outliers for one or more categories could result in excluding lower star rating reviews, which could impact model\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "models implemented.html",
    "href": "models implemented.html",
    "title": "3  Models Implemented",
    "section": "",
    "text": "3.1 Examined Model - Multiple Linear Regression",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---multiple-linear-regression",
    "href": "models implemented.html#examined-model---multiple-linear-regression",
    "title": "3  Models Implemented",
    "section": "",
    "text": "3.1.1 Base Model\n\n\n3.1.2 Model Modifications",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---decision-tree",
    "href": "models implemented.html#examined-model---decision-tree",
    "title": "3  Models Implemented",
    "section": "3.2 Examined Model - Decision Tree",
    "text": "3.2 Examined Model - Decision Tree",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---naive-bayes",
    "href": "models implemented.html#examined-model---naive-bayes",
    "title": "3  Models Implemented",
    "section": "3.3 Examined Model - Naive Bayes",
    "text": "3.3 Examined Model - Naive Bayes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model--",
    "href": "models implemented.html#examined-model--",
    "title": "3  Models Implemented",
    "section": "3.4 Examined Model - ?",
    "text": "3.4 Examined Model - ?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#model-comparison",
    "href": "models implemented.html#model-comparison",
    "title": "3  Models Implemented",
    "section": "3.5 Model Comparison",
    "text": "3.5 Model Comparison\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "Here’s our conclusions section\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Guha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022.\n“Perceived Usefulness of Online Customer Reviews: A Review Mining\nApproach Using Machine Learning & Exploratory Data Analysis.”\nJournal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product\nFeatures from Online Reviews.” 2010 IEEE 7th International\nConference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending\nProducts to Customers Using Opinion Mining of Online Product Reviews and\nFeatures.” 2015 International Conference on Circuits, Power\nand Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018.\n“Extracting and Summarizing Affective Features and Responses from\nOnline Product Descriptions and Reviews: A Kansei Text Mining\nApproach.” Engineering Applications of Artificial\nIntelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "References"
    ]
  }
]