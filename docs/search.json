[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 5302 - Final Project",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "CSCI 5302 - Final Project",
    "section": "1.1 Abstract",
    "text": "1.1 Abstract\nWe explore the research performed in Guha Majumder, Dutta Gupta, and Paul (2022), and seek to further it via their recommendations for predicting the perceived usefulness of online customer reviews to potential customers. Our work focuses on the expansion and generalization of their multiple linear regression model. To check the model’s general applicability, we collect additional products and reviews, and do so for the same products from multiple e-commerce websites to examine whether such models are applicable to any platform, or if the models may be platform-specific. Furthermore, we explore use of additional features and coefficients, and use of other prediction and classification models to assess the degree to which a customer review is useful to future customers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#concept-and-motivation",
    "href": "index.html#concept-and-motivation",
    "title": "CSCI 5302 - Final Project",
    "section": "1.2 Concept and Motivation",
    "text": "1.2 Concept and Motivation\n\nCustomers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product. According to research by Guha Majumder, Dutta Gupta, and Paul (2022), if a customer can gather and understand product quality before the purchase, it is considered a search good, while experience goods are those which must be purchased or experienced to evaluate them. When a product is more in the directon of experience vs. search-based, other customers’ experiences can shed light on its features and return on investment than information directly from the vendor can. Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.\nWe seek to expound upon the research of (Guha Majumder, Dutta Gupta, and Paul 2022) to explore additional recommended research areas to improve upon and increase the general applicability of the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#our-research-plan",
    "href": "index.html#our-research-plan",
    "title": "CSCI 5302 - Final Project",
    "section": "1.3 Our Research Plan",
    "text": "1.3 Our Research Plan\nGuha Majumder, Dutta Gupta, and Paul (2022) provided the following summary model for what aspects and features they took into consideration in predicting the perceived usefulness of a customer review in Figure 1.1.\n\n\n\n\n\n\nFigure 1.1: Model Overview\n\n\n\nFurthermore, the authors provided the following areas for recommended additional research at the conclusion of their paper: \n\nExpand the number of products beyond 3 items (one search, one experience, one mixed) to better generalize the model.\nExplore customer or reviewer metadata for classifying reviewer types to enhance model performance.\n\nWe seek to examine the above two above items, and to explore the possibility of assessing a scale for products to determine the extent to which they are a search or experience-based product. We further seek to inspect additional potential modifiers to the underlying model for statistical and operational applicability; we’ve sought out work from other research teams to identify potential methods we can leverage to pursue these ends.\n\nDetermining the polarity of a customer review by employing a classifier such as Naive Bayes.\nUsing Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.\nAttempting to assess the reliability of a customer’s review based on star-rating and a ‘sentiment score’ of their textual feedback.\n\nExploring methods employed within each of combinations of these research efforts, we will pursue potential improvements on the models outlined in Guha Majumder, Dutta Gupta, and Paul (2022). We will examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon). A summary of our explorations are depicted in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: Model Modification Goals\n\n\n\nThis is not final, but what we plan to explore. If any metrics or measurements are found to not be significant in analysis and prediction of usefulness of a review, we will seek to explain the relationships (or lack thereof) and modify the final model accordingly. By incorporating these additional measures, we may be able to improve upon and generalize the original model to multiple product types across multiple e-commerce vendors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "CSCI 5302 - Final Project",
    "section": "1.4 Why It Matters",
    "text": "1.4 Why It Matters\n\nFeedback from customers can be beneficial to both vendors and consumers, but it is not always ordered by the most informative or beneficial feedback first. Certain features of products, reviews, and reviewers (such as reviewer reliability, review quality, product quality, specificity and detail of the product, amongst others) can impact the usefulness of the feedback on a customer-by-customer basis. Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers. Leveraging metrics and data associated with a product, a review, and a reviewer together may allow for online vendors to improve consumer e-commerce experience, support identificaion of issues with product quality and sales, and enable vendors to adjust practices in product marketing, inventory, and manufacture.\nExamining additional product types could support a generalization of the authors’ methodology to other products. Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals. Producing a reliable scale and methods for classifying a products’ degree of being experienced-based can inform vendors on:\n\nHow to best sort product reviews.\nExamine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment.\nAdjust the product, its marketing, or future production based upon market efficacy.\n\n\n\n\nUnderstand the emotions a customer wants to express through a review is crucial as it will affect the “recommendation score” of that particular product or a different one from a similar category. \n\nTo contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.\nTypically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. \n\nDetermine which customer is trustworthy, meaning who has actually purchased the product versus a customer who gave a false review. Based on the ‘customer reputation score’, our aim is to classify customers into groups to judge reviewer reliability. This has two main aspects:\n\nStar-rating score which is a discrete scale that tells the inclination of a customer.\nText review ‘sentiment score’ using NLP that explains customer opinions based on words.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#literature-survey",
    "href": "index.html#literature-survey",
    "title": "CSCI 5302 - Final Project",
    "section": "1.5 Literature Survey",
    "text": "1.5 Literature Survey\n\nGuha Majumder, Dutta Gupta, and Paul (2022)\n\nExamined multiple-linear regression modeling to calculate the usefulness of an online review based upon type of product (search vs. experience), review sentiment, review star rating, review length, and number of votes for the review as being “useful”. Suggested exploration using larger number of products as well as customer/reviewer metadata.\n\nHu, Gong, and Guo (2010) \n\nThe proposed system employs a two-step process for opinion mining: identifying opinion sentences using a SentiWordNet-based algorithm and extracting product features from all reviews in the database. This feature extraction function focuses on identifying commonly expressed positive or negative opinions before extracting explicit and implicit product features.\n\nRajeev and Rekha (2015) \n\nThis paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.\n\nWang et al. (2018) \n\nAuthors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, and POS tagging.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "CSCI 5302 - Final Project",
    "section": "1.6 Research Questions",
    "text": "1.6 Research Questions\n\nCan the model from Guha Majumder, Dutta Gupta, and Paul (2022) be generalized with:\n\nlarger volume of products and product types from which to mine data?\na sliding scalar multiplier representing the degree to which a product is a “search” (0) or “experience” (1) product?\nAdding modifiers to review content based upon:\n\nCustomer / Reviewer reliability and reputation?\nReview Polarity?\n\n\nCan the polarity of reviews be judged accurately by using a Naive Bayes classification model? Hu, Gong, and Guo (2010)\n\nWhat is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model? Wang et al. (2018)\n\nCan products be classified on their degree of being search or experience based by examining product variables such as:\n\nDegree of specificity in the product description? (e.g. level of detail, length, numeric values, descriptive values may suggest the product is more search than it is experience-based)\nWhether the product is offered in brand-new condition only, or offered as new, used, or refurbished? (e.g. refurbished products may be more search products than they are experience products)\nWhich of the 5 senses the product engages? (e.g. engagement of more senses, or engagement of solely specific senses like hearing and vision may suggest more experience-based than search based; examine relationship between search and experience vs. senses engaged)\nItem rarity (limited production or unique items vs. bulk-produced items)? (e.g. limited production products may be more experience-based than search-based)\n\nCan newer natrual language processing libraries provide a better fit for Review Content metrics examined by Guha Majumder, Dutta Gupta, and Paul (2022)?\nHow does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product? \nCan we categorize customer reviews based on customer experience and sentiment?\nDo specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?\nAre specific quality descriptors in text-based reviews (e.g., ‘enthusiastic’, ‘disappointed’) strongly associated with certain rating levels, and how does this association affect product reputation?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#goals-definition-of-success",
    "href": "index.html#goals-definition-of-success",
    "title": "CSCI 5302 - Final Project",
    "section": "1.7 Goals / Definition of Success",
    "text": "1.7 Goals / Definition of Success\n\n\nReplicate similar results to Guha Majumder, Dutta Gupta, and Paul (2022) with similar product types\nExpound upon Guha Majumder, Dutta Gupta, and Paul (2022) with additional products, including:\n\nOriginal products from (paper): Digital Music, Video Game, and Grocery Item\nAdditional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies\nAdditional Proucts (Amazon, Target, BestBuy): Electronics\nVerify goodness of fit of original model\n\nDeterming best metrics and/or modifiers for Review Content and Customer Reliability\nAchieving similar or better fit than original paper’s modeling; extrapolate to other product types.\n\n\n\nDetermining strength of correlation metrics (support, confidence, lift) between Naive Bayes’ classifier for review polarity Hu, Gong, and Guo (2010)\n\nIntegrate with the model and test if Naive Bayes shows strong correlation metrics.\nCompare and contrast the model with and without incorporation.\n\nSuccessful computation of reputation scores for reviewers\n\nCheck applicability across all sites used for determination of validity within the model.\nIf valid and applicable, execute model against testing data set to ensure it holds.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-schedule-timeline",
    "href": "index.html#project-schedule-timeline",
    "title": "CSCI 5302 - Final Project",
    "section": "1.8 Project Schedule / Timeline",
    "text": "1.8 Project Schedule / Timeline\nBelow in Table 1.1, we lay out the major tasks, deliverables, and their respective due dates for this effort.\n\n\n\n\nTable 1.1: Major Project Tasks\n\n\n\n\n\n\n\n\nTask\nDue Date\n\n\n\n\nMilestone 1 Submission\nFeb 26 2024\n\n\nProduct Identification and Selection\nFeb 28 2024\n\n\nVendor Identification and Selection\nFeb 28 2024\n\n\nData Collection\nMar 8 2024\n\n\nData Cleaning/Pre-Processing\nMar 17 2024\n\n\nMilestone 2 Submission\nMar 20 2024\n\n\nReview Classification (Naive Bayes, Kansei)\nMar 27 2024\n\n\nProduct Classification\nMar 27 2024\n\n\nReputation Classification\nMar 27 2024\n\n\nExploratory Data Analysis\nMar 31 2024\n\n\nMilestone 3 Submission\nUnknown\n\n\nModel Selection\nApril 7 2024\n\n\nModel Testing:\nApril 11 2024\n\n\nComplete Final Paper / Milestone 4\nApril 17 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Collection and Exploration",
    "section": "",
    "text": "2.1 Data Collection Overview\nThe original efforts by Guha Majumder, Dutta Gupta, and Paul (2022) selected three products, all listed on Amazon for sale. In our efforts, we leveraged python Selenium, urllib, and Beautiful Soup to scrape data from 20 different products across multiple websites (Amazon, BestBuy, and Target). Where possible, we sought to collect the exact same 20 products from each site and customer feedback associated with each.\nAs part of collection, to the greatest extent we were able, we cleaned information during the scraping process. Doing this enabled us to have minimal cleaning efforts after collection. Post collection, remaining items such as handling and removing special characters, unicode characters, addressing customer reviews written in foreign languages, and addressing misspellings remained necessary.\nIn terms of simplicity for scraping our data, we manually identified a list of products from each of the aforementioned sites. Our team divided responsibilities to produce scraping code customized for each of the three websites.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-collection-details",
    "href": "data.html#data-collection-details",
    "title": "2  Data Collection and Exploration",
    "section": "2.2 Data Collection Details",
    "text": "2.2 Data Collection Details\nIn collecting our data, in order to adhere to the model implemented by Guha Majumder, Dutta Gupta, and Paul (2022), we required the following data points:\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Title\nstring\n\n\nProduct Category*\nstring\n\n\nProduct Details/Specs\nstring\n\n\nProduct Cost\nfloat\n\n\n\n\n\nFor the product category variable - we may add our own manual categorization. Guha Majumder, Dutta Gupta, and Paul (2022) manually set the value for this variable. Part of the intent of our research is to seek out means and methods to replace this variable with a continuous scale (ranging from 0 for a “search” good, to a 1 for an “experience” good).\nAs an initial proxy for this variable and to operationalize it, we leverage a measure of subjectivity for the product - namely how subjective (e.g. how many adverb, adjective, and other word modifiers) are present within the details and specifications of a product. A product that more aligns to a “search” product, we hypothesize, will have fewer modifying words and be oriented toward the facts of the object.\nFor example, a desk has specific dimensions for length, width, and height, an associated weight, and material from which the desk is made, and possibly some warranty information - all of which are likely to be contained within the product description and specifications. We would characterize such a good as a “search” good (or a 0 on our scale). Leveraging existing language processing tools should allow us to calculate a value for subjectivity in the product’s description and specifications.\nInitially, we’ll explore product subjectivity in the combination of the specification and the description, though it may be necessary to explore product subjectivity solely within one of these fields or the other to pursue our modeling.\n\n\n\n\nTable 2.1: Review Data Required\n\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nVerified Purchase\nboolean\n\n\nStar Rating\nfloat\n\n\nReview Content\nstring\n\n\nUseful Votes\ninteger\n\n\n\n\n\n\n\n\nIn Table 2.1, we outline the specific datapoints we sought out for reviews across each website. Guha Majumder, Dutta Gupta, and Paul (2022) leveraged star rating, review content (specifically the review length), and the number of votes for the review being useful as key measures in their research. To further their work, we plan on exploring the impacts of verified product purchasers and the impact of verification on how useful a review may be to potential customers.\n\n\n\n\nTable 2.2: Additional Calculated Columns, Post Data Collection\n\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Subjectivity\nfloat\n\n\nReview Length (Words)\ninteger\n\n\nReview Subjectivity\nfloat\n\n\nReview Polarity\nfloat\n\n\n\n\n\n\n\n\nPost collection, we added the calculations listed in Table 2.2 to our review data and product data (less reputation score). Each of these calculations will allow us to better understand our underlying data and explore possibilities of where and how each may fit into models for review usefulness.\nWe have also established a master listing of all products for which we collected data and have associated arbitrary identifiers with the products. In instances where we’ve successfully pulled data for identical products from multiple websites, it can allow us to explore the impact on product and review metrics and investigate the listing site as a treatment variable.\nFor instance - exploring the impact of review subjectivity, polarity, length, and usefulness, based upon which site the product was listed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-collection-procedures",
    "href": "data.html#data-collection-procedures",
    "title": "2  Data Collection and Exploration",
    "section": "2.3 Data Collection Procedures",
    "text": "2.3 Data Collection Procedures\nWe wrote code to allow us to gather information from each website. The general process for each e-commerce platform is similar. To alleviate any unnecessary burden for any of these websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought. This manual identification also allowed us to ensure, in most cases, that we were getting the exact same product during data capture. This hybrid approach enabled higher certainty in getting the same product while also accelerating collection, structuring, and cleaning of product review information.\n\nGathering from Amazon (All Products)\n\nProduct & Review data was scraped from Amazon’s website using Python and Selenium. A Selenium WebDriver was utilized to automate web browser interactions. After navigating to product categories like electronics, home appliances, furniture, books, and grocery, Selenium’s functions were employed to locate review elements. These elements were then parsed and collected, storing the data in a structured format i.e. a CSV file. Pagination handling was implemented to scrape reviews from multiple pages.\n\nChallenge: Amazon’s product “All Reviews” webpage HTML structure had 10 reviews per page with a “Next Page” navigation button that was clickable only up-to 10 review pages. This restricted our scope of the number of reviews being scraped per product to a maximum of 100.\nSolution: Instead of scraping based on the “All Reviews” webpage, we decided to scrape reviews based on “star-rating” thereby, increasing our scope from a total of 100 reviews per product to having a maximum of 100 reviews per star rating i.e. 5*100 = 500 reviews per product.\n\n\nGathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)\n\nJust like Target and Amazon, even BestBuy has dynamic content on its web page. We employed Python with Selenium to automate the exploration of product pages, unveiling hidden content, and harvesting essential data. Employing Selenium’s functionalities, we initiated the traversal process, enabling the program to automatically expand pertinent sections to uncover additional information. By targeting elements such as product details and reviews, we orchestrated the seamless extraction of critical fields from each product’s page. This automated approach allowed us to efficiently parse through an extensive array of reviews, ensuring a comprehensive analysis of user feedback for the products under scrutiny. We systematically stored the extracted data in our records tables for further analysis and reference.\n\nGathering from Target (All products)\n\nTarget has dynamic content on their webpages. We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data. We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-exploration",
    "href": "data.html#data-exploration",
    "title": "2  Data Collection and Exploration",
    "section": "2.4 Data Exploration",
    "text": "2.4 Data Exploration\nAfter collection and cleaning, we plan to explore our data via visualization, seeking to answer key research questions.\n\nIs the price of a product higher, given it’s offered on Amazon, BestBuy, or Target? \nIs a product’s star rating affected by which e-commerce platform is selling it?\nIs there a substantial difference in number of product reviews on one e-commerce platform vs. another?\nIs one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of “this review is helpful” votes to total number of reviews)? \nWhat is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?\nDo certain product categories perform better on specific platforms? \nAre users more likely to leave reviews on one platform over another?\nDo customers show different purchasing behaviors based on promotional strategies employed by platforms?\n\nStructuring our data properly during the collection process will enable us to explore and answer these questions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-exploration-and-visualization",
    "href": "data.html#data-exploration-and-visualization",
    "title": "2  Data Collection and Exploration",
    "section": "2.5 Data Exploration and Visualization",
    "text": "2.5 Data Exploration and Visualization\nFor our data exploration, we plan to examine solely the reviews for which we have data from all of our websites. Due to the nature of the vendors, not all offer the same products online. We’ve included some unique products from each site (and may even gather more), but will exclude them from initial analysis.\nThe common items between all 3 websites include the following:\n\n\n\n\n\n\n\nproduct_title\n\n\n\n\nSamsung Galaxy S22 Ultra 5G Unlocked (128GB) Smartphone - Burgundy\n\n\nHP DeskJet 2755e Wireless All-In-One Color Printer, Scanner, Copier with Instant Ink and HP+ (26K67)\n\n\nJBL Charge 5 Portable Bluetooth Waterproof Speaker - Target Certified Refurbished\n\n\nTurboTax 2023 Deluxe Federal and State Tax Software\n\n\nHamilton Beach 4 slice Toaster 24782\n\n\nLG 65\" Class 4K UHD 2160p Smart OLED TV - OLED65C3\n\n\nGE JES1460DSBB 1.4 Cu. Ft. Black Counter Top Microwave\n\n\nDoritos Nacho Cheese Flavored Tortilla Chips - 14.5oz\n\n\nCrest Cavity & Tartar Protection Toothpaste, Baking Soda & Peroxide - 5.7oz/3pk\n\n\nOXO POP 3pc Plastic Food Storage Container Set Clear\n\n\nHogwarts Legacy - Xbox Series X\n\n\nStar Wars Jedi: Survivor - PlayStation 5\n\n\n\n\n\nThe reason for only examining common products is to check for comparability and similarity of the products associated variables (e.g. product subjectivity, review subjectivity, review polarity, star rating, and so forth) between the websites. If they are similar or comparable, it may mean that we could use single models to make predictions on the usefulness of customer feedback. If they are substantially dissimilar, it may mean that modifiers are needed based upon the e-commerce platform in which the product is listed.\nWe’ll start by looking at distributions of some of these key variables, and check some of the common trends between them, potentially moving on to hypothesis testing of these variables to check for statistically significant differences.\n\n2.5.1 Univariate Plots and Distributions\nFirst, we want to examine the review content across all websites in a single, simple visual - a Wordcloud. Seeing common words and phrases can prime us for what we might expect to see in more detailed statistical plots.\n\n\n\n\n\n\n\n\nFigure 2.1: Wordcloud of review content\n\n\n\n\n\nExamining the Wordcloud, some larger words stick out (“easy”, “good”, “love”, “need” and “great”). There don’t seem to be very many negative singular words here as it pertains to these reviews. This may suggest that the content of reviews, generally, gravitates toward positivity in reviews. We will proceed to examine this with appropriate statistical plots.\n\n\n\n\n\n\n\n\nFigure 2.2: Histogram Plot (star-rating, by-site)\n\n\n\n\n\nExamining the histogram plots for star-rating by website, we can see that, generally, reviews tend to provide more positive than negative feedback for the selected products, supporting what we see coming out of Figure 2.1\n\n\n\n\n\n\n\n\nFigure 2.3: Q-Q plots (star-rating, by-site)\n\n\n\n\n\nAcross all three websites, there appears to be consistency with adherence to, and issues with, the normal distribution for subjectivity. These charts suggest sufficient normal distribution of review subjectivity (degree of inclusion of word modifiers such as adverbs and adjectives).\nTherre seems to be slight skewness in the tails of these Q-Q distributions. Filtering off some of the outliers may grant us reasonable relevance and assurance to perform hypothesis testing and evaluation of these variables across sites (e.g. ANOVA, F-Testing, etc).\nWe’ll try plotting the same Q-Q plot with outliers removed. To remove outliers, initially, we leveraged the inter-quartile range of each variable and excluded any records for which the variable was more than \\(1.5\\cdot \\text{IQR}\\) away from the 1st and 3rd quartiles.\nBefore we proceed to re-examining the Q-Q plot with outliers removed, we’ll examine boxplots for these variables to examine the prevalence of outliers.\n\n\n\n\n\n\n\n\nFigure 2.4: Boxplots - Review Star Rating\n\n\n\n\n\nBoxplots for star ratings on both Target and Amazon are generally higher with outliers on the lower-end of the 1 to 5 scale. Amazon, however, seems to have a wider spread of information\n\n\n\n\n\n\n\n\nFigure 2.5: Boxplots - Review Polarity\n\n\n\n\n\nBoxplots for reviewe polarity suggest common threads between BestBuy and Target in terms of the number summary (min, max, quartiles, and outliers at the lower end). More notably, the polarity (or how positive or negative the content of the reviews are) generally tends toward positive. Amazon, on the otherhand, seems to show a lower center of mass and a narrower spread, with outliers to both extremes for positive and negative polarity.\nNext, we’ll examine subjectivity in the same fashion.\n\n\n\n\n\n\n\n\nFigure 2.6: Boxplots - Review Subjectivity\n\n\n\n\n\nSubjectivity, generally, seems to follow the same trends as review polarity. This suggests that these reviews could come from similar or the same population in terms of polarity and subjectivity. Further statistical analysis would be needed to make a definitive determination here.\nNow that we’ve examined the centers and spread for these variables and understand where some of their outliers may exist, we’ll examine filtering those outliers from their Q-Q plots.\nFirst - Subjectivity.\n\n\n\n\n\n\n\n\nFigure 2.7: Q-Q plots (star-rating, by-site, outliers removed)\n\n\n\n\n\nIt seems that our adjustment for outliers sufficiently made corrections for normality across the sites to better adhere to the normal distribution on the lower tail. We may need to make further adjustments on the upper tail to further refine data selection for our training dataset. Amongst the over 34K reviews in the common dataset, approximately 25.4K reviews remain after removing these outliers using this method.\nAfter identifying additional means to filter the data, these methods should suffice in support of using review subjectivity as a feature within various models.\n\n\n\n\n\n\n\n\nFigure 2.8: Q-Q plots (star-rating, by-site)\n\n\n\n\n\nSimilar to review subjectivity, review polarity has good adherence to the normal distribution (particularly on the quantile interval of [-2,2]). There are similar issues in the tails of these distributions as exist for review subjectivity.\nAs such, reduction in outliers may enable us to perform hypothesis testing during our model design and implementation. We’ll examine the same methods of outlier removal as we did for review subjectivity.\n\n\n\n\n\n\n\n\nFigure 2.9: Q-Q plots (polarity, by-site, oultiers removed)\n\n\n\n\n\nThis method of removal seems to mirror that of review subjectivity, and as such, additional fitration of the dataset will be necessary to enable this feature’s use within various models.\nAnother key distribution we must understand is that of our targeted response variable - how useful a review is, as voted by other customers. We’ll plot the response as a pure density plot to explore it’s shape.\n\n\n\n\n\n\n\n\nFigure 2.10: Distribution of Review Helpful Votes\n\n\n\n\n\nThe black lines represent random samples from the exponential distribution (with \\(E[X] = 1.9\\cdot\\bar{V}\\) with \\(\\bar{V}\\) being the mean for helpful votes within the distribution), and the green lines represent the distribution of helpful votes. It seems that, roughly, the distribution of helpful votes does follow the exponential distribution in the case of Amazon and Target.\n\nExamining the plot of Figure 2.10, the distribution of helpful votes appears to be exponentially distributed on a per-website basis, with many reviews having an expected total count of helpful votes centered fairly low.\nKnowing the distribution of our selected response variable will assist us in the modeling process. The nature of the response variable’s distribution may require us to perform transformations on features and responses (e.g. if we pursue a multiple linear regression model).\n\n\n2.5.2 Bi/Multivariate Plots\n\n\n\n\n\n\n\n\nFigure 2.11: Bivariate Plot for Sensitivity and Polarity, by Site (all purchases)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Bivariate Plot for Sensitivity and Polarity, by Site (verified purchases)\n\n\n\n\n\nIn Figure 2.11 and Figure 2.12, we observe the comparison of multiple features like review polarity, review subjectivity and verified purchases in the form of a Kernel Densoty Plot. The 2 different visuals depict the difference between the whole dataset and after the filter of verified_purchase = 1 is applied. This difference may lead to variations in the distribution and relationship between subjectivity and polarity of reviews across different sites, particularly if there are differences in the characteristics of verified and non-verified purchases.\nWe can see 2 major clusters at (0,0) which are mostly outliers where a review is very short in length. The second cluster around the area where subjectivity is about 0.5 suggests that as the review increases in subjectivity, i.e. the higher an opinionated a review is, the polarity also increases.\nThe isolated data points or “islands” outside of the main clusters suggest outliers or unique instances within the dataset. These isolated points may represent reviews that deviate significantly from the overall patterns observed in the data. They could indicate rare or extreme cases that warrant further investigation. For instance, these outliers might correspond to highly subjective or polarized reviews that are not typical of the majority of reviews.\n\n\n\n\n\n\n\n\nFigure 2.13: Violin Plots of Review Star-Rating vs. Subjectivity, by Site\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.14: Violin Plots for Star Rating vs. Polarity, by Site\n\n\n\n\n\nGenerally, in Figure 2.13 and Figure 2.14, we see a trend for the median polarity and subjectivity of each review to increase as the star rating increases. We also see that, generally, the data suggest that we have a minimum of neutral polarity that tends towards positive as star rating increases.\nSince both median subjectivity and polarity seem to increase with respect to star rating, such a correlation could be useful to us in multiple linear regression, and is generally useful to us for consideration when pursuing model development.\n\n\n2.5.3 Hypothesis Testing for Key Feature and Response Variables\nSome key features we plan to explore in our modeling include review subjectivity and review polarity. Knowing whether or not there is a significant difference for these features between the websites on which they’re hosted will inform us during model selection, design, and implementation. As such, we’ll perform ANOVA and Tukey Honest Significant Difference Tests on these variables between each site.\n\n2.5.3.1 ANOVA Testing\nTo perform our ANOVA testing, we’ll evaluate each dataset’s review polarity and subjectivity as the mean measure, and the website on which the review was posted as the treatment variable. Prior to performing our one-way ANOVA, we’ll filter the datasets down to eliminate outliers, such that the data may represent the outcomes depicted in Figure 2.7 and Figure 2.9. An assumption of ANOVA testing is that the source data (and its respective groups) adhere to the normal distribution.\nWe are leveraging Welch ANOVA and operating under an assumption that the variances between the groups are not equal, as visually evidenced in Figure 2.5 and Figure 2.6.\nHypotheses:\n\nTest 1:\n\n\\(H_0: \\mu_{\\text{Subj,Amazon}}=\\mu_{\\text{Subj,BestBuy}}=\\mu_{\\text{Subj,Target}}\\)\n\\(H_1:\\) at least one mean for review subjectivity is different.\n\nTest 2:\n\n\\(H_0: \\mu_{\\text{Polr,Amazon}}=\\mu_{\\text{Polr,BestBuy}}=\\mu_{\\text{Polr,Target}}\\)\n\\(H_1:\\) at least one mean for review polarity is different.\n\nFor both tests, \\(\\alpha=0.003\\)\n\n\n\n\n\nTable 2.3: Welch ANOVA Results (Polarity)\n\n\n\n\n\n\n\n\nSource\nddof1\nddof2\nF\np-unc\nnp2\n\n\n\n\nsite\n2\n6112.904850\n1362.569459\n0.000000\n0.106472\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Welch ANOVA Results (Subjectivity)\n\n\n\n\n\n\n\n\nSource\nddof1\nddof2\nF\np-unc\nnp2\n\n\n\n\nsite\n2\n6353.711848\n368.490784\n0.000000\n0.025819\n\n\n\n\n\n\n\n\nThe output of both Welch ANOVA tests suggests that the means for review subjectivity and review polarity, given the website on which it was posted, have a statistically significant difference. We’ll seek to visualize these differences using a plot of the Tukey Honest Significance Test.\n\n\n2.5.3.2 Tukey Tests with E-Commerce Platform as Treatment\n\n\n\n\n\n\n\n\nFigure 2.15: Tukey Tests for Star Rating, Polarity, and Subjectivity, by-Site\n\n\n\n\n\nThe Tukey honest significance tests, depicted in Figure 2.15 suggest some interesting patterns between the three websites. Namely, target and best buy seem to have higher polarity, and subjectivity than the same variables for Amazon! Additionally, for each variable and each website, it seems there is no overlap in the variables at the 99.7% confidence level.\nThese statistically significant differences between the reviews, treated by website, indicate to us that we should proceed with caution in our modeling phase. Namely, it may be necessary to include an explicit variable or feature accounting for the source website in our modeling process as a predictor for the response variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-before-after",
    "href": "data.html#data-before-after",
    "title": "2  Data Collection and Exploration",
    "section": "2.6 Data Before / After",
    "text": "2.6 Data Before / After\nMuch of our data cleaning occured during the collection process. Our team took specific steps to pursue cleaning during collection to simplify the process of bringing all information together:\n\nUsing regular expressions to extract key values from text blocks\nLeveraging XPATH, class names, and element IDs to identify HTML fields in which our desired data points resided\n\nPost-scraping, we had to pursue some additional cleanup\n\nRemoval of unicode characters from review content where possible through coding and scripting.\nConversion of numbers, stored as strings, to integers (i.e. star ratings, cost/dollar amounts)\nHandling of missing values (i.e. no ratings, no star ratings, no cost listed)\n\nA particular challenge we came across during the data cleaning process was the handling foreign language reviews, highly repetitive reviews, and misspelled reviews. To better support our calculated measures for subjectivity and polarity, we leveraged the langdetect library to attempt to classify the languages of each of our 45,000+ reviews collected.\n\n\n\n\nTable 2.5: Examples of reviews written in foreign languages\n\n\n\n\n\n\n\n\n\n\nsite\nreviewer_name\nreview_content\n\n\n\n\n11\nAmazon\nMoldea muy bien, me gustó mucho! Es cómodo de ...\nEn perfectas condiciones, 100% el estado de la...\n\n\n13\nAmazon\nDiego Sanchez\nTodo estuvo muy bien\n\n\n22\nAmazon\nDaniel831\nLlevo un da usndolo y aparecer funciona bien y...\n\n\n29\nAmazon\nCarlos Tocto\nExcelente producto y llego bien embalado\n\n\n46\nAmazon\nRocio castrellon\nMe encanto llego en muy buen estado\\nLa vida d...\n\n\n20063\nBestBuy\nIris\nI love Apple, amazing calidad camera and perfe...\n\n\n20087\nBestBuy\nErickL\nAmazing phone:.::::::::::::::..\n\n\n20458\nBestBuy\nsenti\ngreat............................................\n\n\n20569\nBestBuy\nJaimerecios25\nVery Good Printer yessssssssssssssssssssssssss...\n\n\n20709\nBestBuy\nSilviaC\nExcellent product. Excellent price. Will recom...\n\n\n5482\nTarget\nDo it\nGreat upgrade for my teens\n\n\n7284\nTarget\ndaisy78228\ngood for now.....................................\n\n\n7303\nTarget\nPut Jesus first\nI don't want to say anything, thank you. I don...\n\n\n9575\nTarget\nyuenkai\nuse it all the time, kind of simple to me.aaaa...\n\n\n11164\nTarget\nA\nI just got it it is awesome\n\n\n\n\n\n\n\n\n\n\nIn some cases, the language classification by langdetect was a false negative (i.e. classified as a language other than english, when it was indeed English). In our data exploration, we found that many of these false positives were outliers in other categories (whether for review length, review subjectivity, review polarity, or star rating). As such, we find it prudent to exclude these reviews from our dataset when pursuing model development.\nIn total, langdetect classified fewer than 440 reviews (accounting for less than 1% of our collected reviews) as being non-English, or being repeated words or gibberish. Excluding these reviews should have minimal impact on the pursuit of model development.\nTextBlob also offers us the ability to attempt to correct the spelling of reviews. Due to the amount of time it would take us to pursue spelling corre\nHere are some additional examples of gibberish or non-contributional text that impact calculations for review subjectivity and polarity. While some of these could potentially provide value with deeper analysis, we find that these will not contribute significantly to our research.\n\n\n\n\n\n\n\n\n\nsite\nreviewer_name\nreview_content\n\n\n\n\n962\nAmazon\nKiran Kumar\nNaN\n\n\n980\nAmazon\nErvey Gomez\n's s s s ! s s s !\n\n\n1907\nAmazon\nKathya De Alvarenga\nNaN\n\n\n2691\nAmazon\nCristopher Leyva\n10/10\n\n\n2892\nAmazon\nAmazon Customer\n10/10\n\n\n2997\nAmazon\nJoe Zuppardo\nNaN\n\n\n3567\nAmazon\nHAMZAH ALGHAMDI\nNaN\n\n\n21293\nBestBuy\nAndy\n: ) : ) : ) : ) ...\n\n\n23207\nBestBuy\nAndy\n: ) : ) : ) : ) ...\n\n\n\n\n\n\n\nWe are retaining the totality of the data we’ve collected, and will filter the data based upon our findings here so as to keep the most relevant and supportive data in building our models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#insights-from-collection-and-eda",
    "href": "data.html#insights-from-collection-and-eda",
    "title": "2  Data Collection and Exploration",
    "section": "2.7 Insights from Collection and EDA",
    "text": "2.7 Insights from Collection and EDA\nAs in Guha Majumder, Dutta Gupta, and Paul (2022), we find that online reviews, this time across multiple websites, tend toward positivity. The values for star rating tend toward the 3 to 5 out of 5 star range, the polarity tends toward positive as star rating increases, and subjectivity (and one might argue, expressiveness) increases with star rating as well. These correlations can prove useful to us in our research.\nWe’ve also witnessed, tested, and verified that there is a statistically significant difference for review subjectivity and polarity, given the comment was hosted on a specific website. In light of these significant test results, we believe it may be necessary to account for the specific website from whence a review originates in training, testing, and validation data. The stark differences, without adjustment, could negatively impact the performance of any models if we fail to account for these differences therein.\nThe nature of the distribution of useful votes for a review poses a potential challenge to our research. The exponential distribution of useful votes could prove difficult to predict, as more and more useful votes become exceedingly rare for a given customer comment. As such, we may have an easier time with categorizing a review as being useful or not useful, in lieu of predicting a numeric value to represent how useful a comment is (e.g. predict the number of votes in favor of a comment as being useful to other customers).\nFurthermore, exclusion of outliers could also pose challenges to our research. When excluding outliers, since the distribution of star rating tends toward the more positive reviews and results (reference Figure 2.4), we could inadvertently build models that perform the same way and are less able or unable to effectively categorize the usefulness of a lower star-rated review comment. With the nature of these outliers and the fact that having a high number of useful votes in and of itself is an outlier, we may need to examine building models upon the normalized data (e.g. the filters applied in Figure 2.7 and Figure 2.9) as well as the negation of that normalization, focusing on the outliers, so as to ensure that all the cases for our models are covered.\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "models implemented.html",
    "href": "models implemented.html",
    "title": "3  Models Implemented",
    "section": "",
    "text": "3.1 Included Variables\nTable 3.1: Selected Variables for Model Training\n\n\n\n\n\n\n\n\nVariable\nPre-Transformation Type\nPost-Transformation Type\nPurpose\nReason for Inclusion\n\n\n\n\nreview_star_rating\nint\nPCA scaled float\nfeature\nliterature survey\n\n\nreview_subjectivity\nfloat\nPCA scaled float\nfeature\nliterature survey\n\n\nreview_polarity\nfloat\nPCA scaled float\nfeature\nliterature survey\n\n\nreview_length\nint\nPCA scaled float\nfeature\nliterature survey\n\n\nprod_subjectivity\nfloat\nPCA scaled float\nfeature\nintuition\n\n\ntotal_star_rating\nfloat\nPCA scaled float\nfeature\nliterature survey\n\n\nsite\ndummy float\nPCA scaled float\nfeature\nintuition from EDA\n\n\nreview_helpful_votes\nint\nPCA scaled float\nresponse\nliterature survey",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---multiple-linear-regression",
    "href": "models implemented.html#examined-model---multiple-linear-regression",
    "title": "3  Models Implemented",
    "section": "3.5 Examined Model - Multiple Linear Regression",
    "text": "3.5 Examined Model - Multiple Linear Regression\n\n3.5.1 Base Model\n\n\n3.5.2 Model Modifications",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---decision-tree",
    "href": "models implemented.html#examined-model---decision-tree",
    "title": "3  Models Implemented",
    "section": "3.6 Examined Model - Decision Tree",
    "text": "3.6 Examined Model - Decision Tree",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model---naive-bayes",
    "href": "models implemented.html#examined-model---naive-bayes",
    "title": "3  Models Implemented",
    "section": "3.7 Examined Model - Naive Bayes",
    "text": "3.7 Examined Model - Naive Bayes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-model--",
    "href": "models implemented.html#examined-model--",
    "title": "3  Models Implemented",
    "section": "3.8 Examined Model - ?",
    "text": "3.8 Examined Model - ?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#model-comparison",
    "href": "models implemented.html#model-comparison",
    "title": "3  Models Implemented",
    "section": "3.7 Model Comparison",
    "text": "3.7 Model Comparison\nWe examine the following table to compare and contrast our implemented models on our collected data.\n\n\n\n\nTable 3.8: Summary Metrics (all evaluated models)\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\nLogistic Regression (TUNED)\n0.827358\n0.474599\n0.433375\n0.524491\n\n\nLogistic Regression (ADASYN)\n0.783666\n0.474844\n0.371489\n0.657875\n\n\nLogistic Regression (SMOTE)\n0.800246\n0.485426\n0.393358\n0.633760\n\n\nKNN (k=3)\n0.774815\n0.370695\n0.317086\n0.446119\n\n\nKNN (k=5)\n0.772238\n0.401178\n0.329304\n0.513188\n\n\nKNN (k=7)\n0.769438\n0.412000\n0.331799\n0.543331\n\n\nKNN (k=9)\n0.769662\n0.418552\n0.334993\n0.557649\n\n\nSVM-Poly\n0.827022\n0.465003\n0.430404\n0.505652\n\n\nSVM-RBF\n0.849989\n0.418079\n0.493840\n0.362472\n\n\nSVM-Sigmoid\n0.768205\n0.367085\n0.308960\n0.452148\n\n\n\n\n\n\n\n\nThe most interesting results, we find, come from the tuned Logistic Regression model and SVM-Poly models. These seem to hit a sweet spot when it comes to F1 and recall scores. Exceeding certain thresholds (at or around 0.5), seems to have too high a percentage of false positives.\n50% of our oversampled training data held votes as being useful comments, and our regular training data contained samples with approximately 7% being voted as useful.\nFor our testing data, approximately 17% of the records held votes for being useful.\nThe Tuned Logistic Regression predicted a total number of positive (true and false positives) of 1603, or 18% of the available samples. This result is very close in proximity to the actual total for true positives within the data.\nThe SVM Poly model also produced a prediction of approximately 17.7% of the testing data being useful (comparable to the actual value of 17%).\nNone of the other model formulations or permutations achieved a percentage of total positive prediction rate as in close proximity to the actual underlying data.\nThe false positives for SVM-Poly and Tuned Logistic Regression are located here:\n\nlink to Logisitic Regression file here\nlink to SVM-Poly here\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "Here’s our conclusions section\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Guha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022.\n“Perceived Usefulness of Online Customer Reviews: A Review Mining\nApproach Using Machine Learning & Exploratory Data Analysis.”\nJournal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product\nFeatures from Online Reviews.” 2010 IEEE 7th International\nConference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending\nProducts to Customers Using Opinion Mining of Online Product Reviews and\nFeatures.” 2015 International Conference on Circuits, Power\nand Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018.\n“Extracting and Summarizing Affective Features and Responses from\nOnline Product Descriptions and Reviews: A Kansei Text Mining\nApproach.” Engineering Applications of Artificial\nIntelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "models implemented.html#logistic-regression",
    "href": "models implemented.html#logistic-regression",
    "title": "3  Models Implemented",
    "section": "3.3 Logistic Regression",
    "text": "3.3 Logistic Regression\nWith the challenges of meeting and replicating the outcomes from Guha Majumder, Dutta Gupta, and Paul (2022) for a MLR model, we proceeded onward to other options. Our next choice for examination was logistic regression. The MLR called for use of only numeric or continuous variables. Logisitic regression enables us to examine the inclusion of additional categorical variables as part of the regression consideration.\n\n\n3.3.1 Model Training and Test Results\n\n\n\n\n\n\n\n\n\nModel\nUseful Level\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\n0\nLogistic Regression\nabove 0\n0.986275\n0.810916\n1.0\n0.681967",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#knn",
    "href": "models implemented.html#knn",
    "title": "3  Models Implemented",
    "section": "3.3 KNN?",
    "text": "3.3 KNN?\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\n0\nLogistic Regression\n0.814025\n0.253991\n0.153582\n0.733607\n\n\n1\nK-Nearest Neighbors\n0.968518\n0.565854\n0.698795\n0.475410",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#svm-analysis",
    "href": "models implemented.html#svm-analysis",
    "title": "3  Models Implemented",
    "section": "3.4 SVM Analysis",
    "text": "3.4 SVM Analysis\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\n0\nDecision Tree\n0.918998\n0.333618\n0.600000\n0.231043\n\n\n1\nLogistic Regression\n0.971967\n0.624852\n0.739496\n0.540984\n\n\n2\nK-Nearest Neighbors\n0.959763\n0.188948\n0.726027\n0.108607\n\n\n3\nSupport Vector Machine\n0.972940\n0.656180\n0.726368\n0.598361\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12730\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\n0\nDecision Tree\n0.918998\n0.333618\n0.600000\n0.231043\n\n\n1\nLogistic Regression\n0.971967\n0.624852\n0.739496\n0.540984\n\n\n2\nK-Nearest Neighbors\n0.959763\n0.188948\n0.726027\n0.108607\n\n\n3\nSupport Vector Machine\n0.972940\n0.656180\n0.726368\n0.598361\n\n\n4\nSupport Vector Machine2\n0.393716\n0.243482\n0.142627\n0.831325",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#svm-analysis---remove",
    "href": "models implemented.html#svm-analysis---remove",
    "title": "3  Models Implemented",
    "section": "3.4 SVM Analysis - Remove",
    "text": "3.4 SVM Analysis - Remove",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examination-of-the-original-multiple-linear-regression",
    "href": "models implemented.html#examination-of-the-original-multiple-linear-regression",
    "title": "3  Models Implemented",
    "section": "3.3 Examination of the Original Multiple Linear Regression",
    "text": "3.3 Examination of the Original Multiple Linear Regression\n\n3.3.1 Linear Model Construction\nWe leveraged a similar formulation to that which was used within the research of Guha Majumder, Dutta Gupta, and Paul (2022).\nOur version of the linear model is designed as follows: \\(\\hat{y} = \\beta_0+\\beta_1X_{rsr}+\\beta_2X_{rs}+\\beta_3X_{rp}+\\beta_4X_{rl}+\\beta_5X_{ps}\\) \\(+\\beta_6X_{tsr}+\\beta_7X_{s}\\)\nWhere each of the following variables have been standard-scaled to a range between 0 and 1 for the input data:\n\n\\(X_{rsr}\\) corresponds to the individual review’s star rating\n\\(X_{rs}\\) corresponds to the review’s subjectivity score\n\\(X_{rp}\\) corresponds to the review’s polarity score\n\\(X_{rl}\\) corresponds to the review’s length (in words)\n\\(X_{ps}\\) corresponds to the product description subjectivity score\n\\(X_{tsr}\\) is the overall star rating for the product.\n\\(X_{s}\\) is the site on which the comment was found (converted to a dummy variable for each website).\n\nWe leveraged \\(X_{ps}\\) as a proxy for the previous model’s binary attribute for whether or not a good was search-based or experienced based. Guha Majumder, Dutta Gupta, and Paul (2022) leveraged a set of binary variables to classify a good as being search, experience, or mixed products. Our intuition was that, given the subjectivity of a product’s description and/or specifications, a higher subjectivity score would correspond to an experience-based good, a lower subjectivity would correspond to a search-based good, and everything in-between would be a mixed product. This construction allows for any product to have a continuous potential range, and for most products to be mixed products (some tending more toward experience or search).\n\n\n\n3.3.2 Inspection of Linear Model Assumptions\nGenerally, our examination of the multiple linear regression performed by Guha Majumder, Dutta Gupta, and Paul (2022) failed to meet the assumptions of linear regression (normality of residuals, linear pattern in fitted vs. observed values, and constant variance of residuals).\n\n3.3.2.1 Linearity of the Model\nOur model failed to achieve any clear form of linearity between fitted and observed values.\n\n\n\n\n\n\n\n\nFigure 3.1: Linear Relationship Between Predictors and Response\n\n\n\n\n\nThe fitted vs. observed values for this plot are not indicative of a linear pattern between the feature and response variables. It provides a high mean square prediction error of 28.4055, an \\(R^2\\) of 0.0884 and adjusted \\(R^2\\) value of 0.0883. The lack of even a moderate correlation here suggests one of the following:\n\nThe wider spread of data from multiple websites and wider range of products reduced the correlation found by Guha Majumder, Dutta Gupta, and Paul (2022)\nThe linear model is not generalizable.\nThe model is no better than randomly guessing the number of votes a comment could or should have associated with it.\n\n\n\n\n3.3.2.2 Homoscedasticity on Normalized Data\nThis model has substantial challenges with heteroscedasticity. Let’s examine a plot of fitted vs. residuals in the model:\n\n\n\n\n\n\n\n\nFigure 3.2: Heteroscedasticity in Model\n\n\n\n\n\nAs expected for a model that does not have a clear linear pattern, the residuals for this linear model are heteroscedastic. We should expect to see a constant variance in a plot of predicted vs. residual values, with no correlation between the errors and the predictions. Here, we witness this issue directly, lending to the idea that the model is a poor fit for the data, and the nature of the model (e.g. additional data, additional features, or other feature/response transformations) would need to change substantially to produce effective predictive results.\n\n\n3.3.2.3 Normality in Residuals\n\n\n\n\n\n\n\n\nFigure 3.3: Model Residuals are Abnormal\n\n\n\n\n\nFrom the above plot, it is clear that this model fails to adhere to the multiple linear regression requirement for residual normality.\n\n\n3.3.2.4 Conclusion on MLR model\nExpanding the Multiple Linear Regression beyond the scope of the study performed by Guha Majumder, Dutta Gupta, and Paul (2022) seems to fail all of the assumptions of a linear model, and does not appear to produce a tracable predictive pattern that may support classification of a review comment as being useful or not to prospective customers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#k-nearest-neighbors-classification",
    "href": "models implemented.html#k-nearest-neighbors-classification",
    "title": "3  Models Implemented",
    "section": "3.5 K-Nearest Neighbors Classification",
    "text": "3.5 K-Nearest Neighbors Classification\nK-Nearest Neighbors is used to learn and identify the target class instances. It makes predictions by calculating distance (usually, Euclidean distance) between a given instance and all other instances in the dataset in feature space.\n\n3.5.1 Hyperparameter Tuning\nThe value of k is the most critial hyperparameter in the KNN Calssification algorithm. It determines the performance of the model. Usually a small k value leads to overly complex understanding of the data that might result into overfitting, however, a higher k values can lead to underfitting.\nWe have looked at multiple values of k for our given data and compared a set of model metrics - accuracy, F1 score, precision, and recall to find the most optimal model to be the model with k = 3.\nLooking the results shown below we can say that:\n\nKNN at each neighbor level performs poorer in comparison to logistic regression. The accuracy and precision do not reach sufficient levels. Furthermore, the total percent prediction of positive cases (ranging from 21.5-25.6%) far exceed the percent of true positive cases in the testing dataset (approximately 17%). These over-optimistic prediction levels (in combination with a high number of false negatives) suggest we may not be meeting the mark with this model, as once again, votes for a comment as being useful is relatively rare in this dataset.\nFor KNN with n=3 neighbors, we are the closest to the actual percent of positives in the source dataset. This model, however, has poor performance for precision, recall, and F1.\nFor KNN with n=9 neighbors, we improve the F1 score, but the precision and recall remain substantially low.\nKNN, for any number of neighbors and given our selected features, may be an insufficient model for our use case.\n\n\n\n\n3.5.2 KNN Test Results\n\n\n\n\n\nTable 3.6: Test Scores for K-Nearest Neighbors\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\nKNN (k=3)\n0.774815\n0.370695\n0.317086\n0.446119\n\n\nKNN (k=5)\n0.772238\n0.401178\n0.329304\n0.513188\n\n\nKNN (k=7)\n0.769438\n0.412000\n0.331799\n0.543331\n\n\nKNN (k=9)\n0.769662\n0.418552\n0.334993\n0.557649\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: Confusion Matrices for KNN Models\n\n\n\n\n\nSimply examining the above confusion matrices, especially the upper-right hand corner false posistives, we see that KNN is likely over-optimistic about the usefulness of customer feedback.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#support-vector-machine",
    "href": "models implemented.html#support-vector-machine",
    "title": "3  Models Implemented",
    "section": "3.5 Support Vector Machine",
    "text": "3.5 Support Vector Machine\n\n\n\n\n\n\n\n\n\nModel\nUseful Level\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\n0\nLogistic Regression\nabove 0\n0.986275\n0.810916\n1.000000\n0.681967\n\n\n1\nKNN\nabove 0\n0.983516\n0.764884\n0.994751\n0.621311\n\n\n2\nSVM-PCA\nabove 0\n0.986275\n0.810916\n1.000000\n0.681967",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#data-adjustments",
    "href": "models implemented.html#data-adjustments",
    "title": "3  Models Implemented",
    "section": "3.2 Data Adjustments",
    "text": "3.2 Data Adjustments\nAs noted in our exploratory data analysis, each individual site has statistically significant differences in key variables we’re considering in our modeling. To mitigate the potential for under or overfitting, and misrepresentation due to variable scale we perform the following transformations to our data:\n\n\nVariable outlier adjustment. We noted in our EDA that each of the e-commerce platforms had high volumes of outliers with respect to the inter-quartile range. We applied a transformation to our data to map any outlier variable value on a per-website basis from its value to \\(\\mu+3\\cdot sd(\\text{variable})\\) for high-end outliers, and \\(\\mu-3\\cdot sd(\\text{variable})\\) for low-end outliers. In the event that either of these values exceeded the minimum or maximum value of the dataset, we mapped the value to the minimum or maximum accordingly.\nStandard scaling of variables. After adjusting outliers, we re-mapped all of our feature variables to be on the scale of the standard normal distribution \\(N\\sim(0,1)\\)\nResponse variable transformation to binary value. We denoted a single useful vote as meaning that the review was useful to customers, and mapped the value to True/1, and False/0 otherwise.\n\nHere is a sample (first 10 observations) of our data prior to the transformation:\n\n\n\n\nTable 3.2: Data (pre-transformation)\n\n\n\n\n\n\n\n\nreview_star_rating\nreview_subjectivity\nreview_polarity\nreview_length\nprod_subjectivity\ntotal_star_rating\nsite\n\n\n\n\n5\n0.588232\n0.258266\n49.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.430909\n0.447273\n24.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.554959\n0.394215\n66.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n1.000000\n1.000000\n14.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.636364\n0.424242\n31.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.613068\n0.402557\n54.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.601705\n0.407582\n42.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.458333\n0.591667\n38.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.801136\n0.734091\n24.000000\n0.497845\n4.200000\n0.333333\n\n\n5\n0.801136\n0.734091\n24.000000\n0.497845\n4.200000\n0.333333\n\n\n\n\n\n\n\n\nAnd here is a sample of our data after the applied transformations:\n\nSnippet of first 10 observations in our training dataset, post-transformation:\n\n\n\n\n\nTable 3.3: Data (post-transformation)\n\n\n\n\n\n\n\n\nreview_star_rating\nreview_subjectivity\nreview_polarity\nreview_length\nprod_subjectivity\ntotal_star_rating\nsite\n\n\n\n\n0.527771\n0.264387\n0.336418\n-0.347482\n-1.353087\n1.088999\n0.722595\n\n\n0.527771\n0.240213\n-0.563168\n0.998209\n1.428196\n0.716223\n-0.867221\n\n\n0.527771\n1.707921\n1.512800\n-0.544412\n-0.002591\n-1.520431\n0.722595\n\n\n0.527771\n0.110709\n-0.363171\n-0.478769\n1.857024\n-0.774879\n-0.867221\n\n\n0.527771\n-0.968488\n-0.675166\n-0.052086\n1.857024\n-0.774879\n-0.867221\n\n\n-0.481349\n0.326548\n0.236820\n-0.281839\n1.428196\n0.716223\n-0.867221\n\n\n0.527771\n0.499220\n-0.243173\n-0.019265\n-1.353087\n1.088999\n0.722595\n\n\n0.527771\n-1.486502\n1.376802\n-0.544412\n-0.211017\n-0.029328\n0.722595\n\n\n0.527771\n0.251005\n0.535315\n-0.314660\n-1.353087\n1.088999\n0.722595\n\n\n0.527771\n0.823259\n0.370697\n-0.052086\n-0.469912\n0.716223\n-0.867221\n\n\n\n\n\n\n\n\nClasses in the response variable (the number of helpful votes a review received) set were mapped as follows for all non-linear models:\n\n0: if the review had no helful votes\n1: if the review had one or more than 1 helpful votes\n\nOur reasoning for this transformation is that, across the totality of our data, a comment receiving a vote as being useful is exceedingly rare, as uncovered during our exploratory data analysis. As such, even a single vote for being useful should put the comment in the running for being considered useful.\n\n3.2.1 Dimensionality Reduction\nFor all models outside of the multiple linear regression, we performed a principal component analysis on the scaled data.\n\n\n\n\nTable 3.4: Cumulative Variance of Principal Components (selected features)\n\n\n\n\n\n\n\n\nPrincipal Component\nCumulative Variance\nExplained Variance\n\n\n\n\nPC1\n0.350576\n0.350576\n\n\nPC2\n0.548099\n0.197523\n\n\nPC3\n0.673668\n0.125569\n\n\nPC4\n0.788246\n0.114578\n\n\nPC5\n0.878823\n0.090576\n\n\nPC6\n0.945954\n0.067131\n\n\nPC7\n1.000000\n0.054046\n\n\n\n\n\n\n\n\nTo reduce dimensionality for Logistic Regression, Support Vector Machine, and K-Nearest Neighbor models, we elected to reduce from 7 principal commponents to 6. From the above table, we see that these 6 components explain approximately 95% of the variation within the training data. We projected our training and testing data from their 7-dimensional feature space to a reduced 6-dimensional principal component vector space.\n\n\n3.2.2 Training Data\nTo train our dataset, we leveraged the data post-transformation to train each of our models, including the adjustments of outlier datapoints to being within 3 standard deviations of the mean of each variable. We selected an 80% sample of this data and leveraged the same dataset to train each model.\nIn the interest of generalization, we sought to take our training dataset solely from products that were common across all three e-commerce platforms. By working with common data from each site, the data and the models may be able to formulate a more generalized construct of how certain variables behave within the different contexts of each platforms, and assist in better prediction and classification of comments as being useful or not.\n\n\n3.2.3 Testing Data\nFor testing, we evaluated each model against transformed data, omitting the transformation of outliers to being within 3 standard deviations of the mean. We performed this action to enable a fair comparison of each model against one another when working with real-world data.\n\n\n3.2.4 Desired Outcomes and Objectives\nPredictive modeling for the usefulness of a user comment on a product, in and of itself, cannot be conducted 100% objectively. We are interested in the exploration of misclassifications - particularly of false positives.\nOur data exploration revealed that having even a single vote for a comment as being useful was exceedingly rare, with a median and mean number of votes hovering at or about 0 regardless of the website on which the comment was posted.\nAdditionally, from a technological perspective, the ability of a web user interface programmer or designer to simply filter or arrange comments by the number of votes they received is trivial.\nWith the above considerations in mind, we are interested in a model that provides reasonable accuracy while also having an apprpriate amount of recall and a reasonable F1 score. Having a model that perfectly maps predictions to actual outcomes is not useful to this research. Having a number of classified false positives for exploration and subjective evaluation is what interests us.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#logistic-regression-classification",
    "href": "models implemented.html#logistic-regression-classification",
    "title": "3  Models Implemented",
    "section": "3.4 Logistic Regression Classification",
    "text": "3.4 Logistic Regression Classification\nGiven the aforementioned challenges with the linear model, our next choice for examination was logistic regression. The MLR called for use of only numeric or continuous variables. Logisitic regression enables us to examine the inclusion of additional categorical variables as part of the regression consideration.\n\n3.4.1 Hyperparameter Tuning\nLogistic regression is one of the best performing models in the project after SVM . We inspected 3 logistic regression models - one was tuned using the class weight hyperparameter to address the class imbalance present in the dataset. By assigning a higher weight to the minority class (useful level 1) and a lower weight to the majority class (useful level 0), the model was able to better capture the patterns associated with the minority class, leading to improved performance metrics.\n\n\n3.4.2 Oversampling techniques\nAdditionally, 2 more logistic regression models are trained using two oversampling techniques, namely ADASYN and SMOTE. ADASYN, which generates synthetic samples for the minority class based on their difficulty in learning regions, and SMOTE, which creates synthetic samples by interpolating between existing minority class samples, were used to address the class imbalance problem. These techniques help to provide the model with more balanced training data, allowing it to learn the characteristics of both classes more effectively.\nIt is a well know fact that oversampling techniques are employed if there a severe class imbalance if hyperparameter tuning does not improve the model performance. However, despite the heavy class imbalance, the tuned model and the SMOTE model achieve great result with 98% accuracy indicating that it is proficient at making correct predictions. 68% recall is decent, but indicates that the model maybe classifying the positive instances from minority class incorrectly.\n\n\n3.4.3 Logistic Regression Test Results\n\n\n\n\nTable 3.5: Test Scores for Logistic Regression\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\nLogistic Regression (TUNED)\n0.827358\n0.474599\n0.433375\n0.524491\n\n\nLogistic Regression (ADASYN)\n0.783666\n0.474844\n0.371489\n0.657875\n\n\nLogistic Regression (SMOTE)\n0.800246\n0.485426\n0.393358\n0.633760\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: Logistic Regression Confusion Matrices\n\n\n\n\n\nAmongst these models, it appears that the tuned model provides the highest F1 score. The recall and precision are the lowest…meaning…\n\n\nComments, sorted by probabability of being useful in descending order, are located here (link).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#support-vector-machine-classification",
    "href": "models implemented.html#support-vector-machine-classification",
    "title": "3  Models Implemented",
    "section": "3.6 Support Vector Machine Classification",
    "text": "3.6 Support Vector Machine Classification\nSupport Vector Machines work on classification problems by finding an optimal hyperplane that best classifies the target classes in the given feature space. Because of its flexibility of moving the hyperplane and adapting to the intricacies of the data, SVM could be a useful and powerful algorithm for our use case.\n\n3.6.1 Hyperparameter Tuning\nThe complexity of a SVM model is determined by the choice of kernel function that supports the capturing of nuances within the data. Below are our implmentation results of the performance metrics of Support Vector Machine (SVM) models trained with different kernel functions: polynomial (SVM-Poly), radial basis function (SVM-RBF), and sigmoid (SVM-Sigmoid).\nTo tune each version of this model, we leveraged a reduced set of principal components, and adjusted class weighting, as having useful votes for a product is a relative rarity across each of the e-commerce platforms. To boost our recall, we elected to assign weights of 0.2 to class 0 (not useful) and 0.8 to class 1 (useful).\n\n\n3.6.2 SVM Test Results\n\n\n\n\nTable 3.7: Test Scores for Support Vector Machine\n\n\n\n\n\n\n\n\nModel\nAccuracy\nF1\nPrecision\nRecall\n\n\n\n\nSVM-Poly\n0.827022\n0.465003\n0.430404\n0.505652\n\n\nSVM-RBF\n0.849989\n0.418079\n0.493840\n0.362472\n\n\nSVM-Sigmoid\n0.768205\n0.367085\n0.308960\n0.452148\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Confusion Matrices for Support Vector Machines\n\n\n\n\n\nLooking the results shown below we can say that:\n\nSVM with a polynomial kernel (set to degree 1 or linear) predicts a total percent of true positives close to the underlying source data (approximately 17%).\nSVM with radial basis function kernel appears to under-predict positive cases.\nSVM with sigmoid kernel strongly over-predicts false positives\n\nSVM Poly appears to achieve reasonable accuracy while hitting an appropriate level for F1 and Recall for our use case. The presence of false positives gives us something to subjectively examine for its usefulness as a potential customer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  }
]