[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 5302 - Final Project",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#concept-and-motivation",
    "href": "index.html#concept-and-motivation",
    "title": "CSCI 5302 - Final Project",
    "section": "1.1 Concept and Motivation",
    "text": "1.1 Concept and Motivation\n\nCustomers, when searching for products with specific features and aspects, need sufficient information to make a decision as to whether to procure a specific product. According to research by Guha Majumder, Dutta Gupta, and Paul (2022), if a customer can gather and understand product quality before the purchase, it is considered a search good, while experience goods are those which must be purchased or experienced to evaluate them. When a product is more in the directon of experience vs. search-based, other customers’ experiences can shed light on its features and return on investment than information directly from the vendor can. Having reviews from reliable sources with sufficiently detailed information can enable greater confidence in a purchase, improved customer satisfaction, and smooth the process of ecommerce for customers.\nWe seek to expound upon the research of (Guha Majumder, Dutta Gupta, and Paul 2022) to explore additional recommended research areas to improve upon and increase the general applicability of the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-where",
    "href": "index.html#what-where",
    "title": "CSCI 5302 - Final Project",
    "section": "1.2 What / Where",
    "text": "1.2 What / Where\nGuha Majumder, Dutta Gupta, and Paul (2022) provided the following summary model for what aspects and features they took into consideration in predicting the perceived usefulness of a customer review in Figure 1.1.\n\n\n\n\n\n\nFigure 1.1: Model Overview\n\n\n\nFurthermore, the authors provided the following areas for recommended additional research at the conclusion of their paper: \n\nExpand the number of products beyond 3 items (one search, one experience, one mixed) to better generalize the model.\nExplore customer or reviewer metadata for classifying reviewer types to enhance model performance.\n\nWe seek to examine the above two above items, and to explore the possibility of assessing a scale for products to determine the extent to which they are a search or experience-based product. We further seek to inspect additional potential modifiers to the underlying model for statistical and operational applicability; we’ve sought out work from other research teams to identify potential methods we can leverage to pursue these ends.\n\nDetermining the polarity of a customer review by employing a classifier such as Naive Bayes.\nUsing Kansei engineering approaches to convert unstructured product-related texts into feature–affective opinions.\nAttempting to assess the reliability of a customer’s review based on star-rating and a ‘sentiment score’ of their textual feedback.\n\nExploring methods employed within each of combinations of these research efforts, we will pursue potential improvements on the models outlined in Guha Majumder, Dutta Gupta, and Paul (2022). We will examine additional products and product types between multiple e-commerce websites (BestBuy, Target, Amazon). A summary of our explorations are depicted in Figure 1.2.\n\n\n\n\n\n\nFigure 1.2: Model Modification Goals\n\n\n\nThis is not final, but what we plan to explore. If any metrics or measurements are found to not be significant in analysis and prediction of usefulness of a review, we will seek to explain the relationships (or lack thereof) and modify the final model accordingly. By incorporating these additional measures, we may be able to improve upon and generalize the original model to multiple product types across multiple e-commerce vendors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#why-it-matters",
    "href": "index.html#why-it-matters",
    "title": "CSCI 5302 - Final Project",
    "section": "1.3 Why It Matters",
    "text": "1.3 Why It Matters\n\nFeedback from customers can be beneficial to both vendors and consumers, but it is not always ordered by the most informative or beneficial feedback first. Certain features of products, reviews, and reviewers (such as reviewer reliability, review quality, product quality, specificity and detail of the product, amongst others) can impact the usefulness of the feedback on a customer-by-customer basis. Level of detail, star-rating, and number of votes that support the review as being useful to a customer can all help determine its usefulness to other customers. Leveraging metrics and data associated with a product, a review, and a reviewer together may allow for online vendors to improve consumer e-commerce experience, support identificaion of issues with product quality and sales, and enable vendors to adjust practices in product marketing, inventory, and manufacture.\nExamining additional product types could support a generalization of the authors’ methodology to other products. Furthermore, the exploration of a sliding scale for search vs. experience-based products can further support generalization and business goals. Producing a reliable scale and methods for classifying a products’ degree of being experienced-based can inform vendors on:\n\nHow to best sort product reviews\nExamine what are the most helpful reviews to know the performance of the product alongside customer experience and sentiment\nAdjust the product, its marketing, or future production based upon market efficacy\n\n\n\n\nUnderstand the emotions a customer wants to express through a review is crucial as it will affect the “recommendation score” of that particular product or a different one from a similar category. \n\nTo contribute in determining this recommendation score, we can use a probabilistic machine learning algorithm like Naive Bayes to determine the polarity (positive, negative, or neutral) of customer reviews.\nTypically used for amending product design, Kansei Engineering can be used to incorporate human emotional responses into evaluation of a customer review. \n\nDetermine which customer is trustworthy, meaning who has actually purchased the product versus a customer who gave a false review. Based on the ‘customer reputation score’, our aim is to classify customers into groups to judge reviewer reliability. This has two main aspects:\n\nStar-rating score which is a discrete scale that tells the inclination of a customer.\nText review ‘sentiment score’ using NLP that explains customer opinions based on words.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#literature-survey",
    "href": "index.html#literature-survey",
    "title": "CSCI 5302 - Final Project",
    "section": "1.4 Literature Survey",
    "text": "1.4 Literature Survey\n\n\nGuha Majumder, Dutta Gupta, and Paul (2022)\n\nExamined multiple-linear regression modeling to calculate the usefulness of an online review based upon type of product (search vs. experience), review sentiment, review star rating, review length, and number of votes for the review as being “useful”. Suggested exploration using larger number of products as well as customer/reviewer metadata.\n\nHu, Gong, and Guo (2010) \n\nThe proposed system employs a two-step process for opinion mining: identifying opinion sentences using a SentiWordNet-based algorithm and extracting product features from all reviews in the database. This feature extraction function focuses on identifying commonly expressed positive or negative opinions before extracting explicit and implicit product features.\n\nRajeev and Rekha (2015) \n\nThis paper presents techniques like Opinion mining, feature extraction and Naives Bayes classification for review polarity determination. The authors suggest performing both Objective and Subjective analysis of features by considering qualitative and quantitative features of the data respectively.\n\nWang et al. (2018) \n\nAuthors have proposed a solution by implementing Kansei engineering and text mining simultaneously which will help customers in decision making process. It helps to categorize reviews into multiple sections and perform text mining by NLP techniques like Sentence segmentation, Tokenization, and POS tagging.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "CSCI 5302 - Final Project",
    "section": "1.5 Research Questions",
    "text": "1.5 Research Questions\n\nCan the model from Guha Majumder, Dutta Gupta, and Paul (2022) be generalized with:\n\nlarger volume of products and product types from which to mine data?\na sliding scalar multiplier representing the degree to which a product is a “search” (0) or “experience” (1) product?\nAdding modifiers to review content based upon:\n\nCustomer / Reviewer reliability and reputation?\nReview Polarity?\n\n\nCan the polarity of reviews be judged accurately by using a Naive Bayes classification model? Hu, Gong, and Guo (2010)\n\nWhat is the impact of different feature extraction methods (e.g., bag-of-words, TF-IDF) on the performance of Naive Bayes classification model? Wang et al. (2018)\n\nCan products be classified on their degree of being search or experience based by examining product variables such as:\n\nDegree of specificity in the product description? (e.g. level of detail, length, numeric values, descriptive values may suggest the product is more search than it is experience-based)\nWhether the product is offered in brand-new condition only, or offered as new, used, or refurbished? (e.g. refurbished products may be more search products than they are experience products)\nWhich of the 5 senses the product engages? (e.g. engagement of more senses, or engagement of solely specific senses like hearing and vision may suggest more experience-based than search based; examine relationship between search and experience vs. senses engaged)\nItem rarity (limited production or unique items vs. bulk-produced items)? (e.g. limited production products may be more experience-based than search-based)\n\nCan newer natrual language processing libraries provide a better fit for Review Content metrics examined by Guha Majumder, Dutta Gupta, and Paul (2022)?\nHow does sentiment in customer reviews correlate with customer satisfaction metrics or sales figures for a particular product? \nCan we categorize customer reviews based on customer experience and sentiment?\nDo specific product star ratings tend to incite more reviews, and if so, how does this impact the overall reputation measurement?\nAre specific quality descriptors in text-based reviews (e.g., ‘enthusiastic’, ‘disappointed’) strongly associated with certain rating levels, and how does this association affect product reputation?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#goals-definition-of-success",
    "href": "index.html#goals-definition-of-success",
    "title": "CSCI 5302 - Final Project",
    "section": "1.6 Goals / Definition of Success",
    "text": "1.6 Goals / Definition of Success\n\n\nReplicate similar results to Guha Majumder, Dutta Gupta, and Paul (2022) with similar product types\nExpound upon Guha Majumder, Dutta Gupta, and Paul (2022) with additional products, including:\n\nOriginal products from (paper): Digital Music, Video Game, and Grocery Item\nAdditional products (Amazon and Target): Furniture Items, Clothing Items, Home Appliances, Books, Cosmetics, Cleaning supplies\nAdditional Proucts (Amazon, Target, BestBuy): Electronics\nVerify goodness of fit of original model\n\nDeterming best metrics and/or modifiers for Review Content and Customer Reliability\nAchieving similar or better fit than original paper’s modeling; extrapolate to other product types.\n\n\n\nDetermining strength of correlation metrics (support, confidence, lift) between Naive Bayes’ classifier for review polarity Hu, Gong, and Guo (2010)\n\nIntegrate with the model and test if Naive Bayes shows strong correlation metrics.\nCompare and contrast the model with and without incorporation.\n\nSuccessful computation of reputation scores for reviewers\n\nCheck applicability across all sites used for determination of validity within the model.\nIf valid and applicable, execute model against testing data set to ensure it holds.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#project-schedule-timeline",
    "href": "index.html#project-schedule-timeline",
    "title": "CSCI 5302 - Final Project",
    "section": "1.7 Project Schedule / Timeline",
    "text": "1.7 Project Schedule / Timeline\nBelow in Table 1.1, we lay out the major tasks, deliverables, and their respective due dates for this effort.\n\n\n\n\nTable 1.1: Major Project Tasks\n\n\n\n\n\n\n\n\nTask\nDue Date\n\n\n\n\nMilestone 1 Submission\nFeb 26 2024\n\n\nProduct Identification and Selection\nFeb 28 2024\n\n\nVendor Identification and Selection\nFeb 28 2024\n\n\nData Collection\nMar 8 2024\n\n\nData Cleaning/Pre-Processing\nMar 17 2024\n\n\nMilestone 2 Submission\nMar 20 2024\n\n\nReview Classification (Naive Bayes, Kansei)\nMar 27 2024\n\n\nProduct Classification\nMar 27 2024\n\n\nReputation Classification\nMar 27 2024\n\n\nExploratory Data Analysis\nMar 31 2024\n\n\nMilestone 3 Submission\nUnknown\n\n\nModel Selection\nApril 7 2024\n\n\nModel Testing:\nApril 11 2024\n\n\nComplete Final Paper / Milestone 4\nApril 17 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Collection and Exploration",
    "section": "",
    "text": "2.1 Data Collection Overview\nLeverage python Selenium, urllib, and Beautiful Soup to scrape data from X products.\nAs part of collection, to the greatest extent we are able, we cleaned information during the scraping process. We leveraged tools such as the python regular expression library and (anything else?) to pull the exact information we sought while scraping. The only possible additional cleaning that may be required after scraping is the handling of unicode characters within product names, product reviews, and so forth.\nIn terms of simplicity for scraping our data, we will manually identify a list of products. Guha Majumder, Dutta Gupta, and Paul (2022) leveraged solely 3 products from Amazon. Our team seeks to scrape between 20 and 30 products and all their reviews from Target, Amazon, and Best Buy. By doing so, we are greatly increasing the sample size of data compared to the original work performed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-collection-overview",
    "href": "data.html#data-collection-overview",
    "title": "2  Data Collection and Exploration",
    "section": "",
    "text": "Sought to collect some data from 3 websites - target, amazon, and best buy.\nWhere possible, collect same entity from multiple sites",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-collection-details",
    "href": "data.html#data-collection-details",
    "title": "2  Data Collection and Exploration",
    "section": "2.2 Data Collection Details",
    "text": "2.2 Data Collection Details\nIn collecting our data, in order to adhere to the model implemented by Guha Majumder, Dutta Gupta, and Paul (2022), we require the following data points, at a minimum:\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Title\nstring\n\n\nProduct Category*\nstring\n\n\nProduct Details/Specs\nstring\n\n\nProduct Cost\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nVerified Purchase\nboolean\n\n\nStar Rating\nfloat\n\n\nReview Content\nstring\n\n\nUseful Votes\ninteger\n\n\n\n\n\nFrom our collected data, here are some of the calculations we’ll need to run for building our models. If time is available, we will run these calculations using Python’s NLTK before the completion of this milestone.\n\n\n\n\n\n\n\nVariable\nData Type\n\n\n\n\nProduct Subjectivity\nfloat\n\n\nReview Length\ninteger\n\n\nSentiment Score\nfloat\n\n\nReputation Score\nfloat\n\n\nProduct Type Score\nfloat\n\n\nPolarity Score\nfloat\n\n\n\n\n\nIn terms of structuring our stored data, we will have a central table and child tables. Since we will seek, in some cases, to gather the same product from multiple websites, we must have a structure that identifies:\n\nThe full listing of products, assigned an arbitrary ID\nA listing of specific products we intend to scrape from each of the websites we’ve identified. Adding an additional ID of [company_name]-[arbitrary_product_id]\nSite-specific product information\nSite-specific review content and metadata\n\nTo wrangle the potential amount of data we may collect, we will partition review files into their own files under the following convention: [company_name]-[arbitrary_product_id]-review_content.csv. Using this method will allow us to capture thousands of reviews for multiple products without overrunning GitHub filesize limitations. Following this convention will also allow us to easily script out the integration of all these files for analysis when we begin building and applying our models.\nAdditionally, the use of assigning products an arbitrary ID that is common between different vendors will allow us to directly compare review and product information across multiple vendors simultaneously to gather additional insights. Effectively, if we group products by this identifier, we can see how the product performs overall in multiple e-commerce platforms. Similarly, we can use this information to evaluate and run comparisons and tests on individual products, given the treatment of their offering on different platforms. This can help generate even more insight before model application.\nFor instance, we may be able to explore questions like:\n\nIs the price of a product higher, given it’s offered on Amazon, BestBuy, or Target?\nIs a product’s star rating affected by which e-commerce platform is selling it?\nIs there a substantial difference in number of product reviews on one e-commerce platform vs. another?\nIs one e-commerce platform more likely to have input and feedback on reviews (i.e. higher proportion of “this review is helpful” votes to total number of reviews)?\nWhat is the difference in the level of detail provided in product descriptions (e.g. for the same product) across each e-commerce platform?\n\nStructuring our data properly during the collection process will enable us to explore and answer these questions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#collection-procdures",
    "href": "data.html#collection-procdures",
    "title": "2  Data Collection and Exploration",
    "section": "2.3 Collection Procdures?",
    "text": "2.3 Collection Procdures?\nWe wrote code to allow us to (mostly) template out our gathering of information from each website. The general process for each page is similar for data gathering. To alleviate any unnecessary burden on the target websites, we manually identified URLs to the specific products we sought out to gather, and wrote our code to iterate through those URLs and pull the necessary data and features we sought. This hybrid approach saved us time and effort.\n\nGathering from Target (All products)\n\nTarget has dynamic content on their webpages. We used Python Selenium to navigate to product pages and automate the selection of items needed to expand sections to reveal additional data. We also automated the process of expanding out all reviews so as to iterate through and parse the content of every review for each product in question. We extracted the fields listed above (reference here) to store in our records tables.\n\nGathering from Amazon (All Products)\nGathering from BestBuy (Electronic Products, Furniture Item(s)? - no grocery or clothing)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#visualizations",
    "href": "data.html#visualizations",
    "title": "2  Data Collection and Exploration",
    "section": "2.4 Visualizations",
    "text": "2.4 Visualizations\n\nWe require a minimum of 10 unique EDA plots for this milestone. We’ve outlined some of the below but need our data in order and unified prior to development.\n\nScatter Plots\nBar Plots\nBox Plots\nViolin Plots (e.g. same product, two different websites)\nreview length\n\nvs star rating\nvs sentiment\n\nProduct description / detail length\n\nPotentially explore “specificity” classifier\n\nCorrelation analyses and linear regressions\nHeatmaps\nTukey test visuals\n\nInter-Website Comparison of Product Reviews\n\nSame Product\n\nClustering?\nDistances?\n\nAll Products\nInspect the following, visually:\n\nProduct Ratings\nCustomer Sentiments try to score before plotting & turn-in\nReview Polarity try to score and store before plotting\nNaive Bayes Classifier\nReliability estimates\nProduct description subjectivity scores try to score and store before turn-in\nAverage / Spread of number of ratings per product, try to score and store before turn-in\nAverage/Spread of Useful Votes per Product Review, try to score and store before turn-in\nInspection of Data and / or Scoring using Kansei method.\n\n\nWill need to take note on if / how these variables conform to some form of statistical distribution (uniform, normal, exponential, etc)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#data-before-after",
    "href": "data.html#data-before-after",
    "title": "2  Data Collection and Exploration",
    "section": "2.5 Data Before / After",
    "text": "2.5 Data Before / After",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "data.html#insights-from-collection-and-eda",
    "href": "data.html#insights-from-collection-and-eda",
    "title": "2  Data Collection and Exploration",
    "section": "2.6 Insights from Collection and EDA",
    "text": "2.6 Insights from Collection and EDA\n\n\n\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Collection and Exploration</span>"
    ]
  },
  {
    "objectID": "models implemented.html",
    "href": "models implemented.html",
    "title": "3  Models Implemented",
    "section": "",
    "text": "3.1 Examined Models from Literature Research",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#examined-models-from-literature-research",
    "href": "models implemented.html#examined-models-from-literature-research",
    "title": "3  Models Implemented",
    "section": "",
    "text": "Perceived Usefulness of Product Reviews\n\nModeling for search vs. experience goods as “mitigations” on reviews, examining their impact on perceived review usefulness\n\nWealth of Data Paper on Customer Evaulation\n\nExamination of ‘Vine’ customers from Amazon\nDon’t think that BestBuy or Target have the same.\nScope of this would be limited to Amazon, combining research from Perceived Usefulness paper (i.e. their modeling) with a customer “trustworthiness” score for Amazon Vine customers\n\nMay have challenges identifying products with Vine customers\nShould also have a look at “verified purchase” customers, too.\n\nWeaknesses (from authors)\n\nTheir NLP instance wasn’t well trained for fully accurate\nTheir weighting system was manually (i.e. arbitrarily) set as opposed to having a labeled dataset\nCan we fix these problems? If so - what data is needed?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#potential-model-for-search-vs.-experience-weight-scale-of-0-1",
    "href": "models implemented.html#potential-model-for-search-vs.-experience-weight-scale-of-0-1",
    "title": "3  Models Implemented",
    "section": "3.2 Potential Model for Search vs. Experience Weight (scale of 0-1)",
    "text": "3.2 Potential Model for Search vs. Experience Weight (scale of 0-1)\n\n3.2.1 Jaccard Similarity Measure\nhttps://www.sciencedirect.com/science/article/pii/S1567422318300450\n\nCan be used to produce a similarity between two items on a scale of 0 to 1\nMay be able to use this for evaluation of a novel item against a pure-search good vs. a pure-experience good.\n\nMay require additional calculation / computation between these two values - maybe its arithmetic or harmonic mean between search and experience\nUsing that, we could potentially produce a weight.\nAuthors models\n\nModel 1: \\(\\text{Perceived Usefulness} = \\beta_0 + \\beta_1 \\cdot \\text{Review Content} + \\beta_2 \\cdot \\text{Review Length} + \\beta_3\\cdot \\text{Star Rating} + \\beta_4 \\cdot \\text{Total Votes Received} + \\epsilon_1\\)\nModel 2 (product type as a moderator): \\(\\text{Perceived Usefulness} = \\beta_0 + \\beta_1 \\cdot \\text{Review Content} + \\beta_2 \\cdot \\text{Review Length} + \\beta_3\\cdot \\text{Star Rating} + \\beta_4 \\cdot \\text{Total Votes Received} + \\beta_5 \\cdot \\text{Digital Music} + \\beta_6 \\cdot \\text{Video Game} + \\beta_7 \\cdot \\text{Review Content}\\cdot \\text{ Digital Music} + \\beta_8\\cdot\\text{Review Content}\\cdot\\text{ Video Game} + \\epsilon_2\\)\nOur proposition #1 (may require some modification) \\[\n\\text{Perceived Usefulness} = \\beta_0 + \\beta_1 \\cdot \\text{Review Content} + \\beta_2 \\cdot \\text{Review Length} + \\beta_3\\cdot \\text{Star Rating} + \\beta_4 \\cdot \\text{Total Votes Received} + \\gamma\\cdot\\text{Review Content}\n\\]\n\n\n\nor\n\\[\n\\text{Perceived Usefulness} = \\beta_0 + \\gamma\\cdot\\beta_1\\cdot\\text{Review Content} + \\beta_2 \\cdot \\text{Review Length} + \\beta_3\\cdot \\text{Star Rating} + \\beta_4 \\cdot \\text{Total Votes Received}\n\\]\n\nWhere \\(\\gamma\\) is the Jaccard Similarity Score between a given product and elements we are identifying as “pure” experience and “pure” search good.\nOperationalizaton of Jaccard Similarity score Variable (i.e. inputs)\n\nSearch good: those with attributes that can be evaluated prior to purchase or consumption. Consumers rely on prior experience, direct product inspection and other information search activities to locate information that assists in the evaluation process. Most products fall into the search goods category (e.g. clothing, office stationery, home furnishings).\nNumber of measurement specifications?\ncomment/review information?\nWhat else could we gather that could be considered a “universal” tangible or intangible feature from a product online? They need to be applicable to both search and experience goods.\n\nExperience good: those that can be accurately evaluated only after the product has been purchased and experienced. Many personal services fall into this category (e.g. restaurant, hairdresser, beauty salon, theme park, travel, holiday).\n\nsubjective descriptiveness vs. \ncomment/review information?\n\nAre there other methods aside from Jaccard Similarity?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "models implemented.html#model-comparison",
    "href": "models implemented.html#model-comparison",
    "title": "3  Models Implemented",
    "section": "3.3 Model Comparison",
    "text": "3.3 Model Comparison\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models Implemented</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "Here’s our conclusions section\n\n\n\n\nGuha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022. “Perceived Usefulness of Online Customer Reviews: A Review Mining Approach Using Machine Learning & Exploratory Data Analysis.” Journal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product Features from Online Reviews.” 2010 IEEE 7th International Conference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending Products to Customers Using Opinion Mining of Online Product Reviews and Features.” 2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018. “Extracting and Summarizing Affective Features and Responses from Online Product Descriptions and Reviews: A Kansei Text Mining Approach.” Engineering Applications of Artificial Intelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Guha Majumder, Madhumita, Sangita Dutta Gupta, and Justin Paul. 2022.\n“Perceived Usefulness of Online Customer Reviews: A Review Mining\nApproach Using Machine Learning & Exploratory Data Analysis.”\nJournal of Business Research 150 (November): 147–64. https://doi.org/10.1016/j.jbusres.2022.06.012.\n\n\nHu, Weishu, Zhiguo Gong, and Jingzhi Guo. 2010. “Mining Product\nFeatures from Online Reviews.” 2010 IEEE 7th International\nConference on E-Business Engineering, November. https://doi.org/10.1109/icebe.2010.51.\n\n\nRajeev, P Venkata, and V Smrithi Rekha. 2015. “Recommending\nProducts to Customers Using Opinion Mining of Online Product Reviews and\nFeatures.” 2015 International Conference on Circuits, Power\nand Computing Technologies [ICCPCT-2015], March. https://doi.org/10.1109/iccpct.2015.7159433.\n\n\nWang, W. M., Z. Li, Z. G. Tian, J. W. Wang, and M. N. Cheng. 2018.\n“Extracting and Summarizing Affective Features and Responses from\nOnline Product Descriptions and Reviews: A Kansei Text Mining\nApproach.” Engineering Applications of Artificial\nIntelligence 73 (August): 149–62. https://doi.org/10.1016/j.engappai.2018.05.005.",
    "crumbs": [
      "References"
    ]
  }
]